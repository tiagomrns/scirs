//! Cross-platform GPU abstraction layer for unified optimizer acceleration
//!
//! This module provides a unified interface for GPU acceleration across different
//! platforms (NVIDIA CUDA, AMD ROCm, Intel OpenCL, etc.) while maintaining
//! optimal performance for each backend.

use std::collections::HashMap;
use std::fmt;
use std::sync::{Arc, Mutex};

use ndarray::{Array, Array1, Array2, Dimension};
use num_traits::Float;

use crate::error::{OptimError, Result};
use crate::gpu::{GpuOptimError, GpuOptimizerConfig};

#[cfg(feature = "gpu")]
use scirs2_core::gpu::{GpuBackend, GpuContext};

/// Unified GPU optimizer interface
pub trait UnifiedGpuOptimizer<T: Float>: Send + Sync {
    /// Initialize the optimizer with the given configuration
    fn initialize(&mut self, config: &CrossPlatformConfig) -> Result<()>;

    /// Perform optimization step
    fn step(&mut self, params: &mut Array1<T>, gradients: &Array1<T>) -> Result<()>;

    /// Set learning rate
    fn set_learning_rate(&mut self, lr: T) -> Result<()>;

    /// Get current learning rate
    fn get_learning_rate(&self) -> T;

    /// Reset optimizer state
    fn reset(&mut self) -> Result<()>;

    /// Get optimizer name
    fn name(&self) -> &str;

    /// Get performance metrics
    fn get_metrics(&self) -> OptimizationMetrics;

    /// Check if optimizer supports mixed precision
    fn supports_mixed_precision(&self) -> bool;

    /// Enable/disable mixed precision
    fn set_mixed_precision(&mut self, enabled: bool) -> Result<()>;
}

/// Cross-platform GPU configuration
#[derive(Debug, Clone)]
pub struct CrossPlatformConfig {
    /// Preferred GPU backend (auto-detected if None)
    pub preferred_backend: Option<GpuBackend>,

    /// Fallback backends in order of preference
    pub fallback_backends: Vec<GpuBackend>,

    /// Device selection strategy
    pub device_selection: DeviceSelectionStrategy,

    /// Memory management configuration
    pub memory_config: MemoryConfig,

    /// Performance optimization settings
    pub performance_config: PerformanceConfig,

    /// Cross-platform compatibility settings
    pub compatibility_config: CompatibilityConfig,

    /// Enable automatic backend switching
    pub auto_backend_switching: bool,

    /// Performance monitoring configuration
    pub monitoring_config: MonitoringConfig,
}

/// Device selection strategies
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum DeviceSelectionStrategy {
    /// Select the fastest available device
    Fastest,
    /// Select device with most memory
    LargestMemory,
    /// Select device with lowest power consumption
    LowPower,
    /// Round-robin selection for multi-device setups
    RoundRobin,
    /// Custom device selection
    Custom,
    /// User-specified device
    Manual { device_id: i32 },
}

/// Memory management configuration
#[derive(Debug, Clone)]
pub struct MemoryConfig {
    /// Memory pool size per device (bytes)
    pub pool_size: usize,

    /// Enable memory prefetching
    pub enable_prefetch: bool,

    /// Memory allocation strategy
    pub allocation_strategy: MemoryAllocationStrategy,

    /// Enable memory compression
    pub enable_compression: bool,

    /// Memory cleanup threshold (0.0-1.0)
    pub cleanup_threshold: f32,

    /// Enable unified memory (if supported)
    pub enable_unified_memory: bool,

    /// Memory bandwidth optimization
    pub optimize_bandwidth: bool,
}

/// Memory allocation strategies
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum MemoryAllocationStrategy {
    /// First-fit allocation
    FirstFit,
    /// Best-fit allocation
    BestFit,
    /// Buddy system allocation
    BuddySystem,
    /// Pool-based allocation
    PoolBased,
    /// Custom allocation strategy
    Custom,
}

/// Performance optimization configuration
#[derive(Debug, Clone)]
pub struct PerformanceConfig {
    /// Enable kernel fusion
    pub enable_kernel_fusion: bool,

    /// Enable asynchronous execution
    pub enable_async_execution: bool,

    /// Number of streams for overlapping
    pub num_streams: usize,

    /// Enable tensor core acceleration
    pub enable_tensor_cores: bool,

    /// Mixed precision configuration
    pub mixed_precision: MixedPrecisionConfig,

    /// Batch size optimization
    pub optimize_batch_size: bool,

    /// Enable dynamic load balancing
    pub enable_load_balancing: bool,

    /// Performance profiling level
    pub profiling_level: ProfilingLevel,
}

/// Mixed precision configuration
#[derive(Debug, Clone)]
pub struct MixedPrecisionConfig {
    /// Enable mixed precision
    pub enabled: bool,

    /// Loss scaling factor
    pub loss_scale: f32,

    /// Dynamic loss scaling
    pub dynamic_scaling: bool,

    /// Gradient clipping threshold
    pub gradient_clip: Option<f32>,

    /// Precision for different operations
    pub precision_config: PrecisionConfig,
}

/// Precision configuration for different operations
#[derive(Debug, Clone)]
pub struct PrecisionConfig {
    /// Forward pass precision
    pub forward_precision: PrecisionType,

    /// Gradient computation precision
    pub gradient_precision: PrecisionType,

    /// Parameter update precision
    pub update_precision: PrecisionType,

    /// Accumulation precision
    pub accumulation_precision: PrecisionType,
}

/// Supported precision types
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum PrecisionType {
    FP32,
    FP16,
    BF16,
    TF32,
    FP8,
    INT8,
    Dynamic,
}

/// Cross-platform compatibility settings
#[derive(Debug, Clone)]
pub struct CompatibilityConfig {
    /// Enable cross-vendor compatibility mode
    pub cross_vendor_mode: bool,

    /// Fallback to CPU for unsupported operations
    pub cpu_fallback: bool,

    /// Enable operation emulation
    pub enable_emulation: bool,

    /// Compatibility check strictness
    pub compatibility_strictness: CompatibilityStrictness,

    /// Platform-specific optimizations
    pub platform_optimizations: HashMap<GpuBackend, PlatformOptimizations>,
}

/// Compatibility check strictness levels
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum CompatibilityStrictness {
    /// Strict compatibility checks
    Strict,
    /// Moderate compatibility checks
    Moderate,
    /// Relaxed compatibility checks
    Relaxed,
    /// Best-effort compatibility
    BestEffort,
}

/// Platform-specific optimizations
#[derive(Debug, Clone)]
pub struct PlatformOptimizations {
    /// CUDA-specific optimizations
    pub cuda_optimizations: Option<CudaOptimizations>,

    /// ROCm-specific optimizations
    pub rocm_optimizations: Option<RocmOptimizations>,

    /// OpenCL-specific optimizations
    pub opencl_optimizations: Option<OpenCLOptimizations>,

    /// Metal-specific optimizations
    pub metal_optimizations: Option<MetalOptimizations>,

    /// SYCL-specific optimizations
    pub sycl_optimizations: Option<SyclOptimizations>,
}

/// CUDA-specific optimization settings
#[derive(Debug, Clone)]
pub struct CudaOptimizations {
    /// Enable CUDA graphs
    pub enable_cuda_graphs: bool,

    /// Use tensor cores when available
    pub use_tensor_cores: bool,

    /// Enable NVLink optimization
    pub enable_nvlink: bool,

    /// CUDA kernel launch configuration
    pub kernel_config: CudaKernelConfig,

    /// Memory coalescing optimization
    pub optimize_coalescing: bool,
}

/// ROCm-specific optimization settings
#[derive(Debug, Clone)]
pub struct RocmOptimizations {
    /// Enable HIP graphs
    pub enable_hip_graphs: bool,

    /// Use matrix cores when available
    pub use_matrix_cores: bool,

    /// Enable Infinity Fabric optimization
    pub enable_infinity_fabric: bool,

    /// ROCm kernel launch configuration
    pub kernel_config: RocmKernelConfig,

    /// Memory access pattern optimization
    pub optimize_memory_patterns: bool,
}

/// OpenCL-specific optimization settings
#[derive(Debug, Clone)]
pub struct OpenCLOptimizations {
    /// Preferred work group size
    pub work_group_size: usize,

    /// Enable local memory optimization
    pub optimize_local_memory: bool,

    /// Kernel compilation options
    pub compilation_options: Vec<String>,

    /// Buffer management strategy
    pub buffer_management: OpenCLBufferManagement,
}

/// Metal-specific optimization settings
#[derive(Debug, Clone)]
pub struct MetalOptimizations {
    /// Enable Metal Performance Shaders
    pub enable_mps: bool,

    /// Threadgroup size optimization
    pub optimize_threadgroup_size: bool,

    /// Memory bandwidth optimization
    pub optimize_memory_bandwidth: bool,

    /// Shader compilation options
    pub shader_options: Vec<String>,
}

/// SYCL-specific optimization settings
#[derive(Debug, Clone)]
pub struct SyclOptimizations {
    /// Preferred work group size
    pub work_group_size: usize,

    /// Enable sub-group operations
    pub enable_sub_groups: bool,

    /// Memory allocation strategy
    pub memory_strategy: SyclMemoryStrategy,

    /// Kernel optimization level
    pub optimization_level: SyclOptimizationLevel,
}

/// CUDA kernel configuration
#[derive(Debug, Clone)]
pub struct CudaKernelConfig {
    /// Block size for 1D kernels
    pub block_size_1d: usize,

    /// Block size for 2D kernels
    pub block_size_2d: (usize, usize),

    /// Shared memory per block
    pub shared_memory_per_block: usize,

    /// Registers per thread
    pub registers_per_thread: usize,

    /// Enable cooperative groups
    pub enable_cooperative_groups: bool,
}

/// ROCm kernel configuration
#[derive(Debug, Clone)]
pub struct RocmKernelConfig {
    /// Wavefront size
    pub wavefront_size: usize,

    /// Work group size
    pub work_group_size: usize,

    /// Local memory per work group
    pub local_memory_per_work_group: usize,

    /// Enable wavefront-aware optimizations
    pub enable_wavefront_optimizations: bool,
}

/// OpenCL buffer management strategies
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum OpenCLBufferManagement {
    /// Host-side buffer management
    HostSide,
    /// Device-side buffer management
    DeviceSide,
    /// Unified buffer management
    Unified,
    /// Custom buffer management
    Custom,
}

/// SYCL memory strategies
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum SyclMemoryStrategy {
    /// Unified shared memory
    USM,
    /// Buffer/accessor model
    Buffer,
    /// Hybrid approach
    Hybrid,
}

/// SYCL optimization levels
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum SyclOptimizationLevel {
    /// No optimization
    None,
    /// Basic optimization
    Basic,
    /// Aggressive optimization
    Aggressive,
    /// Maximum optimization
    Maximum,
}

/// Performance monitoring configuration
#[derive(Debug, Clone)]
pub struct MonitoringConfig {
    /// Enable performance monitoring
    pub enabled: bool,

    /// Monitoring frequency (operations)
    pub frequency: usize,

    /// Enable real-time monitoring
    pub real_time: bool,

    /// Metrics to collect
    pub metrics: Vec<MetricType>,

    /// Enable automatic optimization
    pub auto_optimization: bool,

    /// Performance alert thresholds
    pub alert_thresholds: AlertThresholds,
}

/// Types of metrics to collect
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum MetricType {
    /// Execution time
    ExecutionTime,
    /// Memory usage
    MemoryUsage,
    /// Power consumption
    PowerConsumption,
    /// Temperature
    Temperature,
    /// Throughput
    Throughput,
    /// Latency
    Latency,
    /// Cache hit rate
    CacheHitRate,
    /// Bandwidth utilization
    BandwidthUtilization,
}

/// Performance alert thresholds
#[derive(Debug, Clone)]
pub struct AlertThresholds {
    /// Maximum execution time (seconds)
    pub max_execution_time: f64,

    /// Maximum memory usage (percentage)
    pub max_memory_usage: f64,

    /// Maximum temperature (Celsius)
    pub max_temperature: f64,

    /// Minimum throughput (operations/second)
    pub min_throughput: f64,

    /// Maximum power consumption (watts)
    pub max_power: f64,
}

/// Profiling levels
#[derive(Debug, Clone, Copy, PartialEq)]
pub enum ProfilingLevel {
    /// No profiling
    None,
    /// Basic profiling
    Basic,
    /// Detailed profiling
    Detailed,
    /// Comprehensive profiling
    Comprehensive,
}

/// Optimization performance metrics
#[derive(Debug, Clone)]
pub struct OptimizationMetrics {
    /// Total execution time
    pub total_execution_time: f64,

    /// Average iteration time
    pub avg_iteration_time: f64,

    /// Throughput (iterations/second)
    pub throughput: f64,

    /// Memory usage statistics
    pub memory_stats: MemoryUsageStats,

    /// Power consumption statistics
    pub power_stats: PowerConsumptionStats,

    /// Convergence metrics
    pub convergence_metrics: ConvergenceMetrics,

    /// Backend-specific metrics
    pub backend_metrics: HashMap<String, f64>,
}

/// Memory usage statistics
#[derive(Debug, Clone)]
pub struct MemoryUsageStats {
    /// Current memory usage (bytes)
    pub current_usage: usize,

    /// Peak memory usage (bytes)
    pub peak_usage: usize,

    /// Average memory usage (bytes)
    pub avg_usage: usize,

    /// Memory utilization percentage
    pub utilization_percentage: f64,

    /// Memory bandwidth (GB/s)
    pub bandwidth: f64,
}

/// Power consumption statistics
#[derive(Debug, Clone)]
pub struct PowerConsumptionStats {
    /// Current power consumption (watts)
    pub current_power: f64,

    /// Average power consumption (watts)
    pub avg_power: f64,

    /// Peak power consumption (watts)
    pub peak_power: f64,

    /// Total energy consumed (joules)
    pub total_energy: f64,

    /// Power efficiency (operations/watt)
    pub efficiency: f64,
}

/// Convergence metrics
#[derive(Debug, Clone)]
pub struct ConvergenceMetrics {
    /// Current gradient norm
    pub gradient_norm: f64,

    /// Objective function value
    pub objective_value: f64,

    /// Convergence rate
    pub convergence_rate: f64,

    /// Number of iterations
    pub iterations: usize,

    /// Relative improvement
    pub relative_improvement: f64,
}

/// Cross-platform GPU optimizer manager
pub struct CrossPlatformOptimizer<T: Float> {
    /// Current configuration
    config: CrossPlatformConfig,

    /// Available GPU contexts
    contexts: HashMap<GpuBackend, Arc<GpuContext>>,

    /// Active optimizer instance
    active_optimizer: Option<Box<dyn UnifiedGpuOptimizer<T>>>,

    /// Current backend
    current_backend: Option<GpuBackend>,

    /// Performance metrics
    metrics: Arc<Mutex<OptimizationMetrics>>,

    /// Device capabilities cache
    device_capabilities: HashMap<GpuBackend, DeviceCapabilities>,

    /// Automatic optimization state
    auto_optimization_state: AutoOptimizationState,

    /// Performance history for adaptive optimization
    performance_history: Vec<PerformanceSnapshot>,
}

/// Device capabilities information
#[derive(Debug, Clone)]
pub struct DeviceCapabilities {
    /// Device name
    pub name: String,

    /// Compute capability
    pub compute_capability: (u32, u32),

    /// Total memory (bytes)
    pub total_memory: usize,

    /// Memory bandwidth (GB/s)
    pub memory_bandwidth: f64,

    /// Peak compute performance (FLOPS)
    pub peak_flops: f64,

    /// Supported precision types
    pub supported_precisions: Vec<PrecisionType>,

    /// Tensor core support
    pub tensor_core_support: bool,

    /// Mixed precision support
    pub mixed_precision_support: bool,

    /// Power consumption (watts)
    pub typical_power: f64,

    /// Thermal design power (watts)
    pub tdp: f64,
}

/// Automatic optimization state
#[derive(Debug, Clone)]
pub struct AutoOptimizationState {
    /// Enable automatic backend switching
    pub backend_switching_enabled: bool,

    /// Enable automatic parameter tuning
    pub parameter_tuning_enabled: bool,

    /// Enable adaptive precision
    pub adaptive_precision_enabled: bool,

    /// Optimization iteration count
    pub optimization_iterations: usize,

    /// Last optimization timestamp
    pub last_optimization: std::time::Instant,

    /// Optimization interval (iterations)
    pub optimization_interval: usize,
}

/// Performance snapshot for history tracking
#[derive(Debug, Clone)]
pub struct PerformanceSnapshot {
    /// Timestamp
    pub timestamp: std::time::Instant,

    /// Backend used
    pub backend: GpuBackend,

    /// Configuration hash
    pub config_hash: u64,

    /// Performance metrics
    pub metrics: OptimizationMetrics,

    /// Configuration parameters
    pub parameters: HashMap<String, f64>,
}

impl Default for CrossPlatformConfig {
    fn default() -> Self {
        Self {
            preferred_backend: None,
            fallback_backends: vec![
                GpuBackend::Cuda,
                GpuBackend::Rocm,
                GpuBackend::Metal,
                GpuBackend::Wgpu,
                GpuBackend::Cpu,
            ],
            device_selection: DeviceSelectionStrategy::Fastest,
            memory_config: MemoryConfig::default(),
            performance_config: PerformanceConfig::default(),
            compatibility_config: CompatibilityConfig::default(),
            auto_backend_switching: true,
            monitoring_config: MonitoringConfig::default(),
        }
    }
}

impl Default for MemoryConfig {
    fn default() -> Self {
        Self {
            pool_size: 1024 * 1024 * 1024, // 1GB
            enable_prefetch: true,
            allocation_strategy: MemoryAllocationStrategy::PoolBased,
            enable_compression: false,
            cleanup_threshold: 0.8,
            enable_unified_memory: true,
            optimize_bandwidth: true,
        }
    }
}

impl Default for PerformanceConfig {
    fn default() -> Self {
        Self {
            enable_kernel_fusion: true,
            enable_async_execution: true,
            num_streams: 4,
            enable_tensor_cores: true,
            mixed_precision: MixedPrecisionConfig::default(),
            optimize_batch_size: true,
            enable_load_balancing: true,
            profiling_level: ProfilingLevel::Basic,
        }
    }
}

impl Default for MixedPrecisionConfig {
    fn default() -> Self {
        Self {
            enabled: false,
            loss_scale: 1024.0,
            dynamic_scaling: true,
            gradient_clip: Some(1.0),
            precision_config: PrecisionConfig::default(),
        }
    }
}

impl Default for PrecisionConfig {
    fn default() -> Self {
        Self {
            forward_precision: PrecisionType::FP32,
            gradient_precision: PrecisionType::FP32,
            update_precision: PrecisionType::FP32,
            accumulation_precision: PrecisionType::FP32,
        }
    }
}

impl Default for CompatibilityConfig {
    fn default() -> Self {
        Self {
            cross_vendor_mode: true,
            cpu_fallback: true,
            enable_emulation: false,
            compatibility_strictness: CompatibilityStrictness::Moderate,
            platform_optimizations: HashMap::new(),
        }
    }
}

impl Default for MonitoringConfig {
    fn default() -> Self {
        Self {
            enabled: true,
            frequency: 100,
            real_time: false,
            metrics: vec![
                MetricType::ExecutionTime,
                MetricType::MemoryUsage,
                MetricType::Throughput,
            ],
            auto_optimization: false,
            alert_thresholds: AlertThresholds::default(),
        }
    }
}

impl Default for AlertThresholds {
    fn default() -> Self {
        Self {
            max_execution_time: 10.0,
            max_memory_usage: 90.0,
            max_temperature: 85.0,
            min_throughput: 100.0,
            max_power: 300.0,
        }
    }
}

impl<T: Float + Send + Sync> CrossPlatformOptimizer<T> {
    /// Create new cross-platform optimizer
    pub fn new(config: CrossPlatformConfig) -> Result<Self> {
        let mut optimizer = Self {
            config,
            contexts: HashMap::new(),
            active_optimizer: None,
            current_backend: None,
            metrics: Arc::new(Mutex::new(OptimizationMetrics::default())),
            device_capabilities: HashMap::new(),
            auto_optimization_state: AutoOptimizationState::default(),
            performance_history: Vec::new(),
        };

        optimizer.initialize_backends()?;
        optimizer.detect_device_capabilities()?;
        optimizer.select_optimal_backend()?;

        Ok(optimizer)
    }

    /// Initialize available GPU backends
    fn initialize_backends(&mut self) -> Result<()> {
        let backends_to_try = if let Some(preferred) = self.config.preferred_backend {
            let mut backends = vec![preferred];
            backends.extend(self.config.fallback_backends.iter().cloned());
            backends
        } else {
            self.config.fallback_backends.clone()
        };

        for backend in backends_to_try {
            #[cfg(feature = "gpu")]
            {
                if let Ok(context) = GpuContext::new(backend) {
                    self.contexts.insert(backend, Arc::new(context));
                    println!("✓ Initialized {} backend", backend_name(backend));
                }
            }
        }

        if self.contexts.is_empty() {
            return Err(OptimError::Other("No GPU backends available".to_string()));
        }

        Ok(())
    }

    /// Detect capabilities of available devices
    fn detect_device_capabilities(&mut self) -> Result<()> {
        for (backend, context) in &self.contexts {
            let capabilities = self.query_device_capabilities(*backend, context)?;
            self.device_capabilities.insert(*backend, capabilities);
        }
        Ok(())
    }

    /// Query device capabilities for a specific backend
    fn query_device_capabilities(
        &self,
        backend: GpuBackend,
        context: &Arc<GpuContext>,
    ) -> Result<DeviceCapabilities> {
        #[cfg(feature = "gpu")]
        {
            Ok(DeviceCapabilities {
                name: context
                    .device_name()
                    .unwrap_or_else(|_| format!("{:?} Device", backend)),
                compute_capability: context.compute_capability().unwrap_or((7, 5)),
                total_memory: context.total_memory().unwrap_or(8 * 1024 * 1024 * 1024),
                memory_bandwidth: context.memory_bandwidth_gb_s().unwrap_or(900.0),
                peak_flops: context.peak_flops().unwrap_or(31e12),
                supported_precisions: self.detect_supported_precisions(backend, context),
                tensor_core_support: context.supports_tensor_cores().unwrap_or(false),
                mixed_precision_support: context.supports_mixed_precision().unwrap_or(false),
                typical_power: context.typical_power().unwrap_or(250.0),
                tdp: context.tdp().unwrap_or(300.0),
            })
        }

        #[cfg(not(feature = "gpu"))]
        {
            Ok(DeviceCapabilities {
                name: format!("{:?} Device (Simulated)", backend),
                compute_capability: (7, 5),
                total_memory: 8 * 1024 * 1024 * 1024,
                memory_bandwidth: 900.0,
                peak_flops: 31e12,
                supported_precisions: vec![PrecisionType::FP32, PrecisionType::FP16],
                tensor_core_support: false,
                mixed_precision_support: false,
                typical_power: 250.0,
                tdp: 300.0,
            })
        }
    }

    /// Detect supported precision types for a backend
    fn detect_supported_precisions(
        &self,
        backend: GpuBackend,
        context: &Arc<GpuContext>,
    ) -> Vec<PrecisionType> {
        let mut precisions = vec![PrecisionType::FP32];

        #[cfg(feature = "gpu")]
        {
            if context.supports_fp16().unwrap_or(false) {
                precisions.push(PrecisionType::FP16);
            }
            if context.supports_bf16().unwrap_or(false) {
                precisions.push(PrecisionType::BF16);
            }
            if context.supports_tf32().unwrap_or(false) {
                precisions.push(PrecisionType::TF32);
            }
            if context.supports_fp8().unwrap_or(false) {
                precisions.push(PrecisionType::FP8);
            }
            if context.supports_int8().unwrap_or(false) {
                precisions.push(PrecisionType::INT8);
            }
        }

        precisions
    }

    /// Select optimal backend based on configuration and capabilities
    fn select_optimal_backend(&mut self) -> Result<()> {
        let backend = match self.config.device_selection {
            DeviceSelectionStrategy::Fastest => self.select_fastest_backend(),
            DeviceSelectionStrategy::LargestMemory => self.select_largest_memory_backend(),
            DeviceSelectionStrategy::LowPower => self.select_low_power_backend(),
            DeviceSelectionStrategy::Manual { device_id: _ } => self.select_manual_backend(),
            _ => self.select_fastest_backend(),
        };

        if let Some(backend) = backend {
            self.current_backend = Some(backend);
            println!("✓ Selected {} backend", backend_name(backend));

            // Initialize optimizer for selected backend
            self.initialize_optimizer_for_backend(backend)?;
        } else {
            return Err(OptimError::Other("No suitable backend found".to_string()));
        }

        Ok(())
    }

    /// Select the fastest available backend
    fn select_fastest_backend(&self) -> Option<GpuBackend> {
        self.device_capabilities
            .iter()
            .max_by(|(_, a), (_, b)| a.peak_flops.partial_cmp(&b.peak_flops).unwrap())
            .map(|(backend_)| *backend)
    }

    /// Select backend with largest memory
    fn select_largest_memory_backend(&self) -> Option<GpuBackend> {
        self.device_capabilities
            .iter()
            .max_by_key(|(_, caps)| caps.total_memory)
            .map(|(backend_)| *backend)
    }

    /// Select backend with lowest power consumption
    fn select_low_power_backend(&self) -> Option<GpuBackend> {
        self.device_capabilities
            .iter()
            .min_by(|(_, a), (_, b)| a.typical_power.partial_cmp(&b.typical_power).unwrap())
            .map(|(backend_)| *backend)
    }

    /// Select manually specified backend
    fn select_manual_backend(&self) -> Option<GpuBackend> {
        self.config.preferred_backend
    }

    /// Initialize optimizer for specific backend
    fn initialize_optimizer_for_backend(&mut self, backend: GpuBackend) -> Result<()> {
        // Create optimizer instance based on backend
        // This would typically involve creating a backend-specific optimizer
        // For now, we'll use a placeholder
        println!("Initializing optimizer for {:?} backend", backend);
        Ok(())
    }

    /// Perform optimization step with automatic backend management
    pub fn step(&mut self, params: &mut Array1<T>, gradients: &Array1<T>) -> Result<()> {
        if let Some(ref mut optimizer) = self.active_optimizer {
            let start_time = std::time::Instant::now();

            // Perform optimization step
            let result = optimizer.step(params, gradients);

            // Update metrics
            let execution_time = start_time.elapsed().as_secs_f64();
            self.update_metrics(execution_time);

            // Check for automatic optimization
            if self.auto_optimization_state.backend_switching_enabled {
                self.maybe_switch_backend()?;
            }

            result
        } else {
            Err(OptimError::Other("No active optimizer".to_string()))
        }
    }

    /// Update performance metrics
    fn update_metrics(&mut self, executiontime: f64) {
        if let Ok(mut metrics) = self.metrics.lock() {
            metrics.total_execution_time += execution_time;
            metrics.avg_iteration_time = (metrics.avg_iteration_time + execution_time) / 2.0;
            metrics.throughput = 1.0 / execution_time;

            // Update convergence metrics, memory stats, etc.
            // This would be implemented based on specific requirements
        }
    }

    /// Check if backend switching is beneficial
    fn maybe_switch_backend(&mut self) -> Result<()> {
        let current_performance = self.get_current_performance_score();

        // Check if we should consider switching
        if self.should_consider_backend_switch(current_performance) {
            if let Some(better_backend) = self.find_better_backend(current_performance) {
                self.switch_to_backend(better_backend)?;
            }
        }

        Ok(())
    }

    /// Get current performance score
    fn get_current_performance_score(&self) -> f64 {
        if let Ok(metrics) = self.metrics.lock() {
            // Simple performance score based on throughput and memory efficiency
            let throughput_score = metrics.throughput / 1000.0; // Normalize
            let memory_score = 1.0 - (metrics.memory_stats.utilization_percentage / 100.0);
            (throughput_score + memory_score) / 2.0
        } else {
            0.0
        }
    }

    /// Check if we should consider backend switching
    fn should_consider_backend_switch(&self, currentscore: f64) -> bool {
        // Switch if performance is below threshold and enough time has passed
        current_score < 0.7
            && self
                .auto_optimization_state
                .last_optimization
                .elapsed()
                .as_secs()
                > 60
    }

    /// Find a better backend than current
    fn find_better_backend(&self, _currentscore: f64) -> Option<GpuBackend> {
        // This would involve benchmarking other backends
        // For now, just return the fastest available backend if it's different
        let fastest = self.select_fastest_backend();

        if fastest != self.current_backend {
            fastest
        } else {
            None
        }
    }

    /// Switch to a different backend
    fn switch_to_backend(&mut self, backend: GpuBackend) -> Result<()> {
        println!(
            "Switching from {:?} to {:?} backend",
            self.current_backend, backend
        );

        self.current_backend = Some(backend);
        self.initialize_optimizer_for_backend(backend)?;

        // Update auto-optimization state
        self.auto_optimization_state.last_optimization = std::time::Instant::now();
        self.auto_optimization_state.optimization_iterations += 1;

        Ok(())
    }

    /// Get current performance metrics
    pub fn get_metrics(&self) -> Result<OptimizationMetrics> {
        self.metrics
            .lock()
            .map(|m| m.clone())
            .map_err(|_| OptimError::Other("Failed to get metrics".to_string()))
    }

    /// Get available backends
    pub fn get_available_backends(&self) -> Vec<GpuBackend> {
        self.contexts.keys().cloned().collect()
    }

    /// Get current backend
    pub fn get_current_backend(&self) -> Option<GpuBackend> {
        self.current_backend
    }

    /// Get device capabilities for a backend
    pub fn get_device_capabilities(&self, backend: GpuBackend) -> Option<&DeviceCapabilities> {
        self.device_capabilities.get(&backend)
    }

    /// Set configuration
    pub fn set_config(&mut self, config: CrossPlatformConfig) -> Result<()> {
        self.config = config;

        // Reinitialize with new configuration
        self.initialize_backends()?;
        self.detect_device_capabilities()?;
        self.select_optimal_backend()?;

        Ok(())
    }

    /// Enable automatic optimization
    pub fn enable_auto_optimization(&mut self, enabled: bool) {
        self.auto_optimization_state.backend_switching_enabled = enabled;
        self.auto_optimization_state.parameter_tuning_enabled = enabled;
        self.auto_optimization_state.adaptive_precision_enabled = enabled;
    }

    /// Generate performance report
    pub fn generate_performance_report(&self) -> String {
        let mut report = String::new();

        report.push_str("=== Cross-Platform GPU Optimizer Performance Report ===\n\n");

        // Current configuration
        report.push_str(&format!("Current Backend: {:?}\n", self.current_backend));
        report.push_str(&format!(
            "Available Backends: {:?}\n",
            self.get_available_backends()
        ));

        // Performance metrics
        if let Ok(metrics) = self.metrics.lock() {
            report.push_str(&format!("\nPerformance Metrics:\n"));
            report.push_str(&format!(
                "  Total Execution Time: {:.2}s\n",
                metrics.total_execution_time
            ));
            report.push_str(&format!(
                "  Average Iteration Time: {:.2}ms\n",
                metrics.avg_iteration_time * 1000.0
            ));
            report.push_str(&format!("  Throughput: {:.2} ops/s\n", metrics.throughput));
            report.push_str(&format!(
                "  Memory Utilization: {:.1}%\n",
                metrics.memory_stats.utilization_percentage
            ));
            report.push_str(&format!(
                "  Average Power: {:.1}W\n",
                metrics.power_stats.avg_power
            ));
        }

        // Device capabilities
        report.push_str("\nDevice Capabilities:\n");
        for (backend, caps) in &self.device_capabilities {
            report.push_str(&format!("  {:?}:\n", backend));
            report.push_str(&format!("    Name: {}\n", caps.name));
            report.push_str(&format!(
                "    Memory: {:.1}GB\n",
                caps.total_memory as f64 / (1024.0 * 1024.0 * 1024.0)
            ));
            report.push_str(&format!(
                "    Bandwidth: {:.1}GB/s\n",
                caps.memory_bandwidth
            ));
            report.push_str(&format!(
                "    Peak FLOPS: {:.1}TFLOPS\n",
                caps.peak_flops / 1e12
            ));
            report.push_str(&format!("    Tensor Cores: {}\n", caps.tensor_core_support));
            report.push_str(&format!(
                "    Mixed Precision: {}\n",
                caps.mixed_precision_support
            ));
        }

        // Auto-optimization status
        report.push_str("\nAuto-Optimization Status:\n");
        report.push_str(&format!(
            "  Backend Switching: {}\n",
            self.auto_optimization_state.backend_switching_enabled
        ));
        report.push_str(&format!(
            "  Parameter Tuning: {}\n",
            self.auto_optimization_state.parameter_tuning_enabled
        ));
        report.push_str(&format!(
            "  Adaptive Precision: {}\n",
            self.auto_optimization_state.adaptive_precision_enabled
        ));
        report.push_str(&format!(
            "  Optimization Iterations: {}\n",
            self.auto_optimization_state.optimization_iterations
        ));

        report
    }
}

impl Default for OptimizationMetrics {
    fn default() -> Self {
        Self {
            total_execution_time: 0.0,
            avg_iteration_time: 0.0,
            throughput: 0.0,
            memory_stats: MemoryUsageStats::default(),
            power_stats: PowerConsumptionStats::default(),
            convergence_metrics: ConvergenceMetrics::default(),
            backend_metrics: HashMap::new(),
        }
    }
}

impl Default for MemoryUsageStats {
    fn default() -> Self {
        Self {
            current_usage: 0,
            peak_usage: 0,
            avg_usage: 0,
            utilization_percentage: 0.0,
            bandwidth: 0.0,
        }
    }
}

impl Default for PowerConsumptionStats {
    fn default() -> Self {
        Self {
            current_power: 0.0,
            avg_power: 0.0,
            peak_power: 0.0,
            total_energy: 0.0,
            efficiency: 0.0,
        }
    }
}

impl Default for ConvergenceMetrics {
    fn default() -> Self {
        Self {
            gradient_norm: 0.0,
            objective_value: 0.0,
            convergence_rate: 0.0,
            iterations: 0,
            relative_improvement: 0.0,
        }
    }
}

impl Default for AutoOptimizationState {
    fn default() -> Self {
        Self {
            backend_switching_enabled: false,
            parameter_tuning_enabled: false,
            adaptive_precision_enabled: false,
            optimization_iterations: 0,
            last_optimization: std::time::Instant::now(),
            optimization_interval: 1000,
        }
    }
}

/// Get human-readable backend name
#[allow(dead_code)]
fn backend_name(backend: GpuBackend) -> &'static str {
    match _backend {
        GpuBackend::Cuda => "CUDA",
        GpuBackend::Rocm => "ROCm",
        GpuBackend::Metal => "Metal",
        GpuBackend::Wgpu => "WebGPU",
        GpuBackend::Cpu => "CPU",
    }
}

/// Convenience function to create a cross-platform optimizer with default settings
#[allow(dead_code)]
pub fn create_cross_platform_optimizer<T: Float>() -> Result<CrossPlatformOptimizer<T>> {
    let config = CrossPlatformConfig::default();
    CrossPlatformOptimizer::new(config)
}

/// Convenience function to create a cross-platform optimizer with performance-focused settings
#[allow(dead_code)]
pub fn create_performance_optimizer<T: Float>() -> Result<CrossPlatformOptimizer<T>> {
    let mut config = CrossPlatformConfig::default();
    config.device_selection = DeviceSelectionStrategy::Fastest;
    config.performance_config.enable_tensor_cores = true;
    config.performance_config.enable_async_execution = true;
    config.performance_config.mixed_precision.enabled = true;
    config.auto_backend_switching = true;
    config.monitoring_config.auto_optimization = true;

    CrossPlatformOptimizer::new(config)
}

/// Convenience function to create a cross-platform optimizer with memory-optimized settings
#[allow(dead_code)]
pub fn create_memory_optimizer<T: Float>() -> Result<CrossPlatformOptimizer<T>> {
    let mut config = CrossPlatformConfig::default();
    config.device_selection = DeviceSelectionStrategy::LargestMemory;
    config.memory_config.enable_compression = true;
    config.memory_config.optimize_bandwidth = true;
    config.memory_config.cleanup_threshold = 0.7;
    config.performance_config.mixed_precision.enabled = true;

    CrossPlatformOptimizer::new(config)
}

/// Convenience function to create a cross-platform optimizer with power-efficient settings
#[allow(dead_code)]
pub fn create_power_efficient_optimizer<T: Float>() -> Result<CrossPlatformOptimizer<T>> {
    let mut config = CrossPlatformConfig::default();
    config.device_selection = DeviceSelectionStrategy::LowPower;
    config.performance_config.mixed_precision.enabled = true;
    config
        .monitoring_config
        .metrics
        .push(MetricType::PowerConsumption);
    config.monitoring_config.alert_thresholds.max_power = 200.0;

    CrossPlatformOptimizer::new(config)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_cross_platform_config_default() {
        let config = CrossPlatformConfig::default();
        assert!(!config.fallback_backends.is_empty());
        assert!(config.auto_backend_switching);
        assert!(matches!(
            config.device_selection,
            DeviceSelectionStrategy::Fastest
        ));
    }

    #[test]
    fn test_memory_config_default() {
        let config = MemoryConfig::default();
        assert!(config.pool_size > 0);
        assert!(config.enable_prefetch);
        assert!(config.optimize_bandwidth);
        assert!(config.cleanup_threshold > 0.0 && config.cleanup_threshold <= 1.0);
    }

    #[test]
    fn test_performance_config_default() {
        let config = PerformanceConfig::default();
        assert!(config.enable_kernel_fusion);
        assert!(config.enable_async_execution);
        assert!(config.num_streams > 0);
        assert!(config.enable_tensor_cores);
    }

    #[test]
    fn test_precision_config() {
        let config = PrecisionConfig::default();
        assert_eq!(config.forward_precision, PrecisionType::FP32);
        assert_eq!(config.gradient_precision, PrecisionType::FP32);
        assert_eq!(config.update_precision, PrecisionType::FP32);
        assert_eq!(config.accumulation_precision, PrecisionType::FP32);
    }

    #[test]
    fn test_device_selection_strategies() {
        let strategies = [
            DeviceSelectionStrategy::Fastest,
            DeviceSelectionStrategy::LargestMemory,
            DeviceSelectionStrategy::LowPower,
            DeviceSelectionStrategy::RoundRobin,
            DeviceSelectionStrategy::Custom,
            DeviceSelectionStrategy::Manual { device_id: 0 },
        ];

        for strategy in &strategies {
            // Test that all strategies are valid
            assert!(matches!(
                strategy,
                DeviceSelectionStrategy::Fastest
                    | DeviceSelectionStrategy::LargestMemory
                    | DeviceSelectionStrategy::LowPower
                    | DeviceSelectionStrategy::RoundRobin
                    | DeviceSelectionStrategy::Custom
                    | DeviceSelectionStrategy::Manual { .. }
            ));
        }
    }

    #[test]
    fn test_backend_name_function() {
        assert_eq!(backend_name(GpuBackend::Cuda), "CUDA");
        assert_eq!(backend_name(GpuBackend::Rocm), "ROCm");
        assert_eq!(backend_name(GpuBackend::Metal), "Metal");
        assert_eq!(backend_name(GpuBackend::Wgpu), "WebGPU");
        assert_eq!(backend_name(GpuBackend::Cpu), "CPU");
    }

    #[test]
    fn test_convenience_functions() {
        // Test that convenience functions create valid optimizers
        // Note: These will fail in CI without GPU hardware, so we just test compilation
        let _perf_optimizer = create_performance_optimizer::<f32>();
        let _memory_optimizer = create_memory_optimizer::<f32>();
        let _power_optimizer = create_power_efficient_optimizer::<f32>();
    }

    #[test]
    fn test_precision_types() {
        let precisions = [
            PrecisionType::FP32,
            PrecisionType::FP16,
            PrecisionType::BF16,
            PrecisionType::TF32,
            PrecisionType::FP8,
            PrecisionType::INT8,
            PrecisionType::Dynamic,
        ];

        for precision in &precisions {
            assert!(matches!(
                precision,
                PrecisionType::FP32
                    | PrecisionType::FP16
                    | PrecisionType::BF16
                    | PrecisionType::TF32
                    | PrecisionType::FP8
                    | PrecisionType::INT8
                    | PrecisionType::Dynamic
            ));
        }
    }

    #[test]
    fn test_metric_types() {
        let metrics = [
            MetricType::ExecutionTime,
            MetricType::MemoryUsage,
            MetricType::PowerConsumption,
            MetricType::Temperature,
            MetricType::Throughput,
            MetricType::Latency,
            MetricType::CacheHitRate,
            MetricType::BandwidthUtilization,
        ];

        for metric in &metrics {
            assert!(matches!(
                metric,
                MetricType::ExecutionTime
                    | MetricType::MemoryUsage
                    | MetricType::PowerConsumption
                    | MetricType::Temperature
                    | MetricType::Throughput
                    | MetricType::Latency
                    | MetricType::CacheHitRate
                    | MetricType::BandwidthUtilization
            ));
        }
    }
}
