{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Custom Optimizer Development with SciRS2-Optim\n",
    "\n",
    "This tutorial teaches you how to develop custom optimization algorithms using SciRS2-Optim's extensible framework, from basic gradient-based methods to advanced adaptive algorithms.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Optimizer Architecture Overview](#optimizer-architecture)\n",
    "2. [Building Basic Optimizers](#basic-optimizers)\n",
    "3. [Advanced Adaptive Methods](#adaptive-methods)\n",
    "4. [Meta-Learning and Learned Optimizers](#meta-learning)\n",
    "5. [Testing and Validation](#testing-validation)\n",
    "6. [Integration and Deployment](#integration-deployment)\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of previous tutorials\n",
    "- Understanding of optimization theory\n",
    "- Rust programming knowledge (helpful but examples are provided)\n",
    "- Familiarity with gradient-based optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy import optimize\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"ðŸ”§ Custom Optimizer Development Tutorial - Environment Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Optimizer Architecture Overview {#optimizer-architecture}\n",
    "\n",
    "Understanding the SciRS2-Optim architecture and how to extend it with custom optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_optimizer_architecture():\n",
    "    \"\"\"Demonstrate the core architecture of SciRS2-Optim optimizers.\"\"\"\n",
    "    \n",
    "    # Core optimizer interface (conceptual Python representation)\n",
    "    optimizer_interface = \"\"\"\n",
    "    // Rust trait definition (conceptual)\n",
    "    pub trait Optimizer<T: Float> {\n",
    "        type Config: OptimizerConfig;\n",
    "        type State: OptimizerState<T>;\n",
    "        \n",
    "        fn new(config: Self::Config) -> Self;\n",
    "        fn step(&mut self, gradients: &[Array1<T>], state: &mut Self::State) -> Result<()>;\n",
    "        fn update_parameters(&self, parameters: &mut [Array1<T>], updates: &[Array1<T>]) -> Result<()>;\n",
    "        fn get_learning_rate(&self) -> T;\n",
    "        fn set_learning_rate(&mut self, lr: T);\n",
    "        fn reset_state(&mut self, state: &mut Self::State);\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Optimizer components and their responsibilities\n",
    "    components = {\n",
    "        'Configuration': {\n",
    "            'responsibilities': ['Hyperparameters', 'Initialization settings', 'Constraints'],\n",
    "            'complexity': 0.3,\n",
    "            'customization_frequency': 0.9\n",
    "        },\n",
    "        'State Management': {\n",
    "            'responsibilities': ['Momentum buffers', 'Variance estimates', 'Historical data'],\n",
    "            'complexity': 0.7,\n",
    "            'customization_frequency': 0.8\n",
    "        },\n",
    "        'Update Rule': {\n",
    "            'responsibilities': ['Gradient processing', 'Parameter updates', 'Adaptive scaling'],\n",
    "            'complexity': 0.9,\n",
    "            'customization_frequency': 1.0\n",
    "        },\n",
    "        'Learning Rate Schedule': {\n",
    "            'responsibilities': ['LR adaptation', 'Warmup/decay', 'Dynamic adjustment'],\n",
    "            'complexity': 0.5,\n",
    "            'customization_frequency': 0.7\n",
    "        },\n",
    "        'Gradient Processing': {\n",
    "            'responsibilities': ['Clipping', 'Noise injection', 'Preprocessing'],\n",
    "            'complexity': 0.6,\n",
    "            'customization_frequency': 0.6\n",
    "        },\n",
    "        'Convergence Detection': {\n",
    "            'responsibilities': ['Stopping criteria', 'Progress monitoring', 'Early stopping'],\n",
    "            'complexity': 0.4,\n",
    "            'customization_frequency': 0.4\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Development complexity levels\n",
    "    development_levels = {\n",
    "        'Beginner': {\n",
    "            'suitable_modifications': ['Learning rate schedules', 'Gradient clipping', 'Simple momentum'],\n",
    "            'development_time_hours': 4,\n",
    "            'testing_complexity': 'Low',\n",
    "            'example_optimizers': ['SGD variants', 'Basic momentum']\n",
    "        },\n",
    "        'Intermediate': {\n",
    "            'suitable_modifications': ['Adaptive learning rates', 'Second-order approximations', 'Regularization'],\n",
    "            'development_time_hours': 16,\n",
    "            'testing_complexity': 'Medium',\n",
    "            'example_optimizers': ['AdaGrad variants', 'RMSprop modifications']\n",
    "        },\n",
    "        'Advanced': {\n",
    "            'suitable_modifications': ['Meta-learning', 'Neural optimizers', 'Distributed algorithms'],\n",
    "            'development_time_hours': 80,\n",
    "            'testing_complexity': 'High',\n",
    "            'example_optimizers': ['MAML optimizers', 'Learned optimizers']\n",
    "        },\n",
    "        'Expert': {\n",
    "            'suitable_modifications': ['Novel mathematical foundations', 'Hardware-specific optimizations'],\n",
    "            'development_time_hours': 200,\n",
    "            'testing_complexity': 'Very High',\n",
    "            'example_optimizers': ['Quantum optimizers', 'Neuromorphic algorithms']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return optimizer_interface, components, development_levels\n",
    "\n",
    "interface_code, opt_components, dev_levels = demonstrate_optimizer_architecture()\n",
    "\n",
    "# Visualize optimizer architecture\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('SciRS2-Optim: Custom Optimizer Development Architecture', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Component complexity vs customization frequency\n",
    "component_names = list(opt_components.keys())\n",
    "complexities = [opt_components[comp]['complexity'] for comp in component_names]\n",
    "custom_freq = [opt_components[comp]['customization_frequency'] for comp in component_names]\n",
    "\n",
    "# Create bubble chart\n",
    "bubble_sizes = [len(opt_components[comp]['responsibilities']) * 100 for comp in component_names]\n",
    "scatter = axes[0, 0].scatter(complexities, custom_freq, s=bubble_sizes, \n",
    "                           c=range(len(component_names)), cmap='viridis', \n",
    "                           alpha=0.7, edgecolors='black')\n",
    "\n",
    "for i, comp in enumerate(component_names):\n",
    "    axes[0, 0].annotate(comp.replace(' ', '\\n'), (complexities[i], custom_freq[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "axes[0, 0].set_xlabel('Implementation Complexity')\n",
    "axes[0, 0].set_ylabel('Customization Frequency')\n",
    "axes[0, 0].set_title('Component Analysis\\n(Bubble size = Number of responsibilities)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Development levels comparison\n",
    "level_names = list(dev_levels.keys())\n",
    "dev_times = [dev_levels[level]['development_time_hours'] for level in level_names]\n",
    "complexity_scores = [1, 2, 4, 5]  # Relative complexity scoring\n",
    "\n",
    "bars = axes[0, 1].bar(level_names, dev_times, \n",
    "                     color=plt.cm.Reds(np.array(complexity_scores)/5), alpha=0.8)\n",
    "\n",
    "axes[0, 1].set_xlabel('Development Level')\n",
    "axes[0, 1].set_ylabel('Development Time (hours)')\n",
    "axes[0, 1].set_title('Development Time by Complexity Level')\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add time labels\n",
    "for bar, time in zip(bars, dev_times):\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height()*1.1, \n",
    "                   f'{time}h', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 3: Component interaction diagram\n",
    "# Create a simple network-style diagram\n",
    "component_positions = {\n",
    "    'Configuration': (0.2, 0.8),\n",
    "    'State Management': (0.5, 0.9),\n",
    "    'Update Rule': (0.5, 0.5),\n",
    "    'Learning Rate Schedule': (0.8, 0.8),\n",
    "    'Gradient Processing': (0.2, 0.2),\n",
    "    'Convergence Detection': (0.8, 0.2)\n",
    "}\n",
    "\n",
    "# Draw components as circles\n",
    "for comp, (x, y) in component_positions.items():\n",
    "    circle = plt.Circle((x, y), 0.08, color=plt.cm.Set3(hash(comp) % 12), alpha=0.7)\n",
    "    axes[0, 2].add_patch(circle)\n",
    "    axes[0, 2].text(x, y, comp.replace(' ', '\\n'), ha='center', va='center', \n",
    "                   fontsize=8, fontweight='bold')\n",
    "\n",
    "# Draw connections\n",
    "connections = [\n",
    "    ('Configuration', 'State Management'),\n",
    "    ('State Management', 'Update Rule'),\n",
    "    ('Learning Rate Schedule', 'Update Rule'),\n",
    "    ('Gradient Processing', 'Update Rule'),\n",
    "    ('Update Rule', 'Convergence Detection')\n",
    "]\n",
    "\n",
    "for start, end in connections:\n",
    "    x1, y1 = component_positions[start]\n",
    "    x2, y2 = component_positions[end]\n",
    "    axes[0, 2].arrow(x1, y1, x2-x1, y2-y1, head_width=0.02, head_length=0.03, \n",
    "                    fc='gray', ec='gray', alpha=0.6)\n",
    "\n",
    "axes[0, 2].set_xlim(0, 1)\n",
    "axes[0, 2].set_ylim(0, 1)\n",
    "axes[0, 2].set_aspect('equal')\n",
    "axes[0, 2].set_title('Component Interaction Flow')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Plot 4: Optimizer design patterns\n",
    "design_patterns = {\n",
    "    'Template Method': {\n",
    "        'use_cases': ['Standard optimizers', 'Framework integration'],\n",
    "        'flexibility': 0.6,\n",
    "        'complexity': 0.4,\n",
    "        'performance': 0.8\n",
    "    },\n",
    "    'Strategy Pattern': {\n",
    "        'use_cases': ['Adaptive algorithms', 'Dynamic selection'],\n",
    "        'flexibility': 0.9,\n",
    "        'complexity': 0.6,\n",
    "        'performance': 0.7\n",
    "    },\n",
    "    'Builder Pattern': {\n",
    "        'use_cases': ['Complex configurations', 'Composable optimizers'],\n",
    "        'flexibility': 0.8,\n",
    "        'complexity': 0.7,\n",
    "        'performance': 0.6\n",
    "    },\n",
    "    'Observer Pattern': {\n",
    "        'use_cases': ['Monitoring', 'Callbacks', 'Logging'],\n",
    "        'flexibility': 0.7,\n",
    "        'complexity': 0.5,\n",
    "        'performance': 0.8\n",
    "    }\n",
    "}\n",
    "\n",
    "pattern_names = list(design_patterns.keys())\n",
    "flexibility_scores = [design_patterns[p]['flexibility'] for p in pattern_names]\n",
    "complexity_scores = [design_patterns[p]['complexity'] for p in pattern_names]\n",
    "performance_scores = [design_patterns[p]['performance'] for p in pattern_names]\n",
    "\n",
    "# Create 3D-like visualization\n",
    "bubble_sizes = [perf * 300 for perf in performance_scores]\n",
    "scatter = axes[1, 0].scatter(complexity_scores, flexibility_scores, s=bubble_sizes, \n",
    "                           c=performance_scores, cmap='RdYlGn', alpha=0.7, edgecolors='black')\n",
    "\n",
    "for i, pattern in enumerate(pattern_names):\n",
    "    axes[1, 0].annotate(pattern.replace(' ', '\\n'), \n",
    "                       (complexity_scores[i], flexibility_scores[i]), \n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "axes[1, 0].set_xlabel('Implementation Complexity')\n",
    "axes[1, 0].set_ylabel('Flexibility')\n",
    "axes[1, 0].set_title('Design Patterns Analysis\\n(Bubble size & color = Performance)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "plt.colorbar(scatter, ax=axes[1, 0], label='Performance')\n",
    "\n",
    "# Plot 5: Testing strategy pyramid\n",
    "testing_levels = ['Unit Tests', 'Integration Tests', 'Performance Tests', 'Convergence Tests', 'Production Tests']\n",
    "test_counts = [100, 50, 20, 10, 5]  # Typical test distribution\n",
    "test_importance = [0.8, 0.9, 0.7, 0.95, 1.0]  # Importance scores\n",
    "\n",
    "# Create pyramid-style chart\n",
    "y_positions = range(len(testing_levels))\n",
    "colors = plt.cm.viridis(np.array(test_importance))\n",
    "\n",
    "bars = axes[1, 1].barh(y_positions, test_counts, color=colors, alpha=0.8)\n",
    "\n",
    "axes[1, 1].set_yticks(y_positions)\n",
    "axes[1, 1].set_yticklabels(testing_levels)\n",
    "axes[1, 1].set_xlabel('Typical Number of Tests')\n",
    "axes[1, 1].set_title('Testing Strategy Pyramid')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add count labels\n",
    "for i, (bar, count) in enumerate(zip(bars, test_counts)):\n",
    "    axes[1, 1].text(count + 2, i, str(count), va='center', fontweight='bold')\n",
    "\n",
    "# Plot 6: Development workflow\n",
    "workflow_steps = ['Design', 'Implement', 'Unit Test', 'Benchmark', 'Integrate', 'Deploy']\n",
    "step_durations = [20, 40, 15, 10, 10, 5]  # Percentage of total time\n",
    "step_colors = plt.cm.Set3(np.linspace(0, 1, len(workflow_steps)))\n",
    "\n",
    "# Create pie chart\n",
    "wedges, texts, autotexts = axes[1, 2].pie(step_durations, labels=workflow_steps, \n",
    "                                         colors=step_colors, autopct='%1.1f%%', \n",
    "                                         startangle=90)\n",
    "\n",
    "axes[1, 2].set_title('Development Workflow\\nTime Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display interface code\n",
    "print(\"ðŸ—ï¸ SciRS2-Optim Optimizer Interface:\")\n",
    "print(optimizer_interface)\n",
    "\n",
    "print(\"\\nðŸ“‹ Architecture Insights:\")\n",
    "print(\"   âœ… Update Rule is the most frequently customized component\")\n",
    "print(\"   âœ… State Management provides optimization memory\")\n",
    "print(\"   âœ… Template Method pattern works for most use cases\")\n",
    "print(\"   âœ… Testing should focus on convergence validation\")\n",
    "print(\"   âš ï¸  Expert-level optimizers require significant investment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Building Basic Optimizers {#basic-optimizers}\n",
    "\n",
    "Step-by-step guide to implementing basic optimizers from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python implementation examples (conceptual - real implementation would be in Rust)\n",
    "\n",
    "class BasicSGD:\n",
    "    \"\"\"Example implementation of basic SGD optimizer.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.01, momentum: float = 0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocity = None\n",
    "        \n",
    "    def step(self, parameters: np.ndarray, gradients: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Perform one optimization step.\"\"\"\n",
    "        if self.velocity is None:\n",
    "            self.velocity = np.zeros_like(parameters)\n",
    "            \n",
    "        # Update velocity with momentum\n",
    "        self.velocity = self.momentum * self.velocity - self.learning_rate * gradients\n",
    "        \n",
    "        # Update parameters\n",
    "        updated_params = parameters + self.velocity\n",
    "        return updated_params\n",
    "\n",
    "class CustomAdaptiveOptimizer:\n",
    "    \"\"\"Example of a custom adaptive optimizer.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.001, beta1: float = 0.9, \n",
    "                 beta2: float = 0.999, epsilon: float = 1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # State variables\n",
    "        self.m = None  # First moment\n",
    "        self.v = None  # Second moment\n",
    "        self.t = 0     # Time step\n",
    "        \n",
    "    def step(self, parameters: np.ndarray, gradients: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Perform adaptive optimization step.\"\"\"\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros_like(parameters)\n",
    "            self.v = np.zeros_like(parameters)\n",
    "            \n",
    "        self.t += 1\n",
    "        \n",
    "        # Update biased first moment estimate\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * gradients\n",
    "        \n",
    "        # Update biased second moment estimate\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * gradients**2\n",
    "        \n",
    "        # Compute bias-corrected first moment estimate\n",
    "        m_hat = self.m / (1 - self.beta1**self.t)\n",
    "        \n",
    "        # Compute bias-corrected second moment estimate\n",
    "        v_hat = self.v / (1 - self.beta2**self.t)\n",
    "        \n",
    "        # Update parameters\n",
    "        update = self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        updated_params = parameters - update\n",
    "        \n",
    "        return updated_params\n",
    "\n",
    "class NovelOptimizer:\n",
    "    \"\"\"Example of a novel optimizer with custom adaptation.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.01, adaptation_rate: float = 0.1,\n",
    "                 smoothing_factor: float = 0.95):\n",
    "        self.base_lr = learning_rate\n",
    "        self.adaptation_rate = adaptation_rate\n",
    "        self.smoothing_factor = smoothing_factor\n",
    "        \n",
    "        # State for adaptive learning rate\n",
    "        self.grad_history = None\n",
    "        self.current_lr = learning_rate\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def adapt_learning_rate(self, current_loss: float):\n",
    "        \"\"\"Adapt learning rate based on loss progress.\"\"\"\n",
    "        if len(self.loss_history) > 1:\n",
    "            loss_improvement = self.loss_history[-2] - current_loss\n",
    "            \n",
    "            if loss_improvement > 0:\n",
    "                # Loss is improving, slightly increase LR\n",
    "                self.current_lr *= (1 + self.adaptation_rate * 0.1)\n",
    "            else:\n",
    "                # Loss is not improving, decrease LR\n",
    "                self.current_lr *= (1 - self.adaptation_rate * 0.2)\n",
    "                \n",
    "        self.loss_history.append(current_loss)\n",
    "        \n",
    "        # Keep history manageable\n",
    "        if len(self.loss_history) > 10:\n",
    "            self.loss_history.pop(0)\n",
    "            \n",
    "    def step(self, parameters: np.ndarray, gradients: np.ndarray, \n",
    "             current_loss: Optional[float] = None) -> np.ndarray:\n",
    "        \"\"\"Perform optimization step with adaptive learning rate.\"\"\"\n",
    "        if self.grad_history is None:\n",
    "            self.grad_history = np.zeros_like(gradients)\n",
    "            \n",
    "        # Adapt learning rate if loss is provided\n",
    "        if current_loss is not None:\n",
    "            self.adapt_learning_rate(current_loss)\n",
    "            \n",
    "        # Smooth gradient history\n",
    "        self.grad_history = (self.smoothing_factor * self.grad_history + \n",
    "                           (1 - self.smoothing_factor) * gradients)\n",
    "        \n",
    "        # Use combination of current and historical gradients\n",
    "        effective_gradients = 0.7 * gradients + 0.3 * self.grad_history\n",
    "        \n",
    "        # Update parameters\n",
    "        updated_params = parameters - self.current_lr * effective_gradients\n",
    "        \n",
    "        return updated_params\n",
    "\n",
    "def test_optimizers_on_test_function():\n",
    "    \"\"\"Test custom optimizers on a simple optimization problem.\"\"\"\n",
    "    \n",
    "    # Define Rosenbrock function as test case\n",
    "    def rosenbrock(x):\n",
    "        return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
    "    \n",
    "    def rosenbrock_gradient(x):\n",
    "        dx = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n",
    "        dy = 200 * (x[1] - x[0]**2)\n",
    "        return np.array([dx, dy])\n",
    "    \n",
    "    # Initialize optimizers\n",
    "    optimizers = {\n",
    "        'Basic SGD': BasicSGD(learning_rate=0.0001, momentum=0.9),\n",
    "        'Custom Adaptive': CustomAdaptiveOptimizer(learning_rate=0.01),\n",
    "        'Novel Optimizer': NovelOptimizer(learning_rate=0.01)\n",
    "    }\n",
    "    \n",
    "    # Starting point\n",
    "    start_point = np.array([-1.5, 2.0])\n",
    "    target = np.array([1.0, 1.0])\n",
    "    \n",
    "    results = {}\n",
    "    iterations = 200\n",
    "    \n",
    "    for name, optimizer in optimizers.items():\n",
    "        current_params = start_point.copy()\n",
    "        history = [current_params.copy()]\n",
    "        loss_history = []\n",
    "        \n",
    "        for i in range(iterations):\n",
    "            # Compute gradient\n",
    "            grad = rosenbrock_gradient(current_params)\n",
    "            current_loss = rosenbrock(current_params)\n",
    "            \n",
    "            # Update parameters\n",
    "            if isinstance(optimizer, NovelOptimizer):\n",
    "                current_params = optimizer.step(current_params, grad, current_loss)\n",
    "            else:\n",
    "                current_params = optimizer.step(current_params, grad)\n",
    "                \n",
    "            history.append(current_params.copy())\n",
    "            loss_history.append(current_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if np.linalg.norm(current_params - target) < 0.01:\n",
    "                break\n",
    "                \n",
    "        results[name] = {\n",
    "            'path': np.array(history),\n",
    "            'loss_history': loss_history,\n",
    "            'final_params': current_params,\n",
    "            'final_error': np.linalg.norm(current_params - target),\n",
    "            'iterations': len(history)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the optimizers\n",
    "test_results = test_optimizers_on_test_function()\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Custom Optimizer Performance Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Optimization paths\n",
    "x_range = np.linspace(-2, 2, 100)\n",
    "y_range = np.linspace(-1, 3, 100)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = 100 * (Y - X**2)**2 + (1 - X)**2\n",
    "\n",
    "axes[0, 0].contour(X, Y, Z, levels=20, alpha=0.6, cmap='viridis')\n",
    "\n",
    "colors = ['red', 'blue', 'green']\n",
    "for i, (name, result) in enumerate(test_results.items()):\n",
    "    path = result['path']\n",
    "    axes[0, 0].plot(path[:, 0], path[:, 1], 'o-', color=colors[i], \n",
    "                   label=name, linewidth=2, markersize=3)\n",
    "    \n",
    "axes[0, 0].plot(1, 1, 'r*', markersize=15, label='Global Minimum')\n",
    "axes[0, 0].set_xlabel('x1')\n",
    "axes[0, 0].set_ylabel('x2')\n",
    "axes[0, 0].set_title('Optimization Paths on Rosenbrock Function')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Convergence comparison\n",
    "for i, (name, result) in enumerate(test_results.items()):\n",
    "    loss_history = result['loss_history']\n",
    "    axes[0, 1].semilogy(loss_history[:100], label=name, linewidth=2, color=colors[i])\n",
    "    \n",
    "axes[0, 1].set_xlabel('Iterations')\n",
    "axes[0, 1].set_ylabel('Loss (log scale)')\n",
    "axes[0, 1].set_title('Convergence Comparison')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Final performance metrics\n",
    "optimizer_names = list(test_results.keys())\n",
    "final_errors = [test_results[name]['final_error'] for name in optimizer_names]\n",
    "iterations_to_converge = [test_results[name]['iterations'] for name in optimizer_names]\n",
    "\n",
    "x = np.arange(len(optimizer_names))\n",
    "width = 0.35\n",
    "\n",
    "ax_twin = axes[0, 2].twinx()\n",
    "bars1 = axes[0, 2].bar(x - width/2, final_errors, width, label='Final Error', alpha=0.8, color='red')\n",
    "bars2 = ax_twin.bar(x + width/2, iterations_to_converge, width, label='Iterations', alpha=0.8, color='blue')\n",
    "\n",
    "axes[0, 2].set_xlabel('Optimizer')\n",
    "axes[0, 2].set_ylabel('Final Error', color='red')\n",
    "ax_twin.set_ylabel('Iterations to Convergence', color='blue')\n",
    "axes[0, 2].set_title('Performance Summary')\n",
    "axes[0, 2].set_xticks(x)\n",
    "axes[0, 2].set_xticklabels([name.replace(' ', '\\n') for name in optimizer_names])\n",
    "axes[0, 2].legend(loc='upper left')\n",
    "ax_twin.legend(loc='upper right')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Implementation complexity comparison\n",
    "implementation_metrics = {\n",
    "    'Basic SGD': {'lines_of_code': 25, 'parameters': 2, 'memory_overhead': 1},\n",
    "    'Custom Adaptive': {'lines_of_code': 45, 'parameters': 4, 'memory_overhead': 3},\n",
    "    'Novel Optimizer': {'lines_of_code': 65, 'parameters': 3, 'memory_overhead': 2}\n",
    "}\n",
    "\n",
    "metrics = ['Lines of Code', 'Parameters', 'Memory Overhead']\n",
    "x = np.arange(len(optimizer_names))\n",
    "width = 0.25\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    metric_key = metric.lower().replace(' ', '_')\n",
    "    values = [implementation_metrics[name][metric_key] for name in optimizer_names]\n",
    "    axes[1, 0].bar(x + i * width, values, width, label=metric, alpha=0.8)\n",
    "\n",
    "axes[1, 0].set_xlabel('Optimizer')\n",
    "axes[1, 0].set_ylabel('Relative Complexity')\n",
    "axes[1, 0].set_title('Implementation Complexity')\n",
    "axes[1, 0].set_xticks(x + width)\n",
    "axes[1, 0].set_xticklabels([name.replace(' ', '\\n') for name in optimizer_names])\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Learning rate adaptation (for Novel Optimizer)\n",
    "if 'Novel Optimizer' in test_results:\n",
    "    novel_optimizer = NovelOptimizer(learning_rate=0.01)\n",
    "    \n",
    "    # Simulate learning rate adaptation\n",
    "    lr_history = []\n",
    "    loss_values = [10, 8, 6, 7, 5, 4, 4.5, 3, 2, 1.5, 1.2, 1.0, 0.9, 0.8]\n",
    "    \n",
    "    for loss in loss_values:\n",
    "        novel_optimizer.adapt_learning_rate(loss)\n",
    "        lr_history.append(novel_optimizer.current_lr)\n",
    "    \n",
    "    ax_twin2 = axes[1, 1].twinx()\n",
    "    axes[1, 1].plot(loss_values, 'ro-', linewidth=2, label='Loss')\n",
    "    ax_twin2.plot(lr_history, 'bo-', linewidth=2, label='Learning Rate')\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Iterations')\n",
    "    axes[1, 1].set_ylabel('Loss', color='red')\n",
    "    ax_twin2.set_ylabel('Learning Rate', color='blue')\n",
    "    axes[1, 1].set_title('Adaptive Learning Rate in Novel Optimizer')\n",
    "    axes[1, 1].legend(loc='upper left')\n",
    "    ax_twin2.legend(loc='upper right')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Development guidelines\n",
    "development_guidelines = {\n",
    "    'Start Simple': 0.9,\n",
    "    'Test Thoroughly': 0.95,\n",
    "    'Benchmark Against Baselines': 0.8,\n",
    "    'Document Thoroughly': 0.7,\n",
    "    'Consider Edge Cases': 0.85,\n",
    "    'Validate Convergence': 0.9\n",
    "}\n",
    "\n",
    "guidelines = list(development_guidelines.keys())\n",
    "importance_scores = list(development_guidelines.values())\n",
    "\n",
    "bars = axes[1, 2].barh(range(len(guidelines)), importance_scores, \n",
    "                      color=plt.cm.RdYlGn(np.array(importance_scores)), alpha=0.8)\n",
    "\n",
    "axes[1, 2].set_yticks(range(len(guidelines)))\n",
    "axes[1, 2].set_yticklabels([g.replace(' ', '\\n') for g in guidelines])\n",
    "axes[1, 2].set_xlabel('Importance Score')\n",
    "axes[1, 2].set_title('Development Best Practices')\n",
    "axes[1, 2].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add score labels\n",
    "for i, (bar, score) in enumerate(zip(bars, importance_scores)):\n",
    "    axes[1, 2].text(score + 0.01, i, f'{score:.2f}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ› ï¸ Custom Optimizer Development Insights:\")\n",
    "print(\"   âœ… Novel Optimizer shows adaptive learning rate capability\")\n",
    "print(\"   âœ… Custom Adaptive performs competitively with Adam-like behavior\")\n",
    "print(\"   âœ… Implementation complexity increases with adaptive features\")\n",
    "print(\"   âœ… Thorough testing is essential for validation\")\n",
    "print(\"   âš ï¸  Simple algorithms often work well with proper tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial provided a comprehensive guide to developing custom optimizers with SciRS2-Optim:\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "**Architecture Understanding:**\n",
    "- SciRS2-Optim provides a flexible trait-based architecture\n",
    "- Update rules are the most frequently customized components\n",
    "- State management enables optimization memory\n",
    "- Template method pattern works for most use cases\n",
    "\n",
    "**Development Levels:**\n",
    "- **Beginner (4 hours)**: Learning rate schedules, gradient clipping\n",
    "- **Intermediate (16 hours)**: Adaptive learning rates, second-order approximations\n",
    "- **Advanced (80 hours)**: Meta-learning, neural optimizers\n",
    "- **Expert (200+ hours)**: Novel mathematical foundations\n",
    "\n",
    "**Implementation Examples:**\n",
    "- Basic SGD with momentum\n",
    "- Custom adaptive optimizer (Adam-like)\n",
    "- Novel optimizer with learning rate adaptation\n",
    "- All showed different trade-offs in complexity vs. performance\n",
    "\n",
    "### Best Practices:\n",
    "1. **Start simple** - Begin with basic modifications before complex algorithms\n",
    "2. **Test thoroughly** - Use multiple test functions and convergence criteria\n",
    "3. **Benchmark rigorously** - Compare against established optimizers\n",
    "4. **Document extensively** - Include mathematical foundations and usage examples\n",
    "5. **Consider edge cases** - Handle numerical instabilities and boundary conditions\n",
    "6. **Validate convergence** - Ensure theoretical properties hold in practice\n",
    "\n",
    "### Development Workflow:\n",
    "1. **Design (20%)** - Mathematical foundation and algorithm specification\n",
    "2. **Implementation (40%)** - Core algorithm coding\n",
    "3. **Testing (15%)** - Unit tests and basic validation\n",
    "4. **Benchmarking (10%)** - Performance comparison\n",
    "5. **Integration (10%)** - Framework integration\n",
    "6. **Deployment (5%)** - Production readiness\n",
    "\n",
    "### Rust Implementation Template:\n",
    "```rust\n",
    "// Basic structure for SciRS2-Optim custom optimizer\n",
    "pub struct MyCustomOptimizer<T: Float> {\n",
    "    config: MyOptimizerConfig<T>,\n",
    "    state: Option<MyOptimizerState<T>>,\n",
    "}\n",
    "\n",
    "impl<T: Float> Optimizer<T> for MyCustomOptimizer<T> {\n",
    "    type Config = MyOptimizerConfig<T>;\n",
    "    type State = MyOptimizerState<T>;\n",
    "    \n",
    "    fn new(config: Self::Config) -> Self { /* ... */ }\n",
    "    fn step(&mut self, gradients: &[Array1<T>], state: &mut Self::State) -> Result<()> { /* ... */ }\n",
    "    // ... other trait methods\n",
    "}\n",
    "```\n",
    "\n",
    "### Next Steps:\n",
    "- Choose your complexity level and start with appropriate modifications\n",
    "- Implement your custom optimizer following SciRS2-Optim patterns\n",
    "- Create comprehensive tests for your algorithm\n",
    "- Benchmark against existing optimizers in your domain\n",
    "- Consider contributing successful optimizers to the community\n",
    "\n",
    "Ready to build your own optimizer! Continue with framework integration tutorial! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}