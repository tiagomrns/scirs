//! Ultrathink AI-Driven Optimization Demo
//!
//! This example demonstrates the advanced AI-driven optimization capabilities
//! in ultrathink mode, showcasing neural performance prediction, adaptive
//! hyperparameter tuning, and multi-objective optimization.

use scirs2_core::error::CoreResult;
use scirs2_core::performance_optimization::ultrathink_optimization::*;
use std::time::Instant;

#[allow(dead_code)]
fn main() -> CoreResult<()> {
    println!("ðŸ§  SciRS2 Ultrathink AI-Driven Optimization Demo");
    println!("================================================\n");

    // Create AI optimization engine
    let config = UltrathinkOptimizationConfig {
        enable_neural_prediction: true,
        enable_adaptive_learning: true,
        enable_multi_objective: true,
        learning_rate: 0.001,
        history_window_size: 1000,
        min_samples_for_prediction: 10, // Lower for demo
        strategy_switch_threshold: 0.1,
        context_window_size: 100,
    };

    let ai_engine = AIOptimizationEngine::with_config(config);

    // Demonstrate AI-driven optimization for different scenarios
    demonstrate_scientific_computing_optimization(&ai_engine)?;
    demonstrate_real_time_learning(&ai_engine)?;
    demonstrate_multi_objective_optimization(&ai_engine)?;
    demonstrate_analytics_and_insights(&ai_engine)?;

    Ok(())
}

/// Demonstrate AI optimization for scientific computing workloads
#[allow(dead_code)]
fn demonstrate_scientific_computing_optimization(
    ai_engine: &AIOptimizationEngine,
) -> CoreResult<()> {
    println!("ðŸ”¬ Scientific Computing Optimization");
    println!("===================================");

    let scenarios = vec![
        (
            "Matrix Multiplication (Small)",
            create_execution_context(1000, "f64", "matrix_multiply", 0.3, 0.4),
        ),
        (
            "Matrix Multiplication (Large)",
            create_execution_context(1_000_000, "f64", "matrix_multiply", 0.7, 0.6),
        ),
        (
            "FFT Computation",
            create_execution_context(100_000, "complex64", "fft", 0.5, 0.3),
        ),
        (
            "Linear System Solve",
            create_execution_context(50_000, "f64", "linear_solve", 0.6, 0.8),
        ),
    ];

    for (name, context) in scenarios {
        println!("\nðŸ“Š Optimizing: {name}");

        let recommendation = ai_engine.predict_optimal_strategy(&context)?;

        println!("   Strategy: {:?}", recommendation.recommended_strategy);
        println!(
            "   Confidence: {:.1}%",
            recommendation.confidence_score * 100.0
        );
        println!(
            "   Predicted Time: {:.2} ms",
            recommendation.predicted_performance.execution_time_ns as f64 / 1_000_000.0
        );
        println!(
            "   Predicted Memory: {:.2} MB",
            recommendation.predicted_performance.memory_usage_bytes as f64 / 1_024_000.0
        );
        println!(
            "   Predicted Throughput: {:.0} ops/sec",
            recommendation.predicted_performance.throughput_ops_per_sec
        );

        if let Some(params) = recommendation.optimal_hyperparameters.get("chunk_size") {
            println!("   Optimal Chunk Size: {params:.0}");
        }

        if let Some(ref solution) = recommendation.multi_objective_solution {
            println!(
                "   Multi-objective Score: {:.3}",
                solution.objectives.iter().sum::<f64>() / solution.objectives.len() as f64
            );
        }

        // Simulate actual execution and learning
        simulate_execution_and_learning(ai_engine, &context, &recommendation)?;
    }

    Ok(())
}

/// Demonstrate real-time learning capabilities
#[allow(dead_code)]
fn demonstrate_real_time_learning(ai_engine: &AIOptimizationEngine) -> CoreResult<()> {
    println!("\n\nðŸŽ¯ Real-time Learning Demonstration");
    println!("===================================");

    println!("Simulating iterative optimization with learning...");

    let base_context = create_execution_context(10_000, "f64", "vector_operations", 0.4, 0.5);

    for iteration in 1..=10 {
        println!("\nIteration {iteration}/10:");

        // Get recommendation
        let recommendation = ai_engine.predict_optimal_strategy(&base_context)?;

        // Simulate performance degradation over iterations (system getting busier)
        let mut actual_performance = recommendation.predicted_performance.clone();
        actual_performance.execution_time_ns =
            (actual_performance.execution_time_ns as f64 * (1.0 + iteration as f64 * 0.05)) as u64;

        // Learn from execution
        ai_engine.learn_from_execution(
            &base_context,
            recommendation.recommended_strategy,
            &actual_performance,
        )?;

        println!("   Strategy: {:?}", recommendation.recommended_strategy);
        println!(
            "   Predicted: {:.2} ms",
            recommendation.predicted_performance.execution_time_ns as f64 / 1_000_000.0
        );
        println!(
            "   Actual: {:.2} ms",
            actual_performance.execution_time_ns as f64 / 1_000_000.0
        );
        println!("   Learning improved model accuracy");
    }

    Ok(())
}

/// Demonstrate multi-objective optimization
#[allow(dead_code)]
fn demonstrate_multi_objective_optimization(ai_engine: &AIOptimizationEngine) -> CoreResult<()> {
    println!("\n\nðŸŽ¯ Multi-Objective Optimization");
    println!("===============================");

    let contexts = vec![
        (
            "Performance-Critical",
            create_execution_context(100_000, "f64", "performance_critical", 0.2, 0.3),
        ),
        (
            "Memory-Constrained",
            create_execution_context(50_000, "f64", "memory_constrained", 0.9, 0.8),
        ),
        (
            "Energy-Efficient",
            create_execution_context(75_000, "f64", "energy_efficient", 0.3, 0.4),
        ),
    ];

    for (scenario, context) in contexts {
        println!("\nðŸ“Š Scenario: {scenario}");

        let recommendation = ai_engine.predict_optimal_strategy(&context)?;

        if let Some(solution) = &recommendation.multi_objective_solution {
            println!("   Multi-Objective Analysis:");
            println!("     Performance: {:.3}", solution.objectives[0]);
            println!("     Memory Efficiency: {:.3}", solution.objectives[1]);
            println!("     Energy Efficiency: {:.3}", solution.objectives[2]);
            println!("     Stability: {:.3}", solution.objectives[3]);
            println!("     Scalability: {:.3}", solution.objectives[4]);
            println!("   Dominance Rank: {}", solution.dominance_rank);
            println!("   Strategy: {:?}", solution.strategy);
        }

        println!("   Learning Recommendations:");
        for area in &recommendation.learning_recommendation.focus_areas {
            println!("     - Focus on: {area}");
        }
        for experiment in &recommendation.learning_recommendation.suggested_experiments {
            println!("     - Try: {experiment}");
        }
    }

    Ok(())
}

/// Demonstrate analytics and insights
#[allow(dead_code)]
fn demonstrate_analytics_and_insights(ai_engine: &AIOptimizationEngine) -> CoreResult<()> {
    println!("\n\nðŸ“ˆ AI Optimization Analytics");
    println!("============================");

    let analytics = ai_engine.get_optimization_analytics();

    println!("Model Performance:");
    println!(
        "   Prediction Accuracy: {:.1}%",
        analytics.predictor_accuracy.prediction_accuracy * 100.0
    );
    println!(
        "   R-squared: {:.3}",
        analytics.predictor_accuracy.r_squared
    );
    println!(
        "   RMSE: {:.3}",
        analytics.predictor_accuracy.root_mean_square_error
    );

    println!("\nOptimization Impact:");
    println!("   Total Optimizations: {}", analytics.total_optimizations);
    println!(
        "   Overall Improvement: {:.1}x",
        analytics.improvement_factor
    );
    println!(
        "   Energy Savings: {:.1}%",
        analytics.energy_savings * 100.0
    );
    println!(
        "   Memory Efficiency Gain: {:.1}%",
        analytics.memory_efficiency_gain * 100.0
    );

    println!("\nStrategy Performance:");
    for (strategy, performance) in &analytics.strategy_performance {
        println!("   {strategy:?}: {performance:.3}");
    }

    println!("\nKey Insights:");
    println!("   - AI-driven optimization adapts to changing conditions");
    println!("   - Neural networks learn from execution feedback");
    println!("   - Multi-objective optimization balances competing goals");
    println!("   - Real-time learning improves prediction accuracy");
    println!("   - Hyperparameter tuning optimizes for specific workloads");

    Ok(())
}

/// Create execution context for testing
#[allow(dead_code)]
fn create_execution_context(
    data_size: usize,
    data_type: &str,
    operation: &str,
    cpu_util: f64,
    memory_util: f64,
) -> ExecutionContext {
    ExecutionContext {
        data_size,
        data_type: data_type.to_string(),
        operation_type: operation.to_string(),
        system_load: SystemLoad {
            cpu_utilization: cpu_util,
            memory_utilization: memory_util,
            io_wait: 0.05,
            network_utilization: 0.1,
            active_processes: 150,
        },
        memory_pressure: memory_util * 0.8,
        cpu_characteristics: CpuCharacteristics {
            physical_cores: 8,
            logical_cores: 16,
            base_frequency_mhz: 3200,
            max_frequency_mhz: 4500,
            cache_sizes_kb: vec![32, 256, 16384], // L1, L2, L3
            simd_capabilities: vec!["AVX2".to_string(), "FMA".to_string()],
            architecture: "x86_64".to_string(),
        },
        available_accelerators: vec![AcceleratorType::GPU {
            memory_gb: 8.0,
            compute_capability: "sm_75".to_string(),
        }],
        temperature_celsius: Some(65.0),
    }
}

/// Simulate execution and learning
#[allow(dead_code)]
fn simulate_execution_and_learning(
    ai_engine: &AIOptimizationEngine,
    context: &ExecutionContext,
    recommendation: &AIOptimizationRecommendation,
) -> CoreResult<()> {
    // Simulate actual execution with some variance
    let variance_factor = 0.9 + (Instant::now().elapsed().as_nanos() % 200) as f64 / 1000.0;

    let actual_performance = PerformanceTarget {
        execution_time_ns: (recommendation.predicted_performance.execution_time_ns as f64
            * variance_factor) as u64,
        memory_usage_bytes: (recommendation.predicted_performance.memory_usage_bytes as f64
            * variance_factor) as usize,
        throughput_ops_per_sec: recommendation.predicted_performance.throughput_ops_per_sec
            / variance_factor,
        energy_consumption_j: recommendation.predicted_performance.energy_consumption_j
            * variance_factor,
        cache_hit_rate: (recommendation.predicted_performance.cache_hit_rate * variance_factor)
            .min(1.0),
    };

    // Feed back to AI for learning
    ai_engine.learn_from_execution(
        context,
        recommendation.recommended_strategy,
        &actual_performance,
    )?;

    Ok(())
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_ai_optimization_demo() {
        let result = std::panic::catch_unwind(|| {
            main().unwrap();
        });

        assert!(
            result.is_ok(),
            "AI optimization demo should complete successfully"
        );
    }

    #[test]
    fn test_execution_context_creation() {
        let context = create_execution_context(1000, "f64", "test", 0.5, 0.4);
        assert_eq!(context.data_size, 1000);
        assert_eq!(context.data_type, "f64");
        assert_eq!(context.operation_type, "test");
        assert_eq!(context.system_load.cpu_utilization, 0.5);
    }

    #[test]
    fn test_ai_engine_creation() {
        let engine = AIOptimizationEngine::new();
        let analytics = engine.get_optimization_analytics();
        assert!(analytics.predictor_accuracy.prediction_accuracy > 0.0);
    }
}
