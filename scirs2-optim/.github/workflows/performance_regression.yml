name: Performance Regression Testing

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      baseline_update:
        description: 'Update performance baseline'
        required: false
        default: 'false'
        type: boolean
      regression_sensitivity:
        description: 'Regression detection sensitivity'
        required: false
        default: 'medium'
        type: choice
        options:
        - low
        - medium
        - high

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1
  # Performance test configuration
  PERF_TEST_ITERATIONS: 100
  PERF_TEST_WARMUP: 10
  PERF_BASELINE_DIR: target/performance-baselines
  PERF_REPORT_DIR: target/performance-reports

jobs:
  performance-regression-test:
    name: Performance Regression Testing
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    strategy:
      matrix:
        rust: [stable]
        features: [default, simd, parallel, gpu]
        
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for trend analysis
        
    - name: Setup Rust toolchain
      uses: dtolnay/rust-toolchain@stable
      with:
        toolchain: ${{ matrix.rust }}
        components: rustfmt, clippy
        
    - name: Setup Rust cache
      uses: Swatinem/rust-cache@v2
      with:
        key: ${{ matrix.rust }}-${{ matrix.features }}
        
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          build-essential \
          pkg-config \
          libssl-dev \
          libblas-dev \
          liblapack-dev \
          libhdf5-dev \
          python3 \
          python3-pip \
          hyperfine \
          valgrind \
          perf
          
    - name: Install Python dependencies for analysis
      run: |
        pip3 install --user numpy scipy matplotlib pandas statsmodels
        
    - name: Install cargo-nextest
      run: cargo install cargo-nextest
      
    - name: Install performance profiling tools
      run: |
        cargo install flamegraph
        cargo install cargo-profdata
        # cargo install criterion  # Already part of dev dependencies
        
    - name: Setup performance baseline directory
      run: |
        mkdir -p $PERF_BASELINE_DIR
        mkdir -p $PERF_REPORT_DIR
        
    - name: Download existing baselines
      uses: actions/cache@v3
      with:
        path: ${{ env.PERF_BASELINE_DIR }}
        key: performance-baselines-${{ runner.os }}-${{ matrix.features }}
        restore-keys: |
          performance-baselines-${{ runner.os }}-
          
    - name: Build optimized release
      run: |
        cargo build --release --features ${{ matrix.features }}
        
    - name: Run warm-up benchmarks
      run: |
        echo "Running warm-up benchmarks..."
        cargo run --release --example comprehensive_benchmarking_example -- \
          --warmup $PERF_TEST_WARMUP \
          --iterations 5 \
          --output-format json \
          --output-file $PERF_REPORT_DIR/warmup-results.json
          
    - name: Run comprehensive performance benchmarks
      run: |
        echo "Running comprehensive performance benchmarks..."
        
        # Core optimizer benchmarks
        cargo run --release --example comprehensive_benchmarking_example -- \
          --iterations $PERF_TEST_ITERATIONS \
          --output-format json \
          --output-file $PERF_REPORT_DIR/optimizer-benchmarks.json \
          --features ${{ matrix.features }}
          
        # Memory usage benchmarks
        cargo run --release --example memory_efficient_comparison -- \
          --iterations 50 \
          --memory-tracking \
          --output-file $PERF_REPORT_DIR/memory-benchmarks.json
          
        # GPU acceleration benchmarks (if enabled)
        if [[ "${{ matrix.features }}" == *"gpu"* ]]; then
          cargo run --release --example gpu_optimizers -- \
            --iterations 25 \
            --output-file $PERF_REPORT_DIR/gpu-benchmarks.json || echo "GPU benchmarks skipped (no GPU available)"
        fi
        
    - name: Run micro-benchmarks with Criterion
      run: |
        echo "Running Criterion micro-benchmarks..."
        cargo bench --features ${{ matrix.features }} -- --output-format json | \
          tee $PERF_REPORT_DIR/criterion-results.json
          
    - name: Profile memory usage
      run: |
        echo "Profiling memory usage..."
        
        # Run with Valgrind for detailed memory analysis
        cargo build --release --features ${{ matrix.features }}
        
        timeout 300 valgrind \
          --tool=massif \
          --massif-out-file=$PERF_REPORT_DIR/massif.out \
          --detailed-freq=1 \
          target/release/examples/memory_efficient_comparison || true
          
        # Convert massif output to JSON
        ms_print $PERF_REPORT_DIR/massif.out > $PERF_REPORT_DIR/memory-profile.txt || true
        
    - name: Analyze performance metrics
      run: |
        echo "Analyzing performance metrics..."
        
        # Run custom performance analysis script
        python3 scripts/analyze_performance.py \
          --benchmark-results $PERF_REPORT_DIR \
          --baseline-dir $PERF_BASELINE_DIR \
          --output-report $PERF_REPORT_DIR/analysis-report.json \
          --features ${{ matrix.features }} \
          --commit-hash ${{ github.sha }} \
          --branch ${{ github.ref_name }}
          
    - name: Detect performance regressions
      id: regression-detection
      run: |
        echo "Detecting performance regressions..."
        
        # Run regression detection
        cargo run --release --bin performance-regression-detector -- \
          --benchmark-results $PERF_REPORT_DIR/analysis-report.json \
          --baseline-dir $PERF_BASELINE_DIR \
          --output-report $PERF_REPORT_DIR/regression-report.json \
          --confidence-threshold 0.95 \
          --degradation-threshold 0.05 \
          --sensitivity ${{ github.event.inputs.regression_sensitivity || 'medium' }} \
          --features ${{ matrix.features }}
          
        # Check regression status
        REGRESSION_STATUS=$(python3 -c "
        import json
        with open('$PERF_REPORT_DIR/regression-report.json', 'r') as f:
            report = json.load(f)
        print('failed' if report['status'] == 'Failed' else 'passed')
        ")
        
        echo "regression_status=$REGRESSION_STATUS" >> $GITHUB_OUTPUT
        echo "Regression detection status: $REGRESSION_STATUS"
        
    - name: Update performance baseline
      if: |
        github.event_name == 'push' && 
        github.ref == 'refs/heads/main' && 
        steps.regression-detection.outputs.regression_status == 'passed' ||
        github.event.inputs.baseline_update == 'true'
      run: |
        echo "Updating performance baseline..."
        
        # Update baseline with current results
        cargo run --release --bin performance-baseline-manager -- \
          --action update \
          --results-file $PERF_REPORT_DIR/analysis-report.json \
          --baseline-dir $PERF_BASELINE_DIR \
          --commit-hash ${{ github.sha }} \
          --branch ${{ github.ref_name }} \
          --features ${{ matrix.features }}
          
    - name: Generate performance report
      run: |
        echo "Generating comprehensive performance report..."
        
        # Generate HTML report
        python3 scripts/generate_performance_report.py \
          --regression-report $PERF_REPORT_DIR/regression-report.json \
          --analysis-report $PERF_REPORT_DIR/analysis-report.json \
          --baseline-dir $PERF_BASELINE_DIR \
          --output-html $PERF_REPORT_DIR/performance-report.html \
          --output-markdown $PERF_REPORT_DIR/performance-summary.md \
          --features ${{ matrix.features }}
          
        # Generate JUnit XML for CI integration
        python3 scripts/convert_to_junit.py \
          --regression-report $PERF_REPORT_DIR/regression-report.json \
          --output-junit $PERF_REPORT_DIR/junit-performance.xml \
          --features ${{ matrix.features }}
          
    - name: Upload performance artifacts
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-results-${{ matrix.features }}-${{ matrix.rust }}
        path: |
          ${{ env.PERF_REPORT_DIR }}
          ${{ env.PERF_BASELINE_DIR }}
        retention-days: 30
        
    - name: Comment on PR with performance results
      uses: actions/github-script@v6
      if: github.event_name == 'pull_request'
      with:
        script: |
          const fs = require('fs');
          const path = '${{ env.PERF_REPORT_DIR }}/performance-summary.md';
          
          if (fs.existsSync(path)) {
            const summary = fs.readFileSync(path, 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## Performance Test Results - ${{ matrix.features }}\n\n${summary}`
            });
          }
          
    - name: Publish test results
      uses: dorny/test-reporter@v1
      if: always()
      with:
        name: Performance Regression Tests - ${{ matrix.features }}
        path: ${{ env.PERF_REPORT_DIR }}/junit-performance.xml
        reporter: java-junit
        
    - name: Send Slack notification on regression
      if: |
        steps.regression-detection.outputs.regression_status == 'failed' &&
        github.event_name == 'push'
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        channel: '#performance-alerts'
        text: |
          🚨 Performance regression detected in ${{ github.repository }}
          Branch: ${{ github.ref_name }}
          Commit: ${{ github.sha }}
          Features: ${{ matrix.features }}
          
          Please review the performance report for details.
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_PERFORMANCE_WEBHOOK }}
        
    - name: Fail on performance regression
      if: |
        steps.regression-detection.outputs.regression_status == 'failed' &&
        github.event_name == 'push' &&
        github.ref == 'refs/heads/main'
      run: |
        echo "❌ Performance regression detected on main branch!"
        echo "Please review the performance report and investigate the regression."
        exit 1
        
    - name: Cache updated baselines
      uses: actions/cache@v3
      if: |
        github.event_name == 'push' && 
        github.ref == 'refs/heads/main' && 
        steps.regression-detection.outputs.regression_status == 'passed'
      with:
        path: ${{ env.PERF_BASELINE_DIR }}
        key: performance-baselines-${{ runner.os }}-${{ matrix.features }}-${{ github.sha }}

  performance-summary:
    name: Performance Summary
    runs-on: ubuntu-latest
    needs: performance-regression-test
    if: always()
    
    steps:
    - name: Download all performance artifacts
      uses: actions/download-artifact@v3
      with:
        path: performance-artifacts
        
    - name: Generate consolidated report
      run: |
        echo "Generating consolidated performance report..."
        
        # Create summary report from all feature combinations
        python3 -c "
        import json
        import os
        import glob
        
        summary = {
            'overall_status': 'passed',
            'feature_results': {},
            'timestamp': '$(date -Iseconds)',
            'commit': '${{ github.sha }}',
            'branch': '${{ github.ref_name }}'
        }
        
        for report_file in glob.glob('performance-artifacts/*/regression-report.json'):
            try:
                with open(report_file, 'r') as f:
                    data = json.load(f)
                    
                feature = report_file.split('/')[1].split('-')[2]  # Extract feature name
                summary['feature_results'][feature] = {
                    'status': data.get('status', 'unknown'),
                    'regression_count': data.get('regression_count', 0),
                    'critical_regressions': data.get('critical_regressions', 0)
                }
                
                if data.get('status') == 'Failed':
                    summary['overall_status'] = 'failed'
                    
            except Exception as e:
                print(f'Error processing {report_file}: {e}')
        
        with open('performance-summary.json', 'w') as f:
            json.dump(summary, f, indent=2)
            
        print('Performance Summary:')
        print(json.dumps(summary, indent=2))
        "
        
    - name: Upload consolidated summary
      uses: actions/upload-artifact@v3
      with:
        name: performance-summary
        path: performance-summary.json
        retention-days: 90