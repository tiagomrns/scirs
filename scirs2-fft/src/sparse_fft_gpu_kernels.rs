//! GPU kernel implementations for sparse FFT algorithms
//!
//! This module contains kernel implementations for various sparse FFT algorithms
//! targeted at GPU acceleration. These kernels are designed to be highly
//! optimized for specific GPU architectures and can be used with different
//! GPU backends (CUDA, HIP, SYCL).

use crate::error::{FFTError, FFTResult};
use crate::sparse_fft::{SparseFFTAlgorithm, WindowFunction};
use num_complex::Complex64;
use num_traits::NumCast;
use scirs2_core::simd_ops::PlatformCapabilities;
use std::fmt::Debug;

/// GPU kernel configuration
#[derive(Debug, Clone)]
pub struct KernelConfig {
    /// Block size for GPU kernel
    pub block_size: usize,
    /// Grid size for GPU kernel
    pub grid_size: usize,
    /// Shared memory size per block in bytes
    pub shared_memory_size: usize,
    /// Whether to use mixed precision
    pub use_mixed_precision: bool,
    /// Number of registers per thread
    pub registers_per_thread: usize,
    /// Whether to use tensor cores (if available)
    pub use_tensor_cores: bool,
}

impl Default for KernelConfig {
    fn default() -> Self {
        Self {
            block_size: 256,
            grid_size: 0,                  // will be computed based on input size
            shared_memory_size: 16 * 1024, // 16 KB
            use_mixed_precision: false,
            registers_per_thread: 32,
            use_tensor_cores: false,
        }
    }
}

/// Kernel implementation type
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum KernelImplementation {
    /// Optimized for throughput
    Throughput,
    /// Optimized for latency
    Latency,
    /// Optimized for memory efficiency
    MemoryEfficient,
    /// Optimized for accuracy
    HighAccuracy,
    /// Optimized for power efficiency
    PowerEfficient,
}

/// Kernel execution statistics
#[derive(Debug, Clone)]
pub struct KernelStats {
    /// Kernel execution time
    pub execution_time_ms: f64,
    /// Memory bandwidth used (GB/s)
    pub memory_bandwidth_gb_s: f64,
    /// Compute throughput (GFLOPS)
    pub compute_throughput_gflops: f64,
    /// Memory transfers host->device (bytes)
    pub bytes_transferred_to_device: usize,
    /// Memory transfers device->host (bytes)
    pub bytes_transferred_from_device: usize,
    /// Occupancy percentage
    pub occupancy_percent: f64,
}

/// Trait for GPU kernels
pub trait GPUKernel {
    /// Get kernel name
    fn name(&self) -> &str;

    /// Get kernel configuration
    fn config(&self) -> &KernelConfig;

    /// Set kernel configuration
    fn set_config(&mut self, config: KernelConfig);

    /// Execute kernel
    fn execute(&self) -> FFTResult<KernelStats>;
}

/// Kernel for computing FFT on GPU
#[derive(Debug)]
pub struct FFTKernel {
    /// Kernel configuration
    config: KernelConfig,
    /// Size of the input signal
    input_size: usize,
    /// Input data GPU memory address/identifier
    #[allow(dead_code)]
    input_address: usize,
    /// Output data GPU memory address/identifier
    #[allow(dead_code)]
    output_address: usize,
}

impl FFTKernel {
    /// Create a new FFT kernel
    pub fn new(input_size: usize, input_address: usize, outputaddress: usize) -> Self {
        let mut config = KernelConfig::default();
        // Calculate grid _size based on input _size and block _size
        config.grid_size = input_size.div_ceil(config.block_size);

        Self {
            config,
            input_size,
            input_address,
            output_address: outputaddress,
        }
    }
}

impl GPUKernel for FFTKernel {
    fn name(&self) -> &str {
        "FFT_Kernel"
    }

    fn config(&self) -> &KernelConfig {
        &self.config
    }

    fn set_config(&mut self, config: KernelConfig) {
        self.config = config;
    }

    fn execute(&self) -> FFTResult<KernelStats> {
        // This would call into device-specific FFT implementation
        // For now, just return dummy stats

        // Estimate execution time based on input size
        let execution_time_ms = self.input_size as f64 * 0.001;

        // Create dummy stats
        let stats = KernelStats {
            execution_time_ms,
            memory_bandwidth_gb_s: 500.0,
            compute_throughput_gflops: 10000.0,
            bytes_transferred_to_device: self.input_size * std::mem::size_of::<Complex64>(),
            bytes_transferred_from_device: self.input_size * std::mem::size_of::<Complex64>(),
            occupancy_percent: 80.0,
        };

        Ok(stats)
    }
}

/// Kernel for computing sparse FFT on GPU
#[derive(Debug)]
pub struct SparseFFTKernel {
    /// Kernel configuration
    config: KernelConfig,
    /// Size of the input signal
    input_size: usize,
    /// Expected number of significant frequency components
    sparsity: usize,
    /// Input data GPU memory address/identifier
    #[allow(dead_code)]
    input_address: usize,
    /// Output values GPU memory address/identifier
    #[allow(dead_code)]
    output_values_address: usize,
    /// Output indices GPU memory address/identifier
    #[allow(dead_code)]
    output_indices_address: usize,
    /// Algorithm to use
    algorithm: SparseFFTAlgorithm,
    /// Window function to apply
    window_function: WindowFunction,
}

impl SparseFFTKernel {
    /// Create a new sparse FFT kernel
    #[allow(clippy::too_many_arguments)]
    pub fn new(
        input_size: usize,
        sparsity: usize,
        input_address: usize,
        output_values_address: usize,
        output_indices_address: usize,
        algorithm: SparseFFTAlgorithm,
        window_function: WindowFunction,
    ) -> Self {
        let mut config = KernelConfig::default();
        // Calculate grid _size based on input _size and block _size
        config.grid_size = input_size.div_ceil(config.block_size);

        Self {
            config,
            input_size,
            sparsity,
            input_address,
            output_values_address,
            output_indices_address,
            algorithm,
            window_function,
        }
    }

    /// Apply window function on GPU
    pub fn apply_window(&self) -> FFTResult<KernelStats> {
        // This would apply the selected window function on GPU
        // For now, just return dummy stats
        let execution_time_ms = self.input_size as f64 * 0.0001;

        let stats = KernelStats {
            execution_time_ms,
            memory_bandwidth_gb_s: 400.0,
            compute_throughput_gflops: 1000.0,
            bytes_transferred_to_device: 0,
            bytes_transferred_from_device: 0,
            occupancy_percent: 70.0,
        };

        Ok(stats)
    }

    /// Get algorithm-specific implementation
    pub fn get_algorithm_implementation(&self) -> FFTResult<KernelImplementation> {
        // Choose the best implementation based on algorithm, input size, and GPU capabilities
        match self.algorithm {
            SparseFFTAlgorithm::Sublinear => Ok(KernelImplementation::Throughput),
            SparseFFTAlgorithm::CompressedSensing => Ok(KernelImplementation::HighAccuracy),
            SparseFFTAlgorithm::Iterative => Ok(KernelImplementation::Latency),
            SparseFFTAlgorithm::Deterministic => Ok(KernelImplementation::Throughput),
            SparseFFTAlgorithm::FrequencyPruning => Ok(KernelImplementation::MemoryEfficient),
            SparseFFTAlgorithm::SpectralFlatness => Ok(KernelImplementation::HighAccuracy),
        }
    }
}

impl GPUKernel for SparseFFTKernel {
    fn name(&self) -> &str {
        "SparseFFT_Kernel"
    }

    fn config(&self) -> &KernelConfig {
        &self.config
    }

    fn set_config(&mut self, config: KernelConfig) {
        self.config = config;
    }

    fn execute(&self) -> FFTResult<KernelStats> {
        // This would call into device-specific sparse FFT implementation
        // For now, just return dummy stats

        // Different algorithms have different performance characteristics
        let algorithm_factor = match self.algorithm {
            SparseFFTAlgorithm::Sublinear => 0.8,
            SparseFFTAlgorithm::CompressedSensing => 1.5,
            SparseFFTAlgorithm::Iterative => 1.2,
            SparseFFTAlgorithm::Deterministic => 1.0,
            SparseFFTAlgorithm::FrequencyPruning => 0.9,
            SparseFFTAlgorithm::SpectralFlatness => 1.3,
        };

        // Window functions also affect performance
        let window_factor = match self.window_function {
            WindowFunction::None => 1.0,
            WindowFunction::Hann => 1.1,
            WindowFunction::Hamming => 1.1,
            WindowFunction::Blackman => 1.2,
            WindowFunction::FlatTop => 1.3,
            WindowFunction::Kaiser => 1.4,
        };

        // Estimate execution time based on input size, sparsity, algorithm, and window function
        let execution_time_ms = self.input_size as f64 * algorithm_factor * window_factor * 0.001;

        // Create stats
        let stats = KernelStats {
            execution_time_ms,
            memory_bandwidth_gb_s: 450.0,
            compute_throughput_gflops: 9000.0,
            bytes_transferred_to_device: self.input_size * std::mem::size_of::<Complex64>(),
            bytes_transferred_from_device: (self.sparsity * 2) * std::mem::size_of::<Complex64>(),
            occupancy_percent: 75.0,
        };

        Ok(stats)
    }
}

/// Kernel factory for creating optimized kernels
#[derive(Debug)]
pub struct KernelFactory {
    /// Target GPU architecture
    #[allow(dead_code)]
    arch: String,
    /// Available compute capabilities
    compute_capabilities: Vec<(i32, i32)>,
    /// Available memory (bytes)
    available_memory: usize,
    /// Shared memory per block (bytes)
    shared_memory_per_block: usize,
    /// Maximum threads per block
    max_threads_per_block: usize,
}

impl KernelFactory {
    /// Create a new kernel factory
    pub fn new(
        arch: String,
        compute_capabilities: Vec<(i32, i32)>,
        available_memory: usize,
        shared_memory_per_block: usize,
        max_threads_per_block: usize,
    ) -> Self {
        Self {
            arch,
            compute_capabilities,
            available_memory,
            shared_memory_per_block,
            max_threads_per_block,
        }
    }

    /// Create an FFT kernel optimized for the target GPU
    pub fn create_fft_kernel(
        &self,
        input_size: usize,
        input_address: usize,
        output_address: usize,
    ) -> FFTResult<FFTKernel> {
        let mut kernel = FFTKernel::new(input_size, input_address, output_address);

        // Customize configuration based on GPU
        let mut config = KernelConfig::default();

        // Set block _size based on GPU capabilities
        config.block_size = if self.max_threads_per_block >= 1024 {
            1024
        } else if self.max_threads_per_block >= 512 {
            512
        } else {
            256
        };

        // Calculate grid _size
        config.grid_size = input_size.div_ceil(config.block_size);

        // Set shared memory _size
        config.shared_memory_size = std::cmp::min(
            self.shared_memory_per_block,
            16 * 1024, // 16 KB default
        );

        // Enable mixed precision for newer GPUs
        if !self.compute_capabilities.is_empty()
            && (self.compute_capabilities[0].0 >= 7
                || (self.compute_capabilities[0].0 == 6 && self.compute_capabilities[0].1 >= 1))
        {
            config.use_mixed_precision = true;
        }

        // Enable tensor cores for supported architectures
        if !self.compute_capabilities.is_empty() && self.compute_capabilities[0].0 >= 7 {
            config.use_tensor_cores = true;
        }

        kernel.set_config(config);
        Ok(kernel)
    }

    /// Create a sparse FFT kernel optimized for the target GPU
    #[allow(clippy::too_many_arguments)]
    pub fn create_sparse_fft_kernel(
        &self,
        input_size: usize,
        sparsity: usize,
        input_address: usize,
        output_values_address: usize,
        output_indices_address: usize,
        algorithm: SparseFFTAlgorithm,
        window_function: WindowFunction,
    ) -> FFTResult<SparseFFTKernel> {
        let mut kernel = SparseFFTKernel::new(
            input_size,
            sparsity,
            input_address,
            output_values_address,
            output_indices_address,
            algorithm,
            window_function,
        );

        // Customize configuration based on GPU and algorithm
        let mut config = KernelConfig::default();

        // Optimize block _size based on algorithm
        config.block_size = match algorithm {
            SparseFFTAlgorithm::Sublinear => 256,
            SparseFFTAlgorithm::CompressedSensing => 512,
            SparseFFTAlgorithm::Iterative => 128,
            SparseFFTAlgorithm::Deterministic => 256,
            SparseFFTAlgorithm::FrequencyPruning => 256,
            SparseFFTAlgorithm::SpectralFlatness => 512,
        };

        // Ensure block _size is within GPU limits
        config.block_size = std::cmp::min(config.block_size, self.max_threads_per_block);

        // Calculate grid _size
        config.grid_size = input_size.div_ceil(config.block_size);

        // Optimize shared memory based on algorithm
        config.shared_memory_size = match algorithm {
            SparseFFTAlgorithm::Sublinear => 16 * 1024,
            SparseFFTAlgorithm::CompressedSensing => 32 * 1024,
            SparseFFTAlgorithm::Iterative => 8 * 1024,
            SparseFFTAlgorithm::Deterministic => 16 * 1024,
            SparseFFTAlgorithm::FrequencyPruning => 16 * 1024,
            SparseFFTAlgorithm::SpectralFlatness => 32 * 1024,
        };

        // Ensure shared memory is within GPU limits
        config.shared_memory_size =
            std::cmp::min(config.shared_memory_size, self.shared_memory_per_block);

        // Enable mixed precision for newer GPUs and certain algorithms
        if !self.compute_capabilities.is_empty()
            && (self.compute_capabilities[0].0 >= 7
                || (self.compute_capabilities[0].0 == 6 && self.compute_capabilities[0].1 >= 1))
        {
            // Only enable for algorithms that can benefit without significant accuracy loss
            match algorithm {
                SparseFFTAlgorithm::Sublinear
                | SparseFFTAlgorithm::Deterministic
                | SparseFFTAlgorithm::FrequencyPruning => {
                    config.use_mixed_precision = true;
                }
                _ => {
                    config.use_mixed_precision = false;
                }
            }
        }

        // Enable tensor cores for supported architectures and algorithms
        if !self.compute_capabilities.is_empty() && self.compute_capabilities[0].0 >= 7 {
            // Only enable for algorithms that can benefit from tensor cores
            match algorithm {
                SparseFFTAlgorithm::CompressedSensing | SparseFFTAlgorithm::SpectralFlatness => {
                    config.use_tensor_cores = true;
                }
                _ => {
                    config.use_tensor_cores = false;
                }
            }
        }

        kernel.set_config(config);
        Ok(kernel)
    }

    /// Check if there's enough memory for the requested operation
    pub fn check_memory_requirements(&self, total_bytesneeded: usize) -> FFTResult<()> {
        if total_bytesneeded > self.available_memory {
            return Err(FFTError::MemoryError(format!(
                "Not enough GPU memory: need {} bytes, available {} bytes",
                total_bytesneeded, self.available_memory
            )));
        }

        Ok(())
    }
}

/// Kernel launcher for executing kernels with optimal parameters
pub struct KernelLauncher {
    /// Kernel factory for creating optimized kernels
    factory: KernelFactory,
    /// Active kernels
    active_kernels: Vec<Box<dyn GPUKernel>>,
    /// Total memory allocated
    total_memory_allocated: usize,
}

impl KernelLauncher {
    /// Create a new kernel launcher
    pub fn new(factory: KernelFactory) -> Self {
        Self {
            factory,
            active_kernels: Vec::new(),
            total_memory_allocated: 0,
        }
    }

    /// Allocate memory for FFT operation
    pub fn allocate_fft_memory(&mut self, inputsize: usize) -> FFTResult<(usize, usize)> {
        let element_size = std::mem::size_of::<Complex64>();
        let input_bytes = inputsize * element_size;
        let output_bytes = inputsize * element_size;

        let total_bytes = input_bytes + output_bytes;
        self.factory.check_memory_requirements(total_bytes)?;

        // In a real implementation, this would allocate actual GPU memory
        // For now, just return dummy addresses
        let input_address = 0x10000;
        let output_address = 0x20000;

        self.total_memory_allocated += total_bytes;

        Ok((input_address, output_address))
    }

    /// Allocate memory for sparse FFT operation
    pub fn allocate_sparse_fft_memory(
        &mut self,
        input_size: usize,
        sparsity: usize,
    ) -> FFTResult<(usize, usize, usize)> {
        let element_size = std::mem::size_of::<Complex64>();
        let index_size = std::mem::size_of::<usize>();

        let input_bytes = input_size * element_size;
        let output_values_bytes = sparsity * element_size;
        let output_indices_bytes = sparsity * index_size;

        let total_bytes = input_bytes + output_values_bytes + output_indices_bytes;
        self.factory.check_memory_requirements(total_bytes)?;

        // In a real implementation, this would allocate actual GPU memory
        // For now, just return dummy addresses
        let input_address = 0x10000;
        let output_values_address = 0x20000;
        let output_indices_address = 0x30000;

        self.total_memory_allocated += total_bytes;

        Ok((input_address, output_values_address, output_indices_address))
    }

    /// Launch FFT kernel
    pub fn launch_fft_kernel(
        &mut self,
        input_size: usize,
        input_address: usize,
        output_address: usize,
    ) -> FFTResult<KernelStats> {
        let kernel = self
            .factory
            .create_fft_kernel(input_size, input_address, output_address)?;

        let stats = kernel.execute()?;

        // In a real implementation, we would keep track of the kernel
        // self.active_kernels.push(Box::new(kernel));

        Ok(stats)
    }

    /// Launch sparse FFT kernel
    #[allow(clippy::too_many_arguments)]
    pub fn launch_sparse_fft_kernel(
        &mut self,
        input_size: usize,
        sparsity: usize,
        input_address: usize,
        output_values_address: usize,
        output_indices_address: usize,
        algorithm: SparseFFTAlgorithm,
        window_function: WindowFunction,
    ) -> FFTResult<KernelStats> {
        let kernel = self.factory.create_sparse_fft_kernel(
            input_size,
            sparsity,
            input_address,
            output_values_address,
            output_indices_address,
            algorithm,
            window_function,
        )?;

        // Apply window _function if needed
        if window_function != WindowFunction::None {
            // Launch window kernel first
            kernel.apply_window()?;
        }

        let stats = kernel.execute()?;

        // In a real implementation, we would keep track of the kernel
        // self.active_kernels.push(Box::new(kernel));

        Ok(stats)
    }

    /// Get total memory allocated
    pub fn get_total_memory_allocated(&self) -> usize {
        self.total_memory_allocated
    }

    /// Free all allocated memory
    pub fn free_all_memory(&mut self) {
        // In a real implementation, this would free all GPU memory
        self.active_kernels.clear();
        self.total_memory_allocated = 0;
    }
}

/// Execute sparse FFT on GPU using optimized kernels
///
/// This function provides a high-level interface to the GPU kernel implementation,
/// handling memory allocation, kernel execution, and result collection.
///
/// # Arguments
///
/// * `signal` - Input signal
/// * `sparsity` - Expected number of significant frequency components
/// * `algorithm` - Sparse FFT algorithm to use
/// * `window_function` - Window function to apply
/// * `gpu_arch` - GPU architecture name
/// * `compute_capability` - GPU compute capability
/// * `available_memory` - Available GPU memory in bytes
///
/// # Returns
///
/// * Result containing sparse frequency components and kernel statistics
#[allow(clippy::too_many_arguments)]
#[allow(dead_code)]
pub fn execute_sparse_fft_kernel<T>(
    signal: &[T],
    sparsity: usize,
    algorithm: SparseFFTAlgorithm,
    window_function: WindowFunction,
    gpu_arch: &str,
    compute_capability: (i32, i32),
    available_memory: usize,
) -> FFTResult<(Vec<Complex64>, Vec<usize>, KernelStats)>
where
    T: NumCast + Copy + Debug + 'static,
{
    // Create kernel factory
    let factory = KernelFactory::new(
        gpu_arch.to_string(),
        vec![compute_capability],
        available_memory,
        48 * 1024, // 48 KB shared _memory per block
        1024,      // 1024 threads per block
    );

    // Create kernel launcher
    let mut launcher = KernelLauncher::new(factory);

    // Allocate _memory
    let (input_address, output_values_address, output_indices_address) =
        launcher.allocate_sparse_fft_memory(signal.len(), sparsity)?;

    // In a real implementation, this would copy the signal to GPU _memory

    // Launch kernel
    let stats = launcher.launch_sparse_fft_kernel(
        signal.len(),
        sparsity,
        input_address,
        output_values_address,
        output_indices_address,
        algorithm,
        window_function,
    )?;

    // In a real implementation, this would copy the results back from GPU _memory
    // For now, just return dummy data

    // Create dummy frequency components
    let mut values = Vec::with_capacity(sparsity);
    let mut indices = Vec::with_capacity(sparsity);

    for i in 0..sparsity {
        let idx = i * (signal.len() / sparsity);
        let val = Complex64::new(1.0 / (i + 1) as f64, 0.0);

        values.push(val);
        indices.push(idx);
    }

    // Free _memory
    launcher.free_all_memory();

    Ok((values, indices, stats))
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::f64::consts::PI;

    // Helper function to create a sparse signal
    fn create_sparse_signal(n: usize, frequencies: &[(usize, f64)]) -> Vec<f64> {
        let mut signal = vec![0.0; n];

        for i in 0..n {
            let t = 2.0 * PI * (i as f64) / (n as f64);
            for &(freq, amp) in frequencies {
                signal[i] += amp * (freq as f64 * t).sin();
            }
        }

        signal
    }

    #[test]
    fn test_kernel_factory() {
        // Check if GPU is available
        let caps = PlatformCapabilities::detect();
        if !caps.cuda_available && !caps.gpu_available {
            // Mock test for CPU-only environments
            eprintln!("GPU not available, using mock kernel factory test");
            // Test factory creation still works
            let factory = KernelFactory::new(
                "Mock Device".to_string(),
                vec![(1, 1)],
                1024 * 1024, // 1 MB
                16 * 1024,   // 16 KB
                32,          // 32 threads
            );
            assert!(factory.arch.contains("Mock"));
            return;
        }

        let factory = KernelFactory::new(
            "NVIDIA GeForce RTX 3080".to_string(),
            vec![(8, 6)],
            10 * 1024 * 1024 * 1024, // 10 GB
            48 * 1024,               // 48 KB
            1024,                    // 1024 threads per block
        );

        // Test creating FFT kernel
        let kernel = factory.create_fft_kernel(1024, 0x10000, 0x20000).unwrap();

        // Check configuration
        let config = kernel.config();
        assert_eq!(config.block_size, 1024);
        assert!(config.use_mixed_precision);
        assert!(config.use_tensor_cores);

        // Test creating sparse FFT kernel
        let kernel = factory
            .create_sparse_fft_kernel(
                1024,
                10,
                0x10000,
                0x20000,
                0x30000,
                SparseFFTAlgorithm::Sublinear,
                WindowFunction::Hann,
            )
            .unwrap();

        // Check configuration
        let config = kernel.config();
        assert_eq!(config.block_size, 256);
        assert!(config.use_mixed_precision);
    }

    #[test]
    fn test_kernel_launcher() {
        // Check if GPU is available
        let caps = PlatformCapabilities::detect();
        if !caps.cuda_available && !caps.gpu_available {
            // Mock test for CPU-only environments
            eprintln!("GPU not available, using mock kernel launcher test");
            let factory = KernelFactory::new(
                "Mock Device".to_string(),
                vec![(1, 1)],
                1024 * 1024,
                16 * 1024,
                32,
            );
            let launcher = KernelLauncher::new(factory);
            // Test that launcher is created successfully
            assert_eq!(launcher.get_total_memory_allocated(), 0);
            return;
        }

        let factory = KernelFactory::new(
            "NVIDIA GeForce RTX 3080".to_string(),
            vec![(8, 6)],
            10 * 1024 * 1024 * 1024, // 10 GB
            48 * 1024,               // 48 KB
            1024,                    // 1024 threads per block
        );

        let mut launcher = KernelLauncher::new(factory);

        // Test allocating memory
        let (input_address, output_address) = launcher.allocate_fft_memory(1024).unwrap();
        assert_ne!(input_address, 0);
        assert_ne!(output_address, 0);

        // Test launching FFT kernel
        let stats = launcher
            .launch_fft_kernel(1024, input_address, output_address)
            .unwrap();

        // Check stats
        assert!(stats.execution_time_ms > 0.0);
        assert!(stats.memory_bandwidth_gb_s > 0.0);
        assert!(stats.compute_throughput_gflops > 0.0);

        // Test freeing memory
        launcher.free_all_memory();
        assert_eq!(launcher.get_total_memory_allocated(), 0);
    }

    #[test]
    fn test_execute_sparse_fft_kernel() {
        // Create a sparse signal
        let n = 1024;
        let frequencies = vec![(3, 1.0), (7, 0.5), (15, 0.25)];
        let signal = create_sparse_signal(n, &frequencies);

        // Check if GPU is available
        let caps = PlatformCapabilities::detect();
        if !caps.cuda_available && !caps.gpu_available {
            // Mock test for CPU-only environments
            eprintln!("GPU not available, using mock sparse FFT kernel test");
            // Test with mock device
            let result = execute_sparse_fft_kernel(
                &signal,
                6,
                SparseFFTAlgorithm::Sublinear,
                WindowFunction::Hann,
                "Mock Device",
                (1, 1),
                1024 * 1024, // 1 MB
            );
            // In mock mode, this should still return valid dummy data
            let (values, indices, stats) = result.unwrap();
            assert_eq!(values.len(), 6);
            assert_eq!(indices.len(), 6);
            assert!(stats.execution_time_ms >= 0.0);
            return;
        }

        // Execute sparse FFT kernel with GPU
        let (values, indices, stats) = execute_sparse_fft_kernel(
            &signal,
            6,
            SparseFFTAlgorithm::Sublinear,
            WindowFunction::Hann,
            "NVIDIA GeForce RTX 3080",
            (8, 6),
            10 * 1024 * 1024 * 1024, // 10 GB
        )
        .unwrap();

        // Check results
        assert_eq!(values.len(), 6);
        assert_eq!(indices.len(), 6);
        assert!(stats.execution_time_ms > 0.0);
    }
}
