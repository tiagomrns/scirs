// Enhanced validation functions for Lomb-Scargle periodogram
//
// This module provides comprehensive enhanced validation including SciPy reference
// comparison, noise robustness testing, memory efficiency analysis, and SIMD consistency checks.

use crate::error::{SignalError, SignalResult};
use crate::lombscargle::lombscargle;
use crate::lombscargle_enhanced::{lombscargle_enhanced, LombScargleConfig, WindowType};
use super::types::{EnhancedValidationConfig, EnhancedValidationResult, SciPyComparisonResult};
use super::analytical::validate_analytical_cases;
use rand::prelude::*;
use rand::Rng;
use std::time::Instant;

/// Enhanced validation with comprehensive testing including SciPy reference comparison,
/// noise robustness, memory efficiency, and SIMD consistency.
///
/// This function provides the most thorough validation of Lomb-Scargle implementations,
/// going beyond basic analytical tests to include real-world scenario validation.
///
/// # Arguments
///
/// * `config` - Enhanced validation configuration
///
/// # Returns
///
/// * Enhanced validation result with comprehensive metrics
#[allow(dead_code)]
pub fn validate_lombscargle_enhanced(
    config: &EnhancedValidationConfig,
) -> SignalResult<EnhancedValidationResult> {
    let mut recommendations = Vec::new();

    // 1. Run basic validation
    let base_validation = validate_analytical_cases("enhanced", config.tolerance)?;

    // 2. SciPy reference comparison
    let scipy_comparison = if config.test_scipy_reference {
        validate_against_scipy_reference(config)?
    } else {
        SciPyComparisonResult {
            max_relative_error: 0.0,
            mean_relative_error: 0.0,
            correlation: 1.0,
            peak_detection_accuracy: 100.0,
        }
    };

    // 3. Noise robustness testing
    let noise_robustness = if config.test_with_noise {
        validate_noise_robustness(config)?
    } else {
        95.0
    };

    // 4. Memory efficiency testing
    let memory_efficiency = if config.test_memory_efficiency {
        validate_memory_efficiency(config)?
    } else {
        95.0
    };

    // 5. SIMD consistency testing
    let simd_consistency = if config.test_simd_consistency {
        validate_simd_consistency(config)?
    } else {
        95.0
    };

    // Calculate overall score
    let overall_score = calculate_enhanced_overall_score(
        &base_validation,
        &scipy_comparison,
        noise_robustness,
        memory_efficiency,
        simd_consistency,
    );

    // Generate recommendations
    if scipy_comparison.max_relative_error > config.tolerance * 100.0 {
        recommendations.push(
            "Large discrepancy with SciPy reference detected. Review algorithm implementation."
                .to_string(),
        );
    }

    if noise_robustness < 80.0 {
        recommendations.push(
            "Poor noise robustness. Consider implementing better preprocessing or regularization."
                .to_string(),
        );
    }

    if memory_efficiency < 75.0 {
        recommendations.push(
            "Memory usage is suboptimal. Consider chunked processing for large datasets."
                .to_string(),
        );
    }

    if simd_consistency < 90.0 {
        recommendations.push(
            "SIMD implementation inconsistency detected. Review SIMD code paths.".to_string(),
        );
    }

    Ok(EnhancedValidationResult {
        base_validation,
        scipy_comparison,
        noise_robustness,
        memory_efficiency,
        simd_consistency,
        overall_score,
        recommendations,
    })
}

/// Validate against SciPy reference implementation
#[allow(dead_code)]
fn validate_against_scipy_reference(
    config: &EnhancedValidationConfig,
) -> SignalResult<SciPyComparisonResult> {
    let mut relative_errors = Vec::new();
    let mut correlations = Vec::new();
    let mut peak_accuracies = Vec::new();

    for _ in 0..config.n_iterations {
        // Generate test signal
        let n = 1000;
        let t: Vec<f64> = (0..n)
            .map(|i| i as f64 * 0.01 + rand::rng().random_range(0.0..0.001))
            .collect();
        let freq1 = 0.5;
        let freq2 = 1.5;
        let signal: Vec<f64> = t
            .iter()
            .map(|&ti| (2.0 * std::f64::consts::PI * freq1 * ti).sin() + 0.5 * (2.0 * std::f64::consts::PI * freq2 * ti).sin())
            .collect();

        // Test frequencies
        let freqs: Vec<f64> = (1..=200).map(|i| i as f64 * 0.01).collect();

        // Our implementation
        let (test_freqs, test_power) = lombscargle(&t, &signal, Some(&freqs))?;

        // Reference implementation (simplified SciPy-like algorithm)
        let ref_power = scipy_reference_lombscargle(&t, &signal, &freqs)?;

        // Compare results
        let errors: Vec<f64> = test_power
            .iter()
            .zip(ref_power.iter())
            .filter_map(|(&test, &ref_val)| {
                if ref_val > 1e-10 {
                    Some((test - ref_val).abs() / ref_val)
                } else {
                    None
                }
            })
            .collect();

        if !errors.is_empty() {
            relative_errors.extend(errors);

            // Calculate correlation
            let correlation = calculate_correlation(&test_power, &ref_power);
            correlations.push(correlation);

            // Calculate peak detection accuracy
            let peak_accuracy =
                calculate_peak_detection_accuracy(&test_power, &ref_power, &freqs, &[freq1, freq2]);
            peak_accuracies.push(peak_accuracy);
        }
    }

    let max_relative_error = relative_errors.iter().fold(0.0, |a, &b| a.max(b));
    let mean_relative_error = relative_errors.iter().sum::<f64>() / relative_errors.len() as f64;
    let mean_correlation = correlations.iter().sum::<f64>() / correlations.len() as f64;
    let mean_peak_accuracy = peak_accuracies.iter().sum::<f64>() / peak_accuracies.len() as f64;

    Ok(SciPyComparisonResult {
        max_relative_error,
        mean_relative_error,
        correlation: mean_correlation,
        peak_detection_accuracy: mean_peak_accuracy,
    })
}

/// Simplified SciPy-like reference implementation for comparison
#[allow(dead_code)]
fn scipy_reference_lombscargle(t: &[f64], y: &[f64], freqs: &[f64]) -> SignalResult<Vec<f64>> {
    let n = t.len();
    let mut power = vec![0.0; freqs.len()];

    // Center the data
    let y_mean = y.iter().sum::<f64>() / n as f64;
    let y_centered: Vec<f64> = y.iter().map(|&yi| yi - y_mean).collect();

    for (i, &freq) in freqs.iter().enumerate() {
        let omega = 2.0 * std::f64::consts::PI * freq;

        // Calculate tau (time offset to make periodogram independent of time origin)
        let sum_sin2wt: f64 = t.iter().map(|&ti| (2.0 * omega * ti).sin()).sum();
        let sum_cos2wt: f64 = t.iter().map(|&ti| (2.0 * omega * ti).cos()).sum();
        let tau = (sum_sin2wt / sum_cos2wt).atan() / (2.0 * omega);

        // Calculate periodogram components
        let mut sum_cos_num = 0.0;
        let mut sum_cos_den = 0.0;
        let mut sum_sin_num = 0.0;
        let mut sum_sin_den = 0.0;

        for j in 0..n {
            let phase = omega * (t[j] - tau);
            let cos_phase = phase.cos();
            let sin_phase = phase.sin();

            sum_cos_num += y_centered[j] * cos_phase;
            sum_cos_den += cos_phase * cos_phase;
            sum_sin_num += y_centered[j] * sin_phase;
            sum_sin_den += sin_phase * sin_phase;
        }

        // Lomb-Scargle periodogram
        let cos_term = if sum_cos_den > 1e-15 {
            (sum_cos_num * sum_cos_num) / sum_cos_den
        } else {
            0.0
        };

        let sin_term = if sum_sin_den > 1e-15 {
            (sum_sin_num * sum_sin_num) / sum_sin_den
        } else {
            0.0
        };

        power[i] = 0.5 * (cos_term + sin_term);
    }

    Ok(power)
}

/// Validate noise robustness
#[allow(dead_code)]
fn validate_noise_robustness(config: &EnhancedValidationConfig) -> SignalResult<f64> {
    let mut robustness_scores = Vec::new();
    let snr_linear = 10.0_f64.powf(config.noise_snr_db / 10.0);

    for _ in 0..config.n_iterations {
        // Clean signal
        let n = 500;
        let t: Vec<f64> = (0..n).map(|i| i as f64 * 0.02).collect();
        let freq = 1.0;
        let clean_signal: Vec<f64> = t.iter().map(|&ti| (2.0 * std::f64::consts::PI * freq * ti).sin()).collect();

        // Add noise
        let signal_power = clean_signal.iter().map(|&x| x * x).sum::<f64>() / n as f64;
        let noise_power = signal_power / snr_linear;
        let noise_std = noise_power.sqrt();

        let mut rng = rand::rng();
        let noisy_signal: Vec<f64> = clean_signal
            .iter()
            .map(|&s| s + noise_std * rng.gen_range(-1.0..1.0))
            .collect();

        // Test frequencies around the true frequency
        let freqs: Vec<f64> = (80..120).map(|i| i as f64 * 0.01).collect();

        // Compute periodogram
        match lombscargle(&t, &noisy_signal, Some(&freqs)) {
            Ok((_, power)) => {
                // Find peak
                let peak_idx = power
                    .iter()
                    .enumerate()
                    .max_by(|a, b| a.1.partial_cmp(b.1).unwrap())
                    .map(|(i, _)| i)
                    .unwrap_or(0);

                let detected_freq = freqs[peak_idx];
                let freq_error = (detected_freq - freq).abs() / freq;

                // Score based on frequency accuracy
                let score = if freq_error < 0.01 {
                    100.0
                } else if freq_error < 0.05 {
                    80.0
                } else if freq_error < 0.1 {
                    60.0
                } else {
                    40.0
                };

                robustness_scores.push(score);
            }
            Err(_) => {
                robustness_scores.push(0.0);
            }
        }
    }

    let mean_score = robustness_scores.iter().sum::<f64>() / robustness_scores.len() as f64;
    Ok(mean_score)
}

/// Validate memory efficiency
#[allow(dead_code)]
fn validate_memory_efficiency(config: &EnhancedValidationConfig) -> SignalResult<f64> {
    // Simple memory efficiency test - in a real implementation,
    // this would measure actual memory usage
    let large_n = 10000;
    let t: Vec<f64> = (0..large_n).map(|i| i as f64 * 0.001).collect();
    let signal: Vec<f64> = t.iter().map(|&ti| (2.0 * std::f64::consts::PI * 0.5 * ti).sin()).collect();
    let freqs: Vec<f64> = (1..1000).map(|i| i as f64 * 0.001).collect();

    // Test if large signal processing works
    match lombscargle(&t, &signal, Some(&freqs)) {
        Ok(_) => Ok(90.0),  // Good efficiency if it completes
        Err(_) => Ok(50.0), // Poor efficiency if it fails
    }
}

/// Validate SIMD consistency
#[allow(dead_code)]
fn validate_simd_consistency(config: &EnhancedValidationConfig) -> SignalResult<f64> {
    let mut consistency_scores = Vec::new();

    for _ in 0..10 {
        // Generate test signal
        let n = 1000;
        let t: Vec<f64> = (0..n).map(|i| i as f64 * 0.01).collect();
        let signal: Vec<f64> = t.iter().map(|&ti| (2.0 * std::f64::consts::PI * 0.5 * ti).sin()).collect();
        let freqs: Vec<f64> = (1..200).map(|i| i as f64 * 0.01).collect();

        // Test standard implementation
        let result1 = lombscargle(&t, &signal, Some(&freqs))?;

        // Test enhanced implementation (which may use SIMD)
        let ls_config = LombScargleConfig {
            normalize: true,
            center: true,
            window: Some(WindowType::Hann),
            detrend: false,
        };
        let result2 = lombscargle_enhanced(&t, &signal, Some(&freqs), &ls_config)?;

        // Compare results
        let errors: Vec<f64> = result1
            .1
            .iter()
            .zip(result2.1.iter())
            .filter_map(|(&p1, &p2)| {
                if p1 > 1e-15 {
                    Some((p1 - p2).abs() / p1)
                } else {
                    None
                }
            })
            .collect();

        if !errors.is_empty() {
            let max_error = errors.iter().fold(0.0, |a, &b| a.max(b));
            let score = if max_error < config.tolerance * 10.0 {
                100.0
            } else if max_error < config.tolerance * 100.0 {
                80.0
            } else {
                60.0
            };
            consistency_scores.push(score);
        }
    }

    let mean_score = consistency_scores.iter().sum::<f64>() / consistency_scores.len() as f64;
    Ok(mean_score)
}

/// Calculate correlation between two vectors
#[allow(dead_code)]
fn calculate_correlation(x: &[f64], y: &[f64]) -> f64 {
    let n = x.len().min(y.len()) as f64;
    let mean_x = x.iter().sum::<f64>() / n;
    let mean_y = y.iter().sum::<f64>() / n;

    let mut cov = 0.0;
    let mut var_x = 0.0;
    let mut var_y = 0.0;

    for i in 0..n as usize {
        let dx = x[i] - mean_x;
        let dy = y[i] - mean_y;
        cov += dx * dy;
        var_x += dx * dx;
        var_y += dy * dy;
    }

    if var_x > 1e-15 && var_y > 1e-15 {
        cov / (var_x * var_y).sqrt()
    } else {
        0.0
    }
}

/// Calculate peak detection accuracy
#[allow(dead_code)]
fn calculate_peak_detection_accuracy(
    test_power: &[f64],
    ref_power: &[f64],
    freqs: &[f64],
    true_freqs: &[f64],
) -> f64 {
    let mut accuracies = Vec::new();

    for &true_freq in true_freqs {
        // Find peaks in both power spectra
        let test_peak_idx = find_peak_near_frequency(test_power, freqs, true_freq);
        let ref_peak_idx = find_peak_near_frequency(ref_power, freqs, true_freq);

        if let (Some(test_idx), Some(ref_idx)) = (test_peak_idx, ref_peak_idx) {
            let freq_diff = (freqs[test_idx] - freqs[ref_idx]).abs();
            let freq_resolution = if freqs.len() > 1 {
                freqs[1] - freqs[0]
            } else {
                0.01
            };

            let accuracy = if freq_diff < freq_resolution {
                100.0
            } else if freq_diff < 2.0 * freq_resolution {
                80.0
            } else {
                60.0
            };

            accuracies.push(accuracy);
        }
    }

    if !accuracies.is_empty() {
        accuracies.iter().sum::<f64>() / accuracies.len() as f64
    } else {
        0.0
    }
}

/// Find peak near a specific frequency
#[allow(dead_code)]
fn find_peak_near_frequency(power: &[f64], freqs: &[f64], target_freq: f64) -> Option<usize> {
    let tolerance = 0.1; // 10% tolerance
    let mut best_idx = None;
    let mut best_power = 0.0;

    for (i, (&freq, &pow)) in freqs.iter().zip(power.iter()).enumerate() {
        if (freq - target_freq).abs() / target_freq < tolerance && pow > best_power {
            best_power = pow;
            best_idx = Some(i);
        }
    }

    best_idx
}

/// Calculate enhanced overall score
#[allow(dead_code)]
fn calculate_enhanced_overall_score(
    base_validation: &super::types::ValidationResult,
    scipy_comparison: &SciPyComparisonResult,
    noise_robustness: f64,
    memory_efficiency: f64,
    simd_consistency: f64,
) -> f64 {
    // Convert base validation stability score to 0-100 scale
    let base_score = base_validation.stability_score * 100.0;

    // Convert scipy comparison correlation to score
    let scipy_score = scipy_comparison.correlation * 100.0;

    // Weighted average of all components
    let overall_score = (base_score * 0.3)
        + (scipy_score * 0.25)
        + (noise_robustness * 0.2)
        + (memory_efficiency * 0.15)
        + (simd_consistency * 0.1);

    overall_score.min(100.0).max(0.0)
}