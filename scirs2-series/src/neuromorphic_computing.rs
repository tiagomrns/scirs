//! Neuromorphic Computing for Advanced Time Series Analysis
//!
//! This module implements neuromorphic computing paradigms for time series processing,
//! including spiking neural networks, liquid state machines, reservoir computing,
//! and bio-inspired temporal processing architectures.
//!
//! ## Neuromorphic Architectures
//! - **Spiking Neural Networks**: Event-driven processing with temporal dynamics
//! - **Liquid State Machines**: Random recurrent networks with readout layers
//! - **Reservoir Computing**: Echo state networks optimized for temporal patterns
//! - **Memristive Networks**: Hardware-aware plastic synaptic connections
//! - **Neuromorphic Chips Simulation**: Intel Loihi and IBM TrueNorth-style processing

use ndarray::{Array1, Array2};
use num_traits::{Float, FromPrimitive};
use std::collections::{HashMap, VecDeque};
use std::fmt::Debug;

use crate::error::{Result, TimeSeriesError};

/// Spiking neuron models for neuromorphic computation
#[derive(Debug, Clone)]
pub enum NeuronModel {
    /// Leaky Integrate-and-Fire neuron
    LeakyIntegrateFire {
        /// Membrane time constant
        tau_m: f64,
        /// Resting potential
        v_rest: f64,
        /// Spike threshold
        v_threshold: f64,
        /// Reset potential
        v_reset: f64,
    },
    /// Adaptive Exponential Integrate-and-Fire
    AdaptiveExpIF {
        /// Membrane time constant
        tau_m: f64,
        /// Adaptation time constant
        tau_w: f64,
        /// Slope factor
        delta_t: f64,
        /// Spike threshold
        v_threshold: f64,
        /// Subthreshold adaptation
        a: f64,
        /// Spike-triggered adaptation
        b: f64,
    },
    /// Izhikevich neuron model
    Izhikevich {
        /// Recovery variable time scale
        a: f64,
        /// Sensitivity of recovery variable
        b: f64,
        /// After-spike reset value of membrane potential
        c: f64,
        /// After-spike reset increment for recovery variable
        d: f64,
    },
    /// Hodgkin-Huxley simplified
    HodgkinHuxley {
        /// Sodium conductance
        g_na: f64,
        /// Potassium conductance
        g_k: f64,
        /// Leak conductance
        g_l: f64,
        /// Sodium reversal potential
        e_na: f64,
        /// Potassium reversal potential
        e_k: f64,
        /// Leak reversal potential
        e_l: f64,
    },
}

/// Synaptic plasticity rules for learning
#[derive(Debug, Clone)]
pub enum PlasticityRule {
    /// Spike-Timing Dependent Plasticity
    STDP {
        /// LTP time constant
        tau_plus: f64,
        /// LTD time constant
        tau_minus: f64,
        /// LTP amplitude
        a_plus: f64,
        /// LTD amplitude
        a_minus: f64,
    },
    /// Rate-based Hebbian learning
    Hebbian {
        /// Learning rate parameter
        learning_rate: f64,
        /// Decay rate parameter
        decay_rate: f64,
    },
    /// Homeostatic plasticity
    Homeostatic {
        /// Target firing rate
        target_rate: f64,
        /// Homeostatic time constant
        tau_h: f64,
        /// Scaling factor
        alpha: f64,
    },
    /// Triplet STDP for complex temporal patterns
    TripletSTDP {
        /// Positive time constant
        tau_plus: f64,
        /// Negative time constant
        tau_minus: f64,
        /// Triplet time constant
        tau_x: f64,
        /// Pair LTP amplitude
        a2_plus: f64,
        /// Pair LTD amplitude
        a2_minus: f64,
        /// Triplet LTP amplitude
        a3_plus: f64,
        /// Triplet LTD amplitude
        a3_minus: f64,
    },
}

/// Spike representation for event-driven processing
#[derive(Debug, Clone)]
pub struct Spike {
    /// Time of spike occurrence
    pub time: f64,
    /// Neuron ID that spiked
    pub neuron_id: usize,
    /// Spike amplitude (optional for variable amplitude spikes)
    pub amplitude: f64,
}

/// Neuromorphic neuron state
#[derive(Debug, Clone)]
pub struct NeuronState<F: Float> {
    /// Membrane potential
    pub v: F,
    /// Recovery/adaptation variable
    pub u: F,
    /// Last spike time
    pub last_spike: Option<f64>,
    /// Refractory period remaining
    pub refractory: f64,
    /// Input current
    pub input_current: F,
}

impl<F: Float + FromPrimitive> Default for NeuronState<F> {
    fn default() -> Self {
        Self {
            v: F::from(-70.0).unwrap(), // Resting potential in mV
            u: F::zero(),
            last_spike: None,
            refractory: 0.0,
            input_current: F::zero(),
        }
    }
}

/// Spiking Neural Network for time series processing
#[derive(Debug)]
pub struct SpikingNeuralNetwork<F: Float + Debug> {
    /// Number of neurons in each layer
    #[allow(dead_code)]
    layer_sizes: Vec<usize>,
    /// Neuron models for each layer
    #[allow(dead_code)]
    neuron_models: Vec<NeuronModel>,
    /// Current neuron states
    #[allow(dead_code)]
    neuron_states: Vec<Vec<NeuronState<F>>>,
    /// Synaptic weight matrices between layers
    #[allow(dead_code)]
    weights: Vec<Array2<F>>,
    /// Synaptic delays between neurons
    #[allow(dead_code)]
    delays: Vec<Array2<f64>>,
    /// Plasticity rules for each layer
    #[allow(dead_code)]
    plasticity_rules: Vec<PlasticityRule>,
    /// Spike history for STDP
    #[allow(dead_code)]
    spike_history: VecDeque<Spike>,
    /// Time step for simulation
    #[allow(dead_code)]
    dt: f64,
    /// Current simulation time
    #[allow(dead_code)]
    current_time: f64,
}

impl<F: Float + Debug + Clone + FromPrimitive + std::iter::Sum> SpikingNeuralNetwork<F> {
    /// Create new spiking neural network
    pub fn new(
        layer_sizes: Vec<usize>,
        neuron_models: Vec<NeuronModel>,
        plasticity_rules: Vec<PlasticityRule>,
        dt: f64,
    ) -> Result<Self> {
        if layer_sizes.len() != neuron_models.len()
            || layer_sizes.len() != plasticity_rules.len() + 1
        {
            return Err(TimeSeriesError::InvalidInput(
                "Inconsistent layer configuration".to_string(),
            ));
        }

        let num_layers = layer_sizes.len();

        // Initialize neuron states
        let mut neuron_states = Vec::new();
        for &size in &layer_sizes {
            neuron_states.push(vec![NeuronState::default(); size]);
        }

        // Initialize weights between layers
        let mut weights = Vec::new();
        let mut delays = Vec::new();

        for i in 0..num_layers - 1 {
            let rows = layer_sizes[i + 1];
            let cols = layer_sizes[i];

            // Random weight initialization
            let mut weight_matrix = Array2::zeros((rows, cols));
            let mut delay_matrix = Array2::zeros((rows, cols));

            for row in 0..rows {
                for col in 0..cols {
                    // Initialize with small random weights
                    let weight = F::from((row + col * 17) % 1000).unwrap()
                        / F::from(1000.0).unwrap()
                        * F::from(0.1).unwrap()
                        - F::from(0.05).unwrap();
                    weight_matrix[[row, col]] = weight;

                    // Random delays between 1-10 ms
                    let delay = 1.0 + ((row + col * 23) % 900) as f64 / 100.0;
                    delay_matrix[[row, col]] = delay;
                }
            }

            weights.push(weight_matrix);
            delays.push(delay_matrix);
        }

        Ok(Self {
            layer_sizes,
            neuron_models,
            neuron_states,
            weights,
            delays,
            plasticity_rules,
            spike_history: VecDeque::new(),
            dt,
            current_time: 0.0,
        })
    }

    /// Encode time series data as spike trains
    pub fn encode_time_series(&self, data: &Array1<F>) -> Vec<Spike> {
        let mut spikes = Vec::new();
        let input_neurons = self.layer_sizes[0];

        // Rate coding: higher values generate more frequent spikes
        for (time_idx, &value) in data.iter().enumerate() {
            let time = time_idx as f64 * self.dt;

            // Distribute input across multiple neurons
            for neuron_idx in 0..input_neurons {
                // Each neuron represents a different aspect/range of the input
                let neuron_sensitivity = F::from(neuron_idx as f64 / input_neurons as f64).unwrap();
                let activation = (value - neuron_sensitivity).abs();

                // Convert to spike probability
                let spike_prob = (-activation * F::from(5.0).unwrap()).exp();

                // Generate spikes based on probability
                let random_val =
                    F::from(((time_idx + neuron_idx * 7) % 1000) as f64 / 1000.0).unwrap();
                if random_val < spike_prob {
                    spikes.push(Spike {
                        time,
                        neuron_id: neuron_idx,
                        amplitude: spike_prob.to_f64().unwrap_or(1.0),
                    });
                }
            }
        }

        spikes
    }

    /// Process input spikes through the network
    pub fn process_spikes(&mut self, inputspikes: &[Spike]) -> Result<Vec<Spike>> {
        let mut output_spikes = Vec::new();

        // Process each input spike
        for spike in inputspikes {
            self.current_time = spike.time;

            // Apply input to first layer
            if spike.neuron_id < self.layer_sizes[0] {
                self.neuron_states[0][spike.neuron_id].input_current =
                    self.neuron_states[0][spike.neuron_id].input_current
                        + F::from(spike.amplitude).unwrap();
            }

            // Update all neurons and propagate _spikes
            let layer_spikes = self.update_network()?;
            output_spikes.extend(layer_spikes);

            // Apply plasticity rules
            self.apply_plasticity(spike)?;
        }

        Ok(output_spikes)
    }

    /// Update all neurons in the network
    fn update_network(&mut self) -> Result<Vec<Spike>> {
        let mut all_spikes = Vec::new();

        for layer_idx in 0..self.layer_sizes.len() {
            let model = &self.neuron_models[layer_idx].clone();
            let mut layer_spikes = Vec::new();

            for neuron_idx in 0..self.layer_sizes[layer_idx] {
                let spiked = {
                    let state = &mut self.neuron_states[layer_idx][neuron_idx];

                    // Update neuron based on its model
                    let result =
                        Self::update_neuron_static(state, model, self.dt, self.current_time)?;

                    // Reset input current after processing
                    state.input_current = F::zero();

                    result
                };

                if spiked {
                    let spike = Spike {
                        time: self.current_time,
                        neuron_id: neuron_idx,
                        amplitude: 1.0,
                    };
                    layer_spikes.push(spike.clone());

                    // Propagate spike to next layer if not output layer
                    if layer_idx < self.layer_sizes.len() - 1 {
                        self.propagate_spike(layer_idx, neuron_idx)?;
                    }
                }
            }

            all_spikes.extend(layer_spikes);
        }

        Ok(all_spikes)
    }

    /// Update individual neuron based on its model
    #[allow(dead_code)]
    fn update_neuron(&self, state: &mut NeuronState<F>, model: &NeuronModel) -> Result<bool> {
        // Check refractory period
        if state.refractory > 0.0 {
            state.refractory -= self.dt;
            return Ok(false);
        }

        match model {
            NeuronModel::LeakyIntegrateFire {
                tau_m,
                v_rest,
                v_threshold,
                v_reset,
            } => {
                let tau_m_f = F::from(*tau_m).unwrap();
                let v_rest_f = F::from(*v_rest).unwrap();
                let v_threshold_f = F::from(*v_threshold).unwrap();
                let dt_f = F::from(self.dt).unwrap();

                // dV/dt = (v_rest - V + R*I) / tau_m
                let dv = ((v_rest_f - state.v + state.input_current) / tau_m_f) * dt_f;
                state.v = state.v + dv;

                // Check for spike
                if state.v >= v_threshold_f {
                    state.v = F::from(*v_reset).unwrap();
                    state.refractory = 2.0; // 2ms refractory period
                    state.last_spike = Some(self.current_time);
                    return Ok(true);
                }
            }

            NeuronModel::AdaptiveExpIF {
                tau_m,
                tau_w,
                delta_t,
                v_threshold,
                a,
                b,
            } => {
                let tau_m_f = F::from(*tau_m).unwrap();
                let tau_w_f = F::from(*tau_w).unwrap();
                let delta_t_f = F::from(*delta_t).unwrap();
                let v_threshold_f = F::from(*v_threshold).unwrap();
                let a_f = F::from(*a).unwrap();
                let b_f = F::from(*b).unwrap();
                let dt_f = F::from(self.dt).unwrap();

                // Exponential term
                let exp_term = delta_t_f * ((state.v - v_threshold_f) / delta_t_f).exp();

                // dV/dt = (-V + exp_term + I - u) / tau_m
                let dv = ((-state.v + exp_term + state.input_current - state.u) / tau_m_f) * dt_f;
                state.v = state.v + dv;

                // du/dt = (a*(V - v_rest) - u) / tau_w
                let du = ((a_f * state.v - state.u) / tau_w_f) * dt_f;
                state.u = state.u + du;

                // Check for spike
                if state.v >= v_threshold_f {
                    state.v = F::from(-70.0).unwrap(); // Reset to resting potential
                    state.u = state.u + b_f; // Spike-triggered adaptation
                    state.refractory = 2.0;
                    state.last_spike = Some(self.current_time);
                    return Ok(true);
                }
            }

            NeuronModel::Izhikevich { a, b, c, d } => {
                let a_f = F::from(*a).unwrap();
                let b_f = F::from(*b).unwrap();
                let dt_f = F::from(self.dt).unwrap();

                // dv/dt = 0.04*v^2 + 5*v + 140 - u + I
                let dv = (F::from(0.04).unwrap() * state.v * state.v
                    + F::from(5.0).unwrap() * state.v
                    + F::from(140.0).unwrap()
                    - state.u
                    + state.input_current)
                    * dt_f;
                state.v = state.v + dv;

                // du/dt = a*(b*v - u)
                let du = (a_f * (b_f * state.v - state.u)) * dt_f;
                state.u = state.u + du;

                // Check for spike
                if state.v >= F::from(30.0).unwrap() {
                    state.v = F::from(*c).unwrap();
                    state.u = state.u + F::from(*d).unwrap();
                    state.last_spike = Some(self.current_time);
                    return Ok(true);
                }
            }

            NeuronModel::HodgkinHuxley { .. } => {
                // Simplified HH model - just use LIF dynamics for now
                let tau_m = 20.0;
                let v_rest = -70.0;
                let v_threshold = -55.0;
                let v_reset = -70.0;

                let tau_m_f = F::from(tau_m).unwrap();
                let v_rest_f = F::from(v_rest).unwrap();
                let v_threshold_f = F::from(v_threshold).unwrap();
                let dt_f = F::from(self.dt).unwrap();

                let dv = ((v_rest_f - state.v + state.input_current) / tau_m_f) * dt_f;
                state.v = state.v + dv;

                if state.v >= v_threshold_f {
                    state.v = F::from(v_reset).unwrap();
                    state.refractory = 2.0;
                    state.last_spike = Some(self.current_time);
                    return Ok(true);
                }
            }
        }

        Ok(false)
    }

    /// Static version of update_neuron to avoid borrow checker issues
    fn update_neuron_static(
        state: &mut NeuronState<F>,
        model: &NeuronModel,
        dt: f64,
        current_time: f64,
    ) -> Result<bool> {
        // Check refractory period
        if state.refractory > 0.0 {
            state.refractory -= dt;
            return Ok(false);
        }

        match model {
            NeuronModel::LeakyIntegrateFire {
                tau_m,
                v_rest,
                v_threshold,
                v_reset,
            } => {
                let tau_m_f = F::from(*tau_m).unwrap();
                let v_rest_f = F::from(*v_rest).unwrap();
                let v_threshold_f = F::from(*v_threshold).unwrap();
                let dt_f = F::from(dt).unwrap();

                // dV/dt = (v_rest - V + R*I) / tau_m
                let dv = ((v_rest_f - state.v + state.input_current) / tau_m_f) * dt_f;
                state.v = state.v + dv;

                // Check for spike
                if state.v >= v_threshold_f {
                    state.v = F::from(*v_reset).unwrap();
                    state.refractory = 2.0; // 2ms refractory period
                    state.last_spike = Some(current_time);
                    return Ok(true);
                }
            }

            NeuronModel::AdaptiveExpIF {
                tau_m,
                tau_w,
                delta_t,
                v_threshold,
                a,
                b,
            } => {
                let tau_m_f = F::from(*tau_m).unwrap();
                let tau_w_f = F::from(*tau_w).unwrap();
                let delta_t_f = F::from(*delta_t).unwrap();
                let v_threshold_f = F::from(*v_threshold).unwrap();
                let a_f = F::from(*a).unwrap();
                let b_f = F::from(*b).unwrap();
                let dt_f = F::from(dt).unwrap();

                // Exponential term
                let exp_term = delta_t_f * ((state.v - v_threshold_f) / delta_t_f).exp();

                // dV/dt = (-V + exp_term + I - u) / tau_m
                let dv = ((-state.v + exp_term + state.input_current - state.u) / tau_m_f) * dt_f;
                state.v = state.v + dv;

                // du/dt = (a*(V - v_rest) - u) / tau_w
                let du = ((a_f * state.v - state.u) / tau_w_f) * dt_f;
                state.u = state.u + du;

                // Check for spike
                if state.v >= v_threshold_f {
                    state.v = F::from(-70.0).unwrap(); // Reset to resting potential
                    state.u = state.u + b_f; // Spike-triggered adaptation
                    state.refractory = 2.0;
                    state.last_spike = Some(current_time);
                    return Ok(true);
                }
            }

            NeuronModel::Izhikevich { a, b, c, d } => {
                let a_f = F::from(*a).unwrap();
                let b_f = F::from(*b).unwrap();
                let dt_f = F::from(dt).unwrap();

                // dv/dt = 0.04*v^2 + 5*v + 140 - u + I
                let dv = (F::from(0.04).unwrap() * state.v * state.v
                    + F::from(5.0).unwrap() * state.v
                    + F::from(140.0).unwrap()
                    - state.u
                    + state.input_current)
                    * dt_f;
                state.v = state.v + dv;

                // du/dt = a*(b*v - u)
                let du = (a_f * (b_f * state.v - state.u)) * dt_f;
                state.u = state.u + du;

                // Check for spike
                if state.v >= F::from(30.0).unwrap() {
                    state.v = F::from(*c).unwrap();
                    state.u = state.u + F::from(*d).unwrap();
                    state.last_spike = Some(current_time);
                    return Ok(true);
                }
            }

            NeuronModel::HodgkinHuxley { .. } => {
                // Simplified HH model - just use LIF dynamics for now
                let tau_m = 20.0;
                let v_rest = -70.0;
                let v_threshold = -55.0;
                let v_reset = -70.0;

                let tau_m_f = F::from(tau_m).unwrap();
                let v_rest_f = F::from(v_rest).unwrap();
                let v_threshold_f = F::from(v_threshold).unwrap();
                let dt_f = F::from(dt).unwrap();

                let dv = ((v_rest_f - state.v + state.input_current) / tau_m_f) * dt_f;
                state.v = state.v + dv;

                if state.v >= v_threshold_f {
                    state.v = F::from(v_reset).unwrap();
                    state.refractory = 2.0;
                    state.last_spike = Some(current_time);
                    return Ok(true);
                }
            }
        }

        Ok(false)
    }

    /// Propagate spike to next layer with synaptic delays
    fn propagate_spike(&mut self, layer_idx: usize, neuronidx: usize) -> Result<()> {
        if layer_idx >= self.weights.len() {
            return Ok(());
        }

        let weight_matrix = &self.weights[layer_idx];
        let delay_matrix = &self.delays[layer_idx];

        // Add weighted input to next layer neurons (with delay)
        for target_neuron in 0..self.layer_sizes[layer_idx + 1] {
            let weight = weight_matrix[[target_neuron, neuronidx]];
            let _delay = delay_matrix[[target_neuron, neuronidx]];

            // For simplicity, apply immediately (real implementation would use delay buffers)
            self.neuron_states[layer_idx + 1][target_neuron].input_current =
                self.neuron_states[layer_idx + 1][target_neuron].input_current + weight;
        }

        Ok(())
    }

    /// Apply plasticity rules based on spike timing
    fn apply_plasticity(&mut self, spike: &Spike) -> Result<()> {
        // Add spike to history
        self.spike_history.push_back(spike.clone());

        // Keep only recent spikes for STDP
        let stdp_window = 100.0; // ms
        while let Some(old_spike) = self.spike_history.front() {
            if self.current_time - old_spike.time > stdp_window {
                self.spike_history.pop_front();
            } else {
                break;
            }
        }

        // Apply STDP to relevant synapses
        let rules = self.plasticity_rules.clone();
        for (layer_idx, rule) in rules.iter().enumerate() {
            self.apply_stdp_rule(layer_idx, spike, rule)?;
        }

        Ok(())
    }

    /// Apply STDP rule to synaptic weights
    fn apply_stdp_rule(
        &mut self,
        layer_idx: usize,
        spike: &Spike,
        rule: &PlasticityRule,
    ) -> Result<()> {
        if layer_idx >= self.weights.len() {
            return Ok(());
        }

        match rule {
            PlasticityRule::STDP {
                tau_plus,
                tau_minus,
                a_plus,
                a_minus,
            } => {
                // Find related spikes for STDP
                for history_spike in &self.spike_history {
                    let dt = spike.time - history_spike.time;

                    if dt.abs() < 100.0 && dt != 0.0 {
                        // Within STDP window and not same spike
                        let delta_w = if dt > 0.0 {
                            // Post before pre -> LTD
                            -a_minus * (-dt / tau_minus).exp()
                        } else {
                            // Pre before post -> LTP
                            a_plus * (dt / tau_plus).exp()
                        };

                        // Apply weight change (simplified - would need proper pre/post neuron mapping)
                        if spike.neuron_id < self.weights[layer_idx].ncols()
                            && history_spike.neuron_id < self.weights[layer_idx].nrows()
                        {
                            let current_weight =
                                self.weights[layer_idx][[history_spike.neuron_id, spike.neuron_id]];
                            let new_weight = current_weight + F::from(delta_w).unwrap();

                            // Clip weights to reasonable range
                            let clipped_weight = new_weight
                                .max(F::from(-1.0).unwrap())
                                .min(F::from(1.0).unwrap());
                            self.weights[layer_idx][[history_spike.neuron_id, spike.neuron_id]] =
                                clipped_weight;
                        }
                    }
                }
            }

            PlasticityRule::Hebbian {
                learning_rate,
                decay_rate,
            } => {
                // Simplified Hebbian learning
                let _lr = F::from(*learning_rate).unwrap();
                let decay = F::from(*decay_rate).unwrap();

                // Apply decay to all weights
                for weight in self.weights[layer_idx].iter_mut() {
                    *weight = *weight * (F::one() - decay);
                }
            }

            _ => {
                // Other plasticity rules would be implemented here
            }
        }

        Ok(())
    }

    /// Get network output for time series prediction
    pub fn get_network_output(&self) -> Array1<F> {
        let output_layer_idx = self.layer_sizes.len() - 1;
        let output_size = self.layer_sizes[output_layer_idx];
        let mut output = Array1::zeros(output_size);

        for (i, state) in self.neuron_states[output_layer_idx].iter().enumerate() {
            // Use membrane potential as output (could also use spike rate)
            output[i] = state.v;
        }

        output
    }

    /// Train the network on time series data
    pub fn train(&mut self, data: &Array1<F>, targets: &Array1<F>) -> Result<F> {
        let spikes = self.encode_time_series(data);
        let _output_spikes = self.process_spikes(&spikes)?;

        let network_output = self.get_network_output();

        // Calculate loss (MSE)
        let mut loss = F::zero();
        let min_len = network_output.len().min(targets.len());

        for i in 0..min_len {
            let diff = network_output[i] - targets[i];
            loss = loss + diff * diff;
        }

        Ok(loss / F::from(min_len).unwrap())
    }

    /// Reset network state
    pub fn reset(&mut self) {
        for layer in &mut self.neuron_states {
            for state in layer {
                *state = NeuronState::default();
            }
        }
        self.spike_history.clear();
        self.current_time = 0.0;
    }
}

/// Liquid State Machine for temporal pattern recognition
#[derive(Debug)]
pub struct LiquidStateMachine<F: Float + Debug> {
    /// Reservoir of randomly connected neurons
    #[allow(dead_code)]
    reservoir: SpikingNeuralNetwork<F>,
    /// Readout layer weights
    #[allow(dead_code)]
    readout_weights: Array2<F>,
    /// Reservoir size
    #[allow(dead_code)]
    reservoir_size: usize,
    /// Input dimension
    #[allow(dead_code)]
    input_dim: usize,
    /// Output dimension
    #[allow(dead_code)]
    output_dim: usize,
    /// Spectral radius for stability
    #[allow(dead_code)]
    spectral_radius: f64,
    /// Connection probability
    #[allow(dead_code)]
    connection_prob: f64,
}

impl<F: Float + Debug + Clone + FromPrimitive + std::iter::Sum> LiquidStateMachine<F> {
    /// Create new Liquid State Machine
    pub fn new(
        #[allow(dead_code)] reservoir_size: usize,
        #[allow(dead_code)] input_dim: usize,
        #[allow(dead_code)] output_dim: usize,
        #[allow(dead_code)] spectral_radius: f64,
        #[allow(dead_code)] connection_prob: f64,
    ) -> Result<Self> {
        // Create reservoir with random connectivity
        let layer_sizes = vec![input_dim, reservoir_size];
        let neuron_models = vec![
            NeuronModel::LeakyIntegrateFire {
                tau_m: 20.0,
                v_rest: -70.0,
                v_threshold: -55.0,
                v_reset: -70.0,
            },
            NeuronModel::LeakyIntegrateFire {
                tau_m: 20.0,
                v_rest: -70.0,
                v_threshold: -55.0,
                v_reset: -70.0,
            },
        ];
        let plasticity_rules = vec![PlasticityRule::STDP {
            tau_plus: 20.0,
            tau_minus: 20.0,
            a_plus: 0.01,
            a_minus: 0.01,
        }];

        let reservoir =
            SpikingNeuralNetwork::new(layer_sizes, neuron_models, plasticity_rules, 0.1)?;

        // Initialize readout weights
        let readout_weights = Array2::zeros((output_dim, reservoir_size));

        Ok(Self {
            reservoir,
            readout_weights,
            reservoir_size,
            input_dim,
            output_dim,
            spectral_radius,
            connection_prob,
        })
    }

    /// Process time series through liquid state machine
    pub fn process_time_series(&mut self, data: &Array1<F>) -> Result<Array1<F>> {
        // Reset reservoir state
        self.reservoir.reset();

        // Process data through reservoir
        let spikes = self.reservoir.encode_time_series(data);
        let _output_spikes = self.reservoir.process_spikes(&spikes)?;

        // Get reservoir state
        let reservoir_state = self.reservoir.get_network_output();

        // Apply readout layer
        let mut output = Array1::zeros(self.output_dim);
        for i in 0..self.output_dim {
            let mut sum = F::zero();
            for j in 0..reservoir_state.len().min(self.readout_weights.ncols()) {
                sum = sum + self.readout_weights[[i, j]] * reservoir_state[j];
            }
            output[i] = sum;
        }

        Ok(output)
    }

    /// Train the readout layer using ridge regression
    pub fn train_readout(&mut self, trainingdata: &[(Array1<F>, Array1<F>)]) -> Result<()> {
        if trainingdata.is_empty() {
            return Ok(());
        }

        // Collect reservoir states
        let mut states = Vec::new();
        let mut targets = Vec::new();

        for (input, target) in trainingdata {
            let reservoir_state = self.process_reservoir_only(input)?;
            states.push(reservoir_state);
            targets.push(target.clone());
        }

        // Solve for readout weights using least squares (simplified)
        self.solve_readout_weights(&states, &targets)?;

        Ok(())
    }

    /// Process data through reservoir only (no readout)
    fn process_reservoir_only(&mut self, data: &Array1<F>) -> Result<Array1<F>> {
        self.reservoir.reset();
        let spikes = self.reservoir.encode_time_series(data);
        let _output_spikes = self.reservoir.process_spikes(&spikes)?;
        Ok(self.reservoir.get_network_output())
    }

    /// Solve for readout weights using simplified least squares
    fn solve_readout_weights(&mut self, states: &[Array1<F>], targets: &[Array1<F>]) -> Result<()> {
        if states.is_empty() {
            return Ok(());
        }

        let _n_samples = states.len();
        let state_dim = states[0].len();

        // For each output dimension, solve separately
        for out_dim in 0..self.output_dim {
            // Collect target values for this output dimension
            let mut y = Vec::new();
            for target in targets {
                if out_dim < target.len() {
                    y.push(target[out_dim]);
                } else {
                    y.push(F::zero());
                }
            }

            // Solve using simplified approach (diagonal approximation)
            for j in 0..state_dim.min(self.readout_weights.ncols()) {
                let mut numerator = F::zero();
                let mut denominator = F::zero();

                for (i, state) in states.iter().enumerate() {
                    if j < state.len() && i < y.len() {
                        numerator = numerator + state[j] * y[i];
                        denominator = denominator + state[j] * state[j];
                    }
                }

                self.readout_weights[[out_dim, j]] = if denominator > F::zero() {
                    numerator / denominator
                } else {
                    F::zero()
                };
            }
        }

        Ok(())
    }
}

/// Memristive network for hardware-aware neuromorphic computing
#[derive(Debug)]
pub struct MemristiveNetwork<F: Float + Debug> {
    /// Memristive crossbar array
    crossbar: Array2<MemristorState<F>>,
    /// Network topology
    #[allow(dead_code)]
    topology: NetworkTopology,
    /// Learning parameters
    learning_params: MemristiveLearningParams<F>,
    /// Current state
    #[allow(dead_code)]
    current_state: Array1<F>,
}

/// Memristor device state
#[derive(Debug, Clone)]
pub struct MemristorState<F: Float> {
    /// Resistance value
    pub resistance: F,
    /// Conductance (1/resistance)
    pub conductance: F,
    /// Internal state variable
    pub state: F,
    /// Device parameters
    pub params: MemristorParams<F>,
}

impl<F: Float + num_traits::FromPrimitive> Default for MemristorState<F> {
    fn default() -> Self {
        let resistance = F::from(1000.0).unwrap(); // 1kΩ default
        Self {
            resistance,
            conductance: F::from(1.0).unwrap() / resistance, // 1/R
            state: F::from(0.0).unwrap(),                    // Neutral state
            params: MemristorParams::default(),
        }
    }
}

/// Memristor device parameters
#[derive(Debug, Clone)]
pub struct MemristorParams<F: Float> {
    /// Minimum resistance
    pub r_min: F,
    /// Maximum resistance
    pub r_max: F,
    /// State change rate
    pub alpha: F,
    /// Nonlinearity parameter
    pub beta: F,
}

impl<F: Float + num_traits::FromPrimitive> Default for MemristorParams<F> {
    fn default() -> Self {
        Self {
            r_min: F::from(100.0).unwrap(),   // 100Ω minimum
            r_max: F::from(10000.0).unwrap(), // 10kΩ maximum
            alpha: F::from(0.1).unwrap(),     // Default state change rate
            beta: F::from(1.0).unwrap(),      // Default nonlinearity
        }
    }
}

/// Network topology for memristive arrays
#[derive(Debug, Clone)]
pub enum NetworkTopology {
    /// Fully connected crossbar
    FullyConnected,
    /// Sparse random connections
    Sparse {
        /// Connectivity probability
        connectivity: f64,
    },
    /// Small-world network
    SmallWorld {
        /// Rewiring probability
        rewiring_prob: f64,
    },
    /// Scale-free network
    ScaleFree {
        /// Power law exponent
        gamma: f64,
    },
}

/// Learning parameters for memristive networks
#[derive(Debug, Clone)]
pub struct MemristiveLearningParams<F: Float> {
    /// Learning rate
    pub learning_rate: F,
    /// Decay factor
    pub decay: F,
    /// Noise level
    pub noise: F,
    /// Update threshold
    pub threshold: F,
}

impl<F: Float + Debug + Clone + FromPrimitive> MemristiveNetwork<F> {
    /// Create new memristive network
    pub fn new(
        size: usize,
        topology: NetworkTopology,
        learning_params: MemristiveLearningParams<F>,
    ) -> Self {
        let mut crossbar = Array2::default((size, size));

        // Initialize memristors
        for i in 0..size {
            for j in 0..size {
                crossbar[[i, j]] = MemristorState {
                    resistance: F::from(1000.0).unwrap(), // 1kΩ
                    conductance: F::from(0.001).unwrap(), // 1mS
                    state: F::from(0.5).unwrap(),         // Mid-state
                    params: MemristorParams {
                        r_min: F::from(100.0).unwrap(),   // 100Ω
                        r_max: F::from(10000.0).unwrap(), // 10kΩ
                        alpha: F::from(1.0).unwrap(),
                        beta: F::from(1.0).unwrap(),
                    },
                };
            }
        }

        let current_state = Array1::zeros(size);

        Self {
            crossbar,
            topology,
            learning_params,
            current_state,
        }
    }

    /// Update memristor states based on applied voltage
    pub fn update_memristors(&mut self, voltage: &Array2<F>) -> Result<()> {
        let (rows, cols) = self.crossbar.dim();

        for i in 0..rows {
            for j in 0..cols {
                if i < voltage.nrows() && j < voltage.ncols() {
                    let v = voltage[[i, j]];
                    self.update_single_memristor(i, j, v)?;
                }
            }
        }

        Ok(())
    }

    /// Update single memristor based on applied voltage
    fn update_single_memristor(&mut self, i: usize, j: usize, voltage: F) -> Result<()> {
        let memristor = &mut self.crossbar[[i, j]];
        let params = &memristor.params;

        // Memristor dynamics: dx/dt = alpha * f(x) * g(V)
        let f_x = memristor.state * (F::one() - memristor.state); // Window function
        let g_v = voltage; // Simplified voltage dependence

        let dx = params.alpha * f_x * g_v * F::from(0.01).unwrap(); // Small time step
        memristor.state = (memristor.state + dx).max(F::zero()).min(F::one());

        // Update resistance based on state
        let state_range = params.r_max - params.r_min;
        memristor.resistance = params.r_min + state_range * memristor.state;
        memristor.conductance = F::one() / memristor.resistance;

        Ok(())
    }

    /// Compute network output using memristive crossbar
    pub fn compute_output(&self, input: &Array1<F>) -> Array1<F> {
        let size = self.crossbar.nrows();
        let mut output = Array1::zeros(size);

        for i in 0..size {
            let mut sum = F::zero();
            for j in 0..size.min(input.len()) {
                sum = sum + self.crossbar[[i, j]].conductance * input[j];
            }
            output[i] = sum;
        }

        output
    }

    /// Train the memristive network using spike-timing dependent plasticity
    pub fn train_stdp(
        &mut self,
        pre_spikes: &[f64],
        post_spikes: &[f64],
        neuron_i: usize,
        neuron_j: usize,
    ) -> Result<()> {
        if neuron_i >= self.crossbar.nrows() || neuron_j >= self.crossbar.ncols() {
            return Ok(());
        }

        // Calculate STDP weight change
        let mut total_change = F::zero();

        for &t_pre in pre_spikes {
            for &t_post in post_spikes {
                let dt = t_post - t_pre;
                let tau = 20.0; // STDP time constant

                let weight_change = if dt > 0.0 {
                    // LTP
                    F::from(0.01 * (-dt / tau).exp()).unwrap()
                } else {
                    // LTD
                    F::from(-0.01 * (dt / tau).exp()).unwrap()
                };

                total_change = total_change + weight_change;
            }
        }

        // Apply change to memristor state
        let memristor = &mut self.crossbar[[neuron_i, neuron_j]];
        let state_change = total_change * self.learning_params.learning_rate;
        memristor.state = (memristor.state + state_change)
            .max(F::zero())
            .min(F::one());

        // Update resistance
        let params = &memristor.params.clone();
        let state_range = params.r_max - params.r_min;
        memristor.resistance = params.r_min + state_range * memristor.state;
        memristor.conductance = F::one() / memristor.resistance;

        Ok(())
    }
}

/// **Advanced MODE: NEXT-GENERATION NEUROMORPHIC COMPUTING**
/// Bio-Realistic Dendritic Computation Model
#[derive(Debug)]
pub struct DendriticComputationUnit<F: Float + Debug> {
    /// Dendritic tree structure
    dendritic_tree: DendriticTree<F>,
    /// Synaptic inputs along dendrites
    synaptic_inputs: Vec<SynapticInput<F>>,
    /// Dendritic spine dynamics
    #[allow(dead_code)]
    spine_dynamics: Vec<SpineDynamics<F>>,
    /// Active dendritic currents
    #[allow(dead_code)]
    active_currents: HashMap<DendriticCurrent, F>,
    /// Calcium concentration dynamics
    calcium_dynamics: CalciumDynamics<F>,
}

/// Dendritic tree topology
#[derive(Debug, Clone)]
pub struct DendriticTree<F: Float + Debug> {
    /// Tree nodes representing dendritic segments
    segments: Vec<DendriticSegment<F>>,
    /// Connectivity between segments
    connections: Vec<DendriticConnection>,
    /// Somatic distance for each segment
    soma_distances: Array1<F>,
    /// Segment diameters
    #[allow(dead_code)]
    diameters: Array1<F>,
}

/// Individual dendritic segment
#[derive(Debug, Clone)]
pub struct DendriticSegment<F: Float + Debug> {
    /// Segment ID
    #[allow(dead_code)]
    id: usize,
    /// Membrane potential
    voltage: F,
    /// Length of segment
    #[allow(dead_code)]
    length: F,
    /// Surface area
    #[allow(dead_code)]
    surface_area: F,
    /// Ion channel densities
    channel_densities: HashMap<IonChannel, F>,
    /// Local calcium concentration
    calcium_concentration: F,
}

/// Dendritic connections between segments
#[derive(Debug, Clone)]
pub struct DendriticConnection {
    /// Source segment
    from_segment: usize,
    /// Target segment
    to_segment: usize,
    /// Axial resistance
    resistance: f64,
    /// Connection strength
    #[allow(dead_code)]
    coupling_strength: f64,
}

/// Types of ion channels in dendrites
#[derive(Debug, Clone, Hash, PartialEq, Eq)]
pub enum IonChannel {
    /// Voltage-gated sodium channels
    VGSodium,
    /// Voltage-gated potassium channels
    VGPotassium,
    /// Voltage-gated calcium channels (L-type)
    VGCalciumL,
    /// Voltage-gated calcium channels (T-type)
    VGCalciumT,
    /// Hyperpolarization-activated cation channels
    HCN,
    /// A-type potassium channels
    KA,
    /// SK-type calcium-activated potassium channels
    SK,
    /// BK-type calcium-activated potassium channels
    BK,
}

/// Types of active dendritic currents
#[derive(Debug, Clone, Hash, PartialEq, Eq)]
pub enum DendriticCurrent {
    /// Persistent sodium current
    INaP,
    /// L-type calcium current
    ICaL,
    /// T-type calcium current
    ICaT,
    /// A-type potassium current
    IKA,
    /// Delayed rectifier potassium current
    IKdr,
    /// Calcium-activated potassium current
    IKCa,
    /// Hyperpolarization-activated current
    Ih,
}

/// Synaptic input to dendritic segment
#[derive(Debug, Clone)]
pub struct SynapticInput<F: Float + Debug> {
    /// Input location on dendritic tree
    segmentid: usize,
    /// Distance from soma
    #[allow(dead_code)]
    soma_distance: F,
    /// Synaptic weight
    #[allow(dead_code)]
    weight: F,
    /// Synaptic conductance
    conductance: F,
    /// Reversal potential
    reversal_potential: F,
    /// Synaptic time constants
    #[allow(dead_code)]
    tau_rise: F,
    #[allow(dead_code)]
    tau_decay: F,
    /// NMDA/AMPA ratio
    #[allow(dead_code)]
    nmda_ampa_ratio: F,
}

/// Dendritic spine dynamics
#[derive(Debug, Clone)]
pub struct SpineDynamics<F: Float + Debug> {
    /// Spine head volume
    #[allow(dead_code)]
    head_volume: F,
    /// Spine neck resistance
    #[allow(dead_code)]
    neck_resistance: F,
    /// Calcium compartmentalization
    #[allow(dead_code)]
    calcium_compartment: F,
    /// Plasticity state variables
    #[allow(dead_code)]
    plasticity_variables: PlasticityVariables<F>,
    /// Spine maturation state
    #[allow(dead_code)]
    maturation_level: F,
}

/// Plasticity variables for dendritic spines
#[derive(Debug, Clone)]
pub struct PlasticityVariables<F: Float + Debug> {
    /// CaMKII autophosphorylation level
    #[allow(dead_code)]
    camkii_phosphorylation: F,
    /// Protein synthesis activity
    #[allow(dead_code)]
    protein_synthesis: F,
    /// AMPA receptor trafficking
    #[allow(dead_code)]
    ampa_trafficking: F,
    /// Spine size scaling factor
    #[allow(dead_code)]
    size_scaling: F,
}

/// Calcium dynamics in dendritic compartments
#[derive(Debug, Clone)]
pub struct CalciumDynamics<F: Float + Debug> {
    /// Intracellular calcium concentration
    ca_concentration: Array1<F>,
    /// Calcium buffer concentrations
    #[allow(dead_code)]
    buffer_concentrations: HashMap<CalciumBuffer, Array1<F>>,
    /// Calcium pumps and exchangers
    #[allow(dead_code)]
    pump_activities: HashMap<CalciumPump, F>,
    /// Calcium diffusion coefficients
    #[allow(dead_code)]
    diffusion_coefficients: Array1<F>,
}

/// Types of calcium buffers
#[derive(Debug, Clone, Hash, PartialEq, Eq)]
pub enum CalciumBuffer {
    /// Calmodulin
    Calmodulin,
    /// Parvalbumin
    Parvalbumin,
    /// Calbindin
    Calbindin,
    /// Fixed buffers
    FixedBuffer,
}

/// Types of calcium pumps and exchangers
#[derive(Debug, Clone, Hash, PartialEq, Eq)]
pub enum CalciumPump {
    /// Plasma membrane calcium ATPase
    PMCA,
    /// Sodium-calcium exchanger
    NCX,
    /// Sarco/endoplasmic reticulum calcium ATPase
    SERCA,
}

impl<F: Float + Debug + Clone + FromPrimitive> DendriticComputationUnit<F> {
    /// Create new dendritic computation unit
    pub fn new(__numsegments: usize, treetopology: TreeTopology) -> Self {
        let dendritic_tree = Self::create_dendritic_tree(__numsegments, treetopology);
        let synaptic_inputs = Vec::new();
        let spine_dynamics = Vec::new();

        let mut active_currents = HashMap::new();
        active_currents.insert(DendriticCurrent::INaP, F::zero());
        active_currents.insert(DendriticCurrent::ICaL, F::zero());
        active_currents.insert(DendriticCurrent::IKA, F::zero());

        let calcium_dynamics = CalciumDynamics {
            ca_concentration: Array1::from_elem(__numsegments, F::from(0.0001).unwrap()), // 100 nM baseline
            buffer_concentrations: HashMap::new(),
            pump_activities: HashMap::new(),
            diffusion_coefficients: Array1::from_elem(__numsegments, F::from(0.22).unwrap()), // μm²/ms
        };

        Self {
            dendritic_tree,
            synaptic_inputs,
            spine_dynamics,
            active_currents,
            calcium_dynamics,
        }
    }

    /// Create dendritic tree structure
    fn create_dendritic_tree(_numsegments: usize, topology: TreeTopology) -> DendriticTree<F> {
        let mut _segments = Vec::new();
        let mut connections = Vec::new();
        let mut soma_distances = Array1::zeros(_numsegments);
        let mut diameters = Array1::zeros(_numsegments);

        for i in 0.._numsegments {
            let mut channel_densities = HashMap::new();

            // Distance-dependent channel densities
            let distance_factor = F::from(i as f64 / _numsegments as f64).unwrap();

            channel_densities.insert(
                IonChannel::VGSodium,
                F::from(120.0).unwrap() * (F::one() - distance_factor * F::from(0.5).unwrap()),
            );
            channel_densities.insert(IonChannel::VGPotassium, F::from(36.0).unwrap());
            channel_densities.insert(
                IonChannel::VGCalciumL,
                F::from(0.4).unwrap() * distance_factor,
            );
            channel_densities.insert(IonChannel::HCN, F::from(0.1).unwrap() * distance_factor);

            let segment = DendriticSegment {
                id: i,
                voltage: F::from(-70.0).unwrap(), // Resting potential
                length: F::from(10.0).unwrap(),   // μm
                surface_area: F::from(314.16).unwrap(), // μm² (π*d*L for d=10μm)
                channel_densities,
                calcium_concentration: F::from(0.0001).unwrap(), // 100 nM
            };

            _segments.push(segment);
            soma_distances[i] = F::from(i as f64 * 10.0).unwrap(); // 10 μm per segment
            diameters[i] = F::from(2.0).unwrap() / (F::one() + distance_factor);
            // Tapering diameter
        }

        // Create connections based on topology
        match topology {
            TreeTopology::Linear => {
                for i in 0..(_numsegments - 1) {
                    connections.push(DendriticConnection {
                        from_segment: i,
                        to_segment: i + 1,
                        resistance: 100.0, // MΩ
                        coupling_strength: 1.0,
                    });
                }
            }
            TreeTopology::Branched => {
                // Create binary tree structure
                for i in 0.._numsegments / 2 {
                    if 2 * i + 1 < _numsegments {
                        connections.push(DendriticConnection {
                            from_segment: i,
                            to_segment: 2 * i + 1,
                            resistance: 150.0,
                            coupling_strength: 0.8,
                        });
                    }
                    if 2 * i + 2 < _numsegments {
                        connections.push(DendriticConnection {
                            from_segment: i,
                            to_segment: 2 * i + 2,
                            resistance: 150.0,
                            coupling_strength: 0.8,
                        });
                    }
                }
            }
        }

        DendriticTree {
            segments: _segments,
            connections,
            soma_distances,
            diameters,
        }
    }

    /// Simulate dendritic computation with active properties
    pub fn simulate_dendritic_integration(
        &mut self,
        input_currents: &Array1<F>,
        dt: F,
    ) -> crate::error::Result<Array1<F>> {
        let _numsegments = self.dendritic_tree.segments.len();
        let mut voltages = Array1::zeros(_numsegments);

        // First pass: compute all _currents without mutable borrows
        let mut total_currents = Vec::with_capacity(_numsegments);
        for i in 0.._numsegments {
            let segment = &self.dendritic_tree.segments[i];
            let mut total_current = F::zero();

            // External input current
            if i < input_currents.len() {
                total_current = total_current + input_currents[i];
            }

            // Active ionic _currents
            total_current = total_current + self.compute_active_currents(segment, i)?;

            // Synaptic _currents
            total_current = total_current + self.compute_synaptic_currents(i)?;

            // Axial _currents from neighboring segments
            total_current = total_current + self.compute_axial_currents(i)?;

            total_currents.push(total_current);
        }

        // Second pass: update all segments with computed _currents
        for (i, segment) in self.dendritic_tree.segments.iter_mut().enumerate() {
            let total_current = total_currents[i];

            // Update membrane potential
            let cm = F::from(1.0).unwrap(); // μF/cm²
            let dv = total_current / cm * dt;
            segment.voltage = segment.voltage + dv;

            voltages[i] = segment.voltage;
        }

        // Third pass: update calcium dynamics
        for i in 0.._numsegments {
            self.update_calcium_dynamics(i, dt)?;
        }

        Ok(voltages)
    }

    /// Compute active ionic currents for a segment
    fn compute_active_currents(
        &self,
        segment: &DendriticSegment<F>,
        _segmentid: usize,
    ) -> crate::error::Result<F> {
        let mut total_current = F::zero();
        let v = segment.voltage;
        let ca = segment.calcium_concentration;

        // Voltage-gated sodium current (persistent)
        if let Some(&density) = segment.channel_densities.get(&IonChannel::VGSodium) {
            let e_na = F::from(50.0).unwrap(); // mV
            let m_inf = F::one()
                / (F::one() + (-(v + F::from(38.0).unwrap()) / F::from(7.0).unwrap()).exp());
            let i_na_p = density * m_inf * (v - e_na);
            total_current = total_current - i_na_p * F::from(0.01).unwrap(); // Small persistent component
        }

        // L-type calcium current
        if let Some(&density) = segment.channel_densities.get(&IonChannel::VGCalciumL) {
            let e_ca = F::from(120.0).unwrap(); // mV
            let m_inf = F::one()
                / (F::one() + (-(v + F::from(10.0).unwrap()) / F::from(5.0).unwrap()).exp());
            let i_ca_l = density * m_inf * m_inf * (v - e_ca);
            total_current = total_current - i_ca_l;
        }

        // A-type potassium current
        if let Some(&density) = segment.channel_densities.get(&IonChannel::KA) {
            let e_k = F::from(-85.0).unwrap(); // mV
            let m_inf = F::one()
                / (F::one() + (-(v + F::from(60.0).unwrap()) / F::from(8.5).unwrap()).exp());
            let h_inf = F::one()
                / (F::one() + ((v + F::from(78.0).unwrap()) / F::from(6.0).unwrap()).exp());
            let i_ka = density * m_inf * m_inf * m_inf * h_inf * (v - e_k);
            total_current = total_current - i_ka;
        }

        // Calcium-activated potassium current
        let ca_factor = ca / (ca + F::from(0.001).unwrap()); // Half-activation at 1 μM
        let i_k_ca = F::from(2.0).unwrap() * ca_factor * (v - F::from(-85.0).unwrap());
        total_current = total_current - i_k_ca;

        Ok(total_current)
    }

    /// Compute synaptic currents
    fn compute_synaptic_currents(&self, segmentid: usize) -> crate::error::Result<F> {
        let mut total_current = F::zero();

        for input in &self.synaptic_inputs {
            if input.segmentid == segmentid {
                let segment_voltage = self.dendritic_tree.segments[segmentid].voltage;
                let driving_force = segment_voltage - input.reversal_potential;
                let synaptic_current = input.conductance * driving_force;
                total_current = total_current - synaptic_current;
            }
        }

        Ok(total_current)
    }

    /// Compute axial currents between segments
    fn compute_axial_currents(&self, segmentid: usize) -> crate::error::Result<F> {
        let mut total_current = F::zero();
        let segment_voltage = self.dendritic_tree.segments[segmentid].voltage;

        for connection in &self.dendritic_tree.connections {
            let resistance = F::from(connection.resistance).unwrap();

            if connection.from_segment == segmentid {
                // Current flowing out to target segment
                let target_voltage = self.dendritic_tree.segments[connection.to_segment].voltage;
                let current = (segment_voltage - target_voltage) / resistance;
                total_current = total_current - current; // Outward current
            } else if connection.to_segment == segmentid {
                // Current flowing in from source segment
                let source_voltage = self.dendritic_tree.segments[connection.from_segment].voltage;
                let current = (source_voltage - segment_voltage) / resistance;
                total_current = total_current + current; // Inward current
            }
        }

        Ok(total_current)
    }

    /// Update calcium dynamics in dendritic compartments
    fn update_calcium_dynamics(&mut self, segmentid: usize, dt: F) -> crate::error::Result<()> {
        if segmentid >= self.dendritic_tree.segments.len() {
            return Ok(());
        }

        // Compute values first before borrowing mutably
        let segment_for_influx = &self.dendritic_tree.segments[segmentid];
        let ca_current = self.compute_calcium_influx(segment_for_influx)?;
        let ca_removal = self.compute_calcium_removal(segmentid)?;

        // Calcium influx from voltage-gated channels
        let ca_influx = -ca_current / (F::from(2.0).unwrap() * F::from(96485.0).unwrap()); // Convert current to flux

        // Update calcium concentration
        let dca = (ca_influx - ca_removal) * dt;
        let segment = &mut self.dendritic_tree.segments[segmentid];
        segment.calcium_concentration =
            (segment.calcium_concentration + dca).max(F::from(0.00005).unwrap()); // Minimum 50 nM

        // Update calcium in dynamics array
        if segmentid < self.calcium_dynamics.ca_concentration.len() {
            self.calcium_dynamics.ca_concentration[segmentid] = segment.calcium_concentration;
        }

        Ok(())
    }

    /// Compute calcium influx from voltage-gated channels
    fn compute_calcium_influx(&self, segment: &DendriticSegment<F>) -> crate::error::Result<F> {
        let mut ca_current = F::zero();
        let v = segment.voltage;

        // L-type calcium channels
        if let Some(&density) = segment.channel_densities.get(&IonChannel::VGCalciumL) {
            let e_ca = F::from(120.0).unwrap();
            let m_inf = F::one()
                / (F::one() + (-(v + F::from(10.0).unwrap()) / F::from(5.0).unwrap()).exp());
            ca_current = ca_current + density * m_inf * m_inf * (v - e_ca);
        }

        // T-type calcium channels
        if let Some(&density) = segment.channel_densities.get(&IonChannel::VGCalciumT) {
            let e_ca = F::from(120.0).unwrap();
            let m_inf = F::one()
                / (F::one() + (-(v + F::from(50.0).unwrap()) / F::from(7.4).unwrap()).exp());
            let h_inf = F::one()
                / (F::one() + ((v + F::from(78.0).unwrap()) / F::from(5.0).unwrap()).exp());
            ca_current = ca_current + density * m_inf * m_inf * h_inf * (v - e_ca);
        }

        Ok(ca_current)
    }

    /// Compute calcium removal by pumps and buffers
    fn compute_calcium_removal(&self, segmentid: usize) -> crate::error::Result<F> {
        if segmentid >= self.calcium_dynamics.ca_concentration.len() {
            return Ok(F::zero());
        }

        let ca = self.calcium_dynamics.ca_concentration[segmentid];

        // PMCA pump (plasma membrane calcium ATPase)
        let k_pmca = F::from(0.1).unwrap(); // 1/ms
        let ca_pmca = F::from(0.0005).unwrap(); // 0.5 μM
        let pmca_removal = k_pmca * ca / (ca + ca_pmca);

        // NCX (sodium-calcium exchanger)
        let k_ncx = F::from(0.05).unwrap(); // 1/ms
        let ncx_removal = k_ncx * ca;

        // Buffer binding (simplified)
        let k_buffer = F::from(0.02).unwrap(); // 1/ms
        let buffer_removal = k_buffer * ca;

        Ok(pmca_removal + ncx_removal + buffer_removal)
    }

    /// Add synaptic input to specific dendritic location
    pub fn add_synaptic_input(
        &mut self,
        segmentid: usize,
        weight: F,
        input_type: SynapticType,
    ) -> crate::error::Result<()> {
        if segmentid >= self.dendritic_tree.segments.len() {
            return Err(crate::error::TimeSeriesError::InvalidInput(
                "Segment ID out of bounds".to_string(),
            ));
        }

        let soma_distance = self.dendritic_tree.soma_distances[segmentid];

        let (conductance, reversal_potential, tau_rise, tau_decay, nmda_ampa_ratio) =
            match input_type {
                SynapticType::Excitatory => (
                    F::from(0.1).unwrap(),
                    F::zero(),
                    F::from(0.2).unwrap(),
                    F::from(2.0).unwrap(),
                    F::from(0.3).unwrap(),
                ),
                SynapticType::Inhibitory => (
                    F::from(0.2).unwrap(),
                    F::from(-70.0).unwrap(),
                    F::from(0.5).unwrap(),
                    F::from(5.0).unwrap(),
                    F::zero(),
                ),
            };

        let synaptic_input = SynapticInput {
            segmentid,
            soma_distance,
            weight,
            conductance,
            reversal_potential,
            tau_rise,
            tau_decay,
            nmda_ampa_ratio,
        };

        self.synaptic_inputs.push(synaptic_input);
        Ok(())
    }
}

/// Types of dendritic tree topologies
#[derive(Debug, Clone)]
pub enum TreeTopology {
    /// Linear chain of segments
    Linear,
    /// Branched tree structure
    Branched,
}

/// Types of synaptic inputs
#[derive(Debug, Clone)]
pub enum SynapticType {
    /// Excitatory input (glutamate)
    Excitatory,
    /// Inhibitory input (GABA)
    Inhibitory,
}

/// Synaptic Vesicle Dynamics for Advanced-Realistic Synaptic Transmission
#[derive(Debug)]
pub struct SynapticVesicleDynamics<F: Float + Debug> {
    /// Readily releasable pool (RRP)
    rrp_vesicles: usize,
    /// Recycling pool
    recycling_pool: usize,
    /// Reserve pool
    reserve_pool: usize,
    /// Vesicle release probability
    release_probability: F,
    /// Vesicle replenishment rates
    replenishment_rates: VesicleReplenishmentRates<F>,
    /// Calcium cooperativity
    calcium_cooperativity: F,
    /// Short-term plasticity parameters
    stp_parameters: ShortTermPlasticityParams<F>,
}

/// Vesicle replenishment rates between pools
#[derive(Debug, Clone)]
pub struct VesicleReplenishmentRates<F: Float> {
    /// Reserve to recycling pool rate
    reserve_to_recycling: F,
    /// Recycling to RRP rate
    recycling_to_rrp: F,
    /// Endocytosis rate
    #[allow(dead_code)]
    endocytosis_rate: F,
    /// Exocytosis rate
    #[allow(dead_code)]
    exocytosis_rate: F,
}

/// Short-term plasticity parameters
#[derive(Debug, Clone)]
pub struct ShortTermPlasticityParams<F: Float> {
    /// Facilitation time constant
    tau_facilitation: F,
    /// Depression time constant
    tau_depression: F,
    /// Facilitation strength
    facilitation_strength: F,
    /// Initial depression level
    initial_depression: F,
}

impl<F: Float + Debug + Clone + FromPrimitive> SynapticVesicleDynamics<F> {
    /// Create new synaptic vesicle dynamics
    pub fn new(_initialvesicles: usize) -> Self {
        Self {
            rrp_vesicles: _initialvesicles / 3,
            recycling_pool: _initialvesicles / 3,
            reserve_pool: _initialvesicles / 3,
            release_probability: F::from(0.3).unwrap(),
            replenishment_rates: VesicleReplenishmentRates {
                reserve_to_recycling: F::from(0.01).unwrap(), // 1/s
                recycling_to_rrp: F::from(0.1).unwrap(),      // 1/s
                endocytosis_rate: F::from(0.05).unwrap(),     // 1/s
                exocytosis_rate: F::from(1.0).unwrap(),       // 1/s
            },
            calcium_cooperativity: F::from(4.0).unwrap(), // Hill coefficient
            stp_parameters: ShortTermPlasticityParams {
                tau_facilitation: F::from(100.0).unwrap(), // ms
                tau_depression: F::from(500.0).unwrap(),   // ms
                facilitation_strength: F::from(0.1).unwrap(),
                initial_depression: F::from(1.0).unwrap(),
            },
        }
    }

    /// Simulate vesicle release and dynamics
    pub fn simulate_vesicle_release(
        &mut self,
        calcium_concentration: F,
        dt: F,
    ) -> crate::error::Result<usize> {
        // Calculate release probability based on calcium
        let ca_factor = calcium_concentration.powf(self.calcium_cooperativity);
        let k_half = F::from(0.001).unwrap(); // Half-activation at 1 μM
        let effective_release_prob = self.release_probability * ca_factor
            / (ca_factor + k_half.powf(self.calcium_cooperativity));

        // Number of vesicles released
        let vesicles_released = if self.rrp_vesicles > 0 {
            let release_rate = effective_release_prob * F::from(self.rrp_vesicles).unwrap() * dt;
            let released = release_rate.to_usize().unwrap_or(0).min(self.rrp_vesicles);
            self.rrp_vesicles -= released;
            released
        } else {
            0
        };

        // Update vesicle pools
        self.update_vesicle_pools(dt)?;

        Ok(vesicles_released)
    }

    /// Update vesicle pool dynamics
    fn update_vesicle_pools(&mut self, dt: F) -> crate::error::Result<()> {
        // Reserve pool -> Recycling pool
        let reserve_to_recycling = (F::from(self.reserve_pool).unwrap()
            * self.replenishment_rates.reserve_to_recycling
            * dt)
            .to_usize()
            .unwrap_or(0)
            .min(self.reserve_pool);
        self.reserve_pool -= reserve_to_recycling;
        self.recycling_pool += reserve_to_recycling;

        // Recycling pool -> RRP
        let recycling_to_rrp = (F::from(self.recycling_pool).unwrap()
            * self.replenishment_rates.recycling_to_rrp
            * dt)
            .to_usize()
            .unwrap_or(0)
            .min(self.recycling_pool);
        self.recycling_pool -= recycling_to_rrp;
        self.rrp_vesicles += recycling_to_rrp;

        Ok(())
    }

    /// Apply short-term plasticity
    pub fn apply_short_term_plasticity(
        &mut self,
        inter_spike_interval: F,
    ) -> crate::error::Result<()> {
        // Facilitation
        let facilitation_decay =
            (-inter_spike_interval / self.stp_parameters.tau_facilitation).exp();
        let facilitation_increment =
            self.stp_parameters.facilitation_strength * (F::one() - self.release_probability);
        self.release_probability =
            self.release_probability * facilitation_decay + facilitation_increment;

        // Depression
        let depression_factor = (-inter_spike_interval / self.stp_parameters.tau_depression).exp();
        let depression_recovery =
            (F::one() - self.stp_parameters.initial_depression) * (F::one() - depression_factor);
        self.release_probability = self.release_probability
            * (self.stp_parameters.initial_depression + depression_recovery);

        // Clamp release probability
        self.release_probability = self
            .release_probability
            .max(F::from(0.01).unwrap())
            .min(F::from(0.99).unwrap());

        Ok(())
    }
}

/// Intel Loihi-Style Neuromorphic Architecture Simulation
#[derive(Debug)]
pub struct LoihiStyleNeuromorphicChip<F: Float + Debug> {
    /// Neuromorphic cores
    cores: Vec<NeuromorphicCore<F>>,
    /// Inter-core connectivity
    inter_core_routing: Array2<F>,
    /// Spike routing fabric
    spike_router: SpikeRouter,
    /// On-chip learning engines
    #[allow(dead_code)]
    learning_engines: Vec<OnChipLearningEngine<F>>,
    /// Power management
    power_manager: PowerManager<F>,
}

/// Individual neuromorphic core (like Loihi core)
#[derive(Debug)]
pub struct NeuromorphicCore<F: Float + Debug> {
    /// Core ID
    core_id: usize,
    /// Compartments (neurons)
    compartments: Vec<LoihiCompartment<F>>,
    /// Synaptic memory
    #[allow(dead_code)]
    synaptic_memory: Array2<F>,
    /// Dendrite accumulators
    dendrite_accumulators: Array1<F>,
    /// Axon outputs
    axon_outputs: Vec<bool>,
    /// Learning traces
    #[allow(dead_code)]
    learning_traces: HashMap<usize, LearningTrace<F>>,
}

/// Loihi-style compartment (neuron)
#[derive(Debug, Clone)]
pub struct LoihiCompartment<F: Float + Debug> {
    /// Compartment state variables
    voltage: F,
    current: F,
    /// Bias current
    bias: F,
    /// Voltage decay (leak)
    voltage_decay: F,
    /// Current decay
    current_decay: F,
    /// Spike threshold
    threshold: F,
    /// Refractory period
    refractory_delay: usize,
    /// Refractory counter
    refractory_counter: usize,
}

/// Spike routing for inter-core communication
#[derive(Debug)]
pub struct SpikeRouter {
    /// Routing table
    routing_table: HashMap<(usize, usize), Vec<(usize, usize)>>, // (src_core, src_axon) -> [(dst_core, dst_compartment)]
    /// Spike buffers
    #[allow(dead_code)]
    spike_buffers: HashMap<usize, Vec<SpikePacket>>,
    /// Routing latency
    #[allow(dead_code)]
    routing_latency: usize,
}

/// Spike packet for inter-core communication
#[derive(Debug, Clone)]
pub struct SpikePacket {
    /// Source core
    src_core: usize,
    /// Source axon
    src_axon: usize,
    /// Destination core
    dst_core: usize,
    /// Destination compartment
    dst_compartment: usize,
    /// Spike weight
    weight: i16,
    /// Timestamp
    #[allow(dead_code)]
    timestamp: u64,
}

/// On-chip learning engine
#[derive(Debug)]
pub struct OnChipLearningEngine<F: Float + Debug> {
    /// Learning rule type
    learning_rule: OnChipLearningRule,
    /// Learning parameters
    parameters: HashMap<String, F>,
    /// Trace storage
    #[allow(dead_code)]
    traces: HashMap<usize, LearningTrace<F>>,
}

/// Types of on-chip learning rules
#[derive(Debug, Clone)]
pub enum OnChipLearningRule {
    /// Spike-timing dependent plasticity
    STDP,
    /// Reward-modulated STDP
    RewardSTDP,
    /// Voltage-dependent plasticity
    VoltagePlasticity,
    /// Homeostatic plasticity
    Homeostatic,
}

/// Learning trace for plasticity
#[derive(Debug, Clone)]
pub struct LearningTrace<F: Float> {
    /// Trace value
    #[allow(dead_code)]
    value: F,
    /// Decay time constant
    #[allow(dead_code)]
    tau: F,
    /// Last update time
    #[allow(dead_code)]
    last_update: u64,
}

/// Power management for neuromorphic chip
#[derive(Debug)]
pub struct PowerManager<F: Float + Debug> {
    /// Power consumption per core
    core_power: Array1<F>,
    /// Total power budget
    power_budget: F,
    /// Dynamic voltage scaling
    voltage_scaling: Array1<F>,
    /// Clock gating enables
    clock_gating: Array1<bool>,
    /// Power measurement history
    power_history: Vec<F>,
}

impl<F: Float + Debug + Clone + FromPrimitive> LoihiStyleNeuromorphicChip<F> {
    /// Create new Loihi-style neuromorphic chip
    pub fn new(_num_cores: usize, compartments_percore: usize) -> Self {
        let mut _cores = Vec::new();

        for core_id in 0.._num_cores {
            let _core = NeuromorphicCore::new(core_id, compartments_percore);
            _cores.push(_core);
        }

        let inter_core_routing = Array2::zeros((_num_cores, _num_cores));
        let spike_router = SpikeRouter::new();
        let learning_engines = vec![OnChipLearningEngine::new(OnChipLearningRule::STDP)];
        let power_manager = PowerManager::new(_num_cores);

        Self {
            cores: _cores,
            inter_core_routing,
            spike_router,
            learning_engines,
            power_manager,
        }
    }

    /// Process one time step across all cores
    pub fn process_timestep(
        &mut self,
        input_spikes: &[SpikePacket],
    ) -> crate::error::Result<Vec<SpikePacket>> {
        let mut output_spikes = Vec::new();

        // Distribute input _spikes to cores
        self.distribute_input_spikes(input_spikes)?;

        // Process each core
        for core in &mut self.cores {
            let core_output = core.process_timestep()?;
            output_spikes.extend(core_output);
        }

        // Route _spikes between cores
        let routed_spikes = self.spike_router.route_spikes(&output_spikes)?;

        // Update power consumption
        self.power_manager.update_power_consumption(&self.cores)?;

        Ok(routed_spikes)
    }

    /// Distribute input spikes to appropriate cores
    fn distribute_input_spikes(
        &mut self,
        input_spikes: &[SpikePacket],
    ) -> crate::error::Result<()> {
        for spike in input_spikes {
            if spike.dst_core < self.cores.len() {
                self.cores[spike.dst_core].receive_spike(spike)?;
            }
        }
        Ok(())
    }

    /// Configure inter-core connectivity
    pub fn configure_inter_core_routing(
        &mut self,
        src_core: usize,
        dst_core: usize,
        weight: F,
    ) -> crate::error::Result<()> {
        if src_core >= self.cores.len() || dst_core >= self.cores.len() {
            return Err(crate::error::TimeSeriesError::InvalidInput(
                "Core index out of bounds".to_string(),
            ));
        }

        self.inter_core_routing[[src_core, dst_core]] = weight;
        Ok(())
    }
}

impl<F: Float + Debug + Clone + FromPrimitive> NeuromorphicCore<F> {
    /// Create new neuromorphic core
    pub fn new(_core_id: usize, numcompartments: usize) -> Self {
        let mut _compartments = Vec::new();

        for _ in 0..numcompartments {
            _compartments.push(LoihiCompartment {
                voltage: F::zero(),
                current: F::zero(),
                bias: F::zero(),
                voltage_decay: F::from(0.95).unwrap(), // Decay factor
                current_decay: F::from(0.9).unwrap(),
                threshold: F::from(100.0).unwrap(),
                refractory_delay: 2,
                refractory_counter: 0,
            });
        }

        let synaptic_memory = Array2::zeros((numcompartments, numcompartments));
        let dendrite_accumulators = Array1::zeros(numcompartments);
        let axon_outputs = vec![false; numcompartments];
        let learning_traces = HashMap::new();

        Self {
            core_id: _core_id,
            compartments: _compartments,
            synaptic_memory,
            dendrite_accumulators,
            axon_outputs,
            learning_traces,
        }
    }

    /// Process one timestep for this core
    pub fn process_timestep(&mut self) -> crate::error::Result<Vec<SpikePacket>> {
        let mut output_spikes = Vec::new();

        // Update compartment dynamics
        for (comp_id, compartment) in self.compartments.iter_mut().enumerate() {
            // Update refractory counter
            if compartment.refractory_counter > 0 {
                compartment.refractory_counter -= 1;
                continue;
            }

            // Add dendrite accumulator to current
            compartment.current = compartment.current + self.dendrite_accumulators[comp_id];

            // Update voltage
            compartment.voltage = compartment.voltage * compartment.voltage_decay
                + compartment.current
                + compartment.bias;

            // Update current decay
            compartment.current = compartment.current * compartment.current_decay;

            // Check for spike
            if compartment.voltage >= compartment.threshold {
                compartment.voltage = F::zero(); // Reset voltage
                compartment.refractory_counter = compartment.refractory_delay;
                self.axon_outputs[comp_id] = true;

                // Create output spike packet
                let spike_packet = SpikePacket {
                    src_core: self.core_id,
                    src_axon: comp_id,
                    dst_core: 0, // Will be routed later
                    dst_compartment: 0,
                    weight: 1,
                    timestamp: 0, // Would be set by global timer
                };
                output_spikes.push(spike_packet);
            } else {
                self.axon_outputs[comp_id] = false;
            }

            // Reset dendrite accumulator
            self.dendrite_accumulators[comp_id] = F::zero();
        }

        Ok(output_spikes)
    }

    /// Receive spike from another core or external input
    pub fn receive_spike(&mut self, spike: &SpikePacket) -> crate::error::Result<()> {
        if spike.dst_compartment < self.dendrite_accumulators.len() {
            self.dendrite_accumulators[spike.dst_compartment] =
                self.dendrite_accumulators[spike.dst_compartment] + F::from(spike.weight).unwrap();
        }
        Ok(())
    }
}

impl Default for SpikeRouter {
    fn default() -> Self {
        Self::new()
    }
}

impl SpikeRouter {
    /// Create new spike router
    pub fn new() -> Self {
        Self {
            routing_table: HashMap::new(),
            spike_buffers: HashMap::new(),
            routing_latency: 1,
        }
    }

    /// Route spikes between cores
    pub fn route_spikes(
        &mut self,
        input_spikes: &[SpikePacket],
    ) -> crate::error::Result<Vec<SpikePacket>> {
        let mut routed_spikes = Vec::new();

        for spike in input_spikes {
            let key = (spike.src_core, spike.src_axon);
            if let Some(destinations) = self.routing_table.get(&key) {
                for &(dst_core, dst_compartment) in destinations {
                    let mut routed_spike = spike.clone();
                    routed_spike.dst_core = dst_core;
                    routed_spike.dst_compartment = dst_compartment;
                    routed_spikes.push(routed_spike);
                }
            }
        }

        Ok(routed_spikes)
    }

    /// Configure routing entry
    pub fn add_routing_entry(
        &mut self,
        src_core: usize,
        src_axon: usize,
        destinations: Vec<(usize, usize)>,
    ) {
        self.routing_table
            .insert((src_core, src_axon), destinations);
    }
}

impl<F: Float + Debug + Clone + FromPrimitive> OnChipLearningEngine<F> {
    /// Create new on-chip learning engine
    pub fn new(_learningrule: OnChipLearningRule) -> Self {
        let mut parameters = HashMap::new();

        match _learningrule {
            OnChipLearningRule::STDP => {
                parameters.insert("tau_plus".to_string(), F::from(20.0).unwrap());
                parameters.insert("tau_minus".to_string(), F::from(20.0).unwrap());
                parameters.insert("a_plus".to_string(), F::from(0.01).unwrap());
                parameters.insert("a_minus".to_string(), F::from(0.01).unwrap());
            }
            _ => {
                // Other learning rules would have their own parameters
            }
        }

        Self {
            learning_rule: _learningrule,
            parameters,
            traces: HashMap::new(),
        }
    }

    /// Apply learning rule to synaptic weight
    pub fn apply_learning(
        &mut self,
        pre_spike_time: u64,
        post_spike_time: u64,
        current_weight: F,
    ) -> crate::error::Result<F> {
        match self.learning_rule {
            OnChipLearningRule::STDP => {
                let dt = post_spike_time as i64 - pre_spike_time as i64;
                let default_tau = F::from(20.0).unwrap();
                let default_a = F::from(0.01).unwrap();
                let tau_plus = self.parameters.get("tau_plus").unwrap_or(&default_tau);
                let tau_minus = self.parameters.get("tau_minus").unwrap_or(&default_tau);
                let a_plus = self.parameters.get("a_plus").unwrap_or(&default_a);
                let a_minus = self.parameters.get("a_minus").unwrap_or(&default_a);

                let weight_change = if dt > 0 {
                    // Post before pre -> LTP
                    *a_plus * (-F::from(dt as f64).unwrap() / *tau_plus).exp()
                } else if dt < 0 {
                    // Pre before post -> LTD
                    -*a_minus * (F::from(dt as f64).unwrap() / *tau_minus).exp()
                } else {
                    F::zero()
                };

                Ok(current_weight + weight_change)
            }
            _ => Ok(current_weight), // Other rules would be implemented here
        }
    }
}

impl<F: Float + Debug + Clone + FromPrimitive> PowerManager<F> {
    /// Create new power manager
    pub fn new(_numcores: usize) -> Self {
        Self {
            core_power: Array1::from_elem(_numcores, F::from(0.1).unwrap()), // 0.1W per core
            power_budget: F::from(10.0).unwrap(),                            // 10W total budget
            voltage_scaling: Array1::from_elem(_numcores, F::one()),
            clock_gating: Array1::from_elem(_numcores, false),
            power_history: Vec::new(),
        }
    }

    /// Update power consumption based on core activity
    pub fn update_power_consumption(
        &mut self,
        cores: &[NeuromorphicCore<F>],
    ) -> crate::error::Result<()> {
        let mut total_power = F::zero();

        for (i, core) in cores.iter().enumerate() {
            if i < self.core_power.len() {
                // Calculate dynamic power based on spike activity
                let spike_count = core.axon_outputs.iter().filter(|&&x| x).count();
                let dynamic_power = F::from(spike_count as f64).unwrap() * F::from(0.001).unwrap(); // 1mW per spike
                let static_power = F::from(0.05).unwrap(); // 50mW static power

                self.core_power[i] = static_power + dynamic_power;
                total_power = total_power + self.core_power[i];
            }
        }

        // Apply power management if over budget
        if total_power > self.power_budget {
            self.apply_power_management()?;
        }

        self.power_history.push(total_power);
        Ok(())
    }

    /// Apply power management strategies
    fn apply_power_management(&mut self) -> crate::error::Result<()> {
        // Dynamic voltage and frequency scaling
        for i in 0..self.voltage_scaling.len() {
            if self.core_power[i] > F::from(0.2).unwrap() {
                self.voltage_scaling[i] = F::from(0.8).unwrap(); // Reduce voltage
            } else {
                self.voltage_scaling[i] = F::one();
            }
        }

        // Clock gating for inactive cores
        for i in 0..self.clock_gating.len() {
            self.clock_gating[i] = self.core_power[i] < F::from(0.06).unwrap(); // Gate if low activity
        }

        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use approx::assert_abs_diff_eq;
    use scirs2_core::ScientificNumber;

    #[test]
    fn test_neuron_models() {
        let lif = NeuronModel::LeakyIntegrateFire {
            tau_m: 20.0,
            v_rest: -70.0,
            v_threshold: -55.0,
            v_reset: -70.0,
        };

        match lif {
            NeuronModel::LeakyIntegrateFire { tau_m, .. } => {
                assert_eq!(tau_m, 20.0);
            }
            _ => panic!("Wrong neuron model"),
        }

        let izhikevich = NeuronModel::Izhikevich {
            a: 0.02,
            b: 0.2,
            c: -65.0,
            d: 8.0,
        };

        match izhikevich {
            NeuronModel::Izhikevich { a, b, c, d } => {
                assert_eq!(a, 0.02);
                assert_eq!(b, 0.2);
                assert_eq!(c, -65.0);
                assert_eq!(d, 8.0);
            }
            _ => panic!("Wrong neuron model"),
        }
    }

    #[test]
    fn test_spiking_neural_network() {
        let layer_sizes = vec![3, 5, 2];
        let neuron_models = vec![
            NeuronModel::LeakyIntegrateFire {
                tau_m: 20.0,
                v_rest: -70.0,
                v_threshold: -55.0,
                v_reset: -70.0,
            },
            NeuronModel::LeakyIntegrateFire {
                tau_m: 20.0,
                v_rest: -70.0,
                v_threshold: -55.0,
                v_reset: -70.0,
            },
            NeuronModel::LeakyIntegrateFire {
                tau_m: 20.0,
                v_rest: -70.0,
                v_threshold: -55.0,
                v_reset: -70.0,
            },
        ];
        let plasticity_rules = vec![
            PlasticityRule::STDP {
                tau_plus: 20.0,
                tau_minus: 20.0,
                a_plus: 0.01,
                a_minus: 0.01,
            },
            PlasticityRule::STDP {
                tau_plus: 20.0,
                tau_minus: 20.0,
                a_plus: 0.01,
                a_minus: 0.01,
            },
        ];

        let snn =
            SpikingNeuralNetwork::<f64>::new(layer_sizes, neuron_models, plasticity_rules, 0.1);
        assert!(snn.is_ok());

        let mut network = snn.unwrap();

        // Test spike encoding
        let data = Array1::from_vec(vec![0.1, 0.5, 0.8, 0.3, 0.9]);
        let spikes = network.encode_time_series(&data);
        assert!(!spikes.is_empty());

        // Test network processing
        let result = network.process_spikes(&spikes);
        assert!(result.is_ok());
    }

    #[test]
    fn test_liquid_state_machine() {
        let lsm = LiquidStateMachine::<f64>::new(10, 3, 2, 0.9, 0.1);
        assert!(lsm.is_ok());

        let mut machine = lsm.unwrap();

        // Test processing
        let data = Array1::from_vec(vec![0.1, 0.2, 0.3]);
        let result = machine.process_time_series(&data);
        assert!(result.is_ok());

        let output = result.unwrap();
        assert_eq!(output.len(), 2);
    }

    #[test]
    fn test_memristive_network() {
        let learning_params = MemristiveLearningParams {
            learning_rate: 0.01,
            decay: 0.99,
            noise: 0.01,
            threshold: 0.1,
        };

        let network =
            MemristiveNetwork::<f64>::new(5, NetworkTopology::FullyConnected, learning_params);

        assert_eq!(network.crossbar.dim(), (5, 5));

        // Test output computation
        let input = Array1::from_vec(vec![1.0, 0.5, 0.0, 0.8, 0.3]);
        let output = network.compute_output(&input);
        assert_eq!(output.len(), 5);
    }

    #[test]
    fn test_neuron_state() {
        let state = NeuronState::<f64>::default();

        assert_abs_diff_eq!(state.v.to_f64().unwrap(), -70.0, epsilon = 1e-10);
        assert_abs_diff_eq!(state.u.to_f64().unwrap(), 0.0, epsilon = 1e-10);
        assert!(state.last_spike.is_none());
        assert_eq!(state.refractory, 0.0);
    }

    #[test]
    fn test_plasticity_rules() {
        let stdp = PlasticityRule::STDP {
            tau_plus: 20.0,
            tau_minus: 20.0,
            a_plus: 0.01,
            a_minus: 0.01,
        };

        match stdp {
            PlasticityRule::STDP {
                tau_plus,
                tau_minus,
                a_plus,
                a_minus,
            } => {
                assert_eq!(tau_plus, 20.0);
                assert_eq!(tau_minus, 20.0);
                assert_eq!(a_plus, 0.01);
                assert_eq!(a_minus, 0.01);
            }
            _ => panic!("Wrong plasticity rule"),
        }

        let hebbian = PlasticityRule::Hebbian {
            learning_rate: 0.01,
            decay_rate: 0.001,
        };

        match hebbian {
            PlasticityRule::Hebbian {
                learning_rate,
                decay_rate,
            } => {
                assert_eq!(learning_rate, 0.01);
                assert_eq!(decay_rate, 0.001);
            }
            _ => panic!("Wrong plasticity rule"),
        }
    }

    #[test]
    fn test_spike_structure() {
        let spike = Spike {
            time: 10.5,
            neuron_id: 42,
            amplitude: 1.2,
        };

        assert_eq!(spike.time, 10.5);
        assert_eq!(spike.neuron_id, 42);
        assert_eq!(spike.amplitude, 1.2);
    }
}
