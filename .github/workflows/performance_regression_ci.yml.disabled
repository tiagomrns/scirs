name: Advanced Performance Regression CI

on:
  push:
    branches: [ master, main ]
  pull_request:
    branches: [ master, main ]
  schedule:
    # Run comprehensive performance tests twice daily
    - cron: '0 6,18 * * *'
  workflow_dispatch:
    inputs:
      baseline_commit:
        description: 'Baseline commit hash for comparison'
        required: false
        type: string
      performance_profile:
        description: 'Performance testing profile'
        required: false
        default: 'comprehensive'
        type: choice
        options:
          - quick
          - comprehensive
          - stress
          - memory-focused

env:
  RUST_BACKTRACE: 1
  CARGO_TERM_COLOR: always
  PERFORMANCE_DATA_RETENTION_DAYS: 90
  REGRESSION_CONFIDENCE_THRESHOLD: 0.95

jobs:
  # Matrix-based performance testing across platforms and configurations
  performance-matrix:
    name: Performance Tests (${{ matrix.os }}, ${{ matrix.profile }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        profile: [quick, comprehensive]
        include:
          - os: ubuntu-latest
            profile: stress
            extended: true
          - os: ubuntu-latest 
            profile: memory-focused
            extended: true
            
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      with:
        fetch-depth: 100  # Need history for regression analysis

    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy

    - name: Install system dependencies
      shell: bash
      run: |
        case "${{ matrix.os }}" in
          ubuntu-*)
            sudo apt-get update
            sudo apt-get install -y libopenblas-dev liblapack-dev gfortran \
                                    python3-pip python3-dev valgrind perf \
                                    linux-tools-generic psutil
            pip3 install numpy scipy matplotlib pandas psutil requests
            ;;
          macos-*)
            brew install openblas lapack python3
            pip3 install numpy scipy matplotlib pandas psutil requests
            ;;
          windows-*)
            # Windows-specific setup
            python -m pip install numpy scipy matplotlib pandas psutil requests
            ;;
        esac

    - name: Install performance tools
      uses: taiki-e/install-action@v2
      with:
        tool: cargo-nextest,criterion,cargo-bench,flamegraph

    - name: Cache performance history
      uses: actions/cache@v4
      with:
        path: |
          performance_history.json
          performance_baselines/
          ~/.cargo/registry
          ~/.cargo/git
          target/
        key: ${{ runner.os }}-performance-${{ hashFiles('**/Cargo.lock') }}-${{ github.sha }}
        restore-keys: |
          ${{ runner.os }}-performance-${{ hashFiles('**/Cargo.lock') }}-
          ${{ runner.os }}-performance-

    - name: Setup performance environment
      shell: bash
      run: |
        # Create performance test directory structure
        mkdir -p performance_reports
        mkdir -p performance_baselines
        mkdir -p performance_artifacts
        
        # Setup environment variables
        echo "PERFORMANCE_PROFILE=${{ matrix.profile }}" >> $GITHUB_ENV
        echo "PERFORMANCE_OS=${{ matrix.os }}" >> $GITHUB_ENV
        echo "PERFORMANCE_RUN_ID=${{ github.run_id }}" >> $GITHUB_ENV
        echo "PERFORMANCE_COMMIT=${{ github.sha }}" >> $GITHUB_ENV
        echo "PERFORMANCE_BRANCH=${{ github.ref_name }}" >> $GITHUB_ENV
        
        # Setup CPU affinity for consistent results
        if [ "${{ matrix.os }}" = "ubuntu-latest" ]; then
          echo "PERFORMANCE_CPU_AFFINITY=0,1" >> $GITHUB_ENV
          # Disable CPU frequency scaling for consistent results
          echo 'GOVERNOR=performance' | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor || true
        fi

    - name: Pre-benchmark system profiling
      shell: bash
      run: |
        # Capture system baseline
        echo "=== System Information ===" > performance_artifacts/system_info.txt
        uname -a >> performance_artifacts/system_info.txt
        
        if command -v lscpu >/dev/null 2>&1; then
          echo "=== CPU Information ===" >> performance_artifacts/system_info.txt
          lscpu >> performance_artifacts/system_info.txt
        fi
        
        if command -v free >/dev/null 2>&1; then
          echo "=== Memory Information ===" >> performance_artifacts/system_info.txt
          free -h >> performance_artifacts/system_info.txt
        fi
        
        # Capture initial process state
        python3 -c "
import psutil
import json
import sys

system_info = {
    'cpu_count': psutil.cpu_count(),
    'cpu_freq': psutil.cpu_freq()._asdict() if psutil.cpu_freq() else None,
    'memory': psutil.virtual_memory()._asdict(),
    'disk': psutil.disk_usage('/')._asdict() if hasattr(psutil, 'disk_usage') else None,
    'boot_time': psutil.boot_time()
}

with open('performance_artifacts/system_baseline.json', 'w') as f:
    json.dump(system_info, f, indent=2)
        "

    - name: Build optimized binaries
      shell: bash
      run: |
        echo "Building optimized binaries for performance testing..."
        
        # Build with maximum optimizations
        export RUSTFLAGS="-C target-cpu=native -C opt-level=3 -C lto=fat -C codegen-units=1"
        
        # Build workspace with performance optimizations
        cargo build --workspace --release --all-features
        
        # Build specific benchmarks
        cargo build --benches --release --all-features
        
        # Verify builds
        echo "Build verification complete"

    - name: Execute performance test suite
      shell: bash
      timeout-minutes: 180
      run: |
        echo "Starting performance test suite: ${{ matrix.profile }}"
        
        # Set performance test configuration
        case "${{ matrix.profile }}" in
          quick)
            export BENCHMARK_ITERATIONS=10
            export BENCHMARK_SAMPLE_SIZE=100
            export MEMORY_STRESS_DURATION=30
            ;;
          comprehensive)
            export BENCHMARK_ITERATIONS=50
            export BENCHMARK_SAMPLE_SIZE=1000
            export MEMORY_STRESS_DURATION=120
            ;;
          stress)
            export BENCHMARK_ITERATIONS=100
            export BENCHMARK_SAMPLE_SIZE=5000
            export MEMORY_STRESS_DURATION=300
            ;;
          memory-focused)
            export BENCHMARK_ITERATIONS=20
            export BENCHMARK_SAMPLE_SIZE=500
            export MEMORY_STRESS_DURATION=600
            export ENABLE_MEMORY_PROFILING=1
            ;;
        esac
        
        # Execute scirs2-optim performance tests with enhanced monitoring
        cd scirs2-optim
        
        # Run the comprehensive CI/CD performance integration
        python3 scripts/ci_cd_performance_integration.py \
          --project-root . \
          --config performance_config.json \
          --output-dir ../performance_reports \
          --fail-on-regression \
          --verbose
        
        # Run additional memory profiling if enabled
        if [ "${{ matrix.extended }}" = "true" ]; then
          echo "Running extended performance analysis..."
          
          # Memory leak detection
          if command -v valgrind >/dev/null 2>&1; then
            echo "Running memory leak detection..."
            valgrind --tool=memcheck --leak-check=full --show-leak-kinds=all \
              --track-origins=yes --verbose --log-file=../performance_artifacts/valgrind.log \
              cargo test --release --all-features || true
          fi
          
          # CPU profiling
          if command -v perf >/dev/null 2>&1; then
            echo "Running CPU profiling..."
            perf record -g cargo bench --all-features || true
            perf report --stdio > ../performance_artifacts/perf_report.txt || true
          fi
        fi

    - name: Analyze performance regressions
      shell: bash
      run: |
        echo "Analyzing performance data for regressions..."
        
        cd scirs2-optim
        
        # Run the advanced regression detection
        python3 scripts/analyze_performance.py \
          --input-dir ../performance_reports \
          --output-dir ../performance_artifacts \
          --baseline-dir ../performance_baselines \
          --confidence-threshold $REGRESSION_CONFIDENCE_THRESHOLD \
          --format json,html,junit
        
        # Generate performance comparison reports
        python3 scripts/generate_performance_report.py \
          --current-data ../performance_reports/performance_report.json \
          --baseline-data ../performance_baselines/baseline_${{ matrix.os }}.json \
          --output ../performance_artifacts/regression_analysis.html

    - name: Store performance baselines
      shell: bash
      if: github.ref == 'refs/heads/master' || github.ref == 'refs/heads/main'
      run: |
        echo "Storing performance baselines for future comparisons..."
        
        # Create baseline from current successful run
        if [ -f "performance_reports/performance_report.json" ]; then
          cp performance_reports/performance_report.json \
             performance_baselines/baseline_${{ matrix.os }}_$(date +%Y%m%d).json
          
          # Keep only recent baselines (last 30 days)
          find performance_baselines/ -name "baseline_${{ matrix.os }}_*.json" \
               -type f -mtime +30 -delete || true
        fi

    - name: Generate performance artifacts
      shell: bash
      if: always()
      run: |
        echo "Generating comprehensive performance artifacts..."
        
        # Create performance summary
        cat > performance_artifacts/summary.md << 'EOF'
# Performance Test Summary
        
## Test Configuration
- **OS**: ${{ matrix.os }}
- **Profile**: ${{ matrix.profile }}
- **Commit**: ${{ github.sha }}
- **Branch**: ${{ github.ref_name }}
- **Timestamp**: $(date -u)
        
## Test Results
EOF
        
        # Add regression analysis results if available
        if [ -f "performance_reports/performance_report.json" ]; then
          python3 -c "
import json
import sys

try:
    with open('performance_reports/performance_report.json', 'r') as f:
        data = json.load(f)
    
    analysis = data.get('analysis', {})
    summary = analysis.get('summary', {})
    
    print(f'- **Status**: {summary.get(\"status\", \"unknown\")}')
    print(f'- **Total Benchmarks**: {summary.get(\"total_benchmarks\", 0)}')
    print(f'- **Regressions Detected**: {summary.get(\"regressions_count\", 0)}')
    print(f'- **Warnings**: {summary.get(\"warnings_count\", 0)}')
    
except Exception as e:
    print(f'- **Error**: Could not parse performance data: {e}')
          " >> performance_artifacts/summary.md
        fi
        
        # Create test artifacts archive
        tar -czf performance_artifacts_${{ matrix.os }}_${{ matrix.profile }}.tar.gz \
                 performance_artifacts/ performance_reports/ || true

    - name: Upload performance artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-artifacts-${{ matrix.os }}-${{ matrix.profile }}
        path: |
          performance_artifacts/
          performance_reports/
          performance_artifacts_${{ matrix.os }}_${{ matrix.profile }}.tar.gz
        retention-days: ${{ env.PERFORMANCE_DATA_RETENTION_DAYS }}

    - name: Performance regression notification
      shell: bash
      if: failure()
      run: |
        echo "Performance regression detected! Creating notification..."
        
        # Create regression alert
        cat > performance_regression_alert.json << 'EOF'
{
  "alert_type": "performance_regression",
  "severity": "high",
  "os": "${{ matrix.os }}",
  "profile": "${{ matrix.profile }}",
  "commit": "${{ github.sha }}",
  "branch": "${{ github.ref_name }}",
  "run_id": "${{ github.run_id }}",
  "timestamp": "$(date -u -Iseconds)",
  "details_url": "https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
}
EOF
        
        echo "::error::Performance regression detected in ${{ matrix.profile }} profile on ${{ matrix.os }}"

  # Aggregate performance analysis across all platforms
  performance-analysis:
    name: Aggregate Performance Analysis
    runs-on: ubuntu-latest
    needs: performance-matrix
    if: always()
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install analysis dependencies
      run: |
        pip install numpy pandas matplotlib seaborn plotly jupyter nbformat \
                   scipy scikit-learn requests psutil

    - name: Download all performance artifacts
      uses: actions/download-artifact@v4
      with:
        pattern: performance-artifacts-*
        path: all_performance_data/

    - name: Aggregate performance analysis
      run: |
        echo "Performing aggregate performance analysis..."
        
        # Create comprehensive analysis script
        cat > aggregate_analysis.py << 'EOF'
import json
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path
import sys

def analyze_performance_data():
    """Analyze performance data across all platforms and profiles"""
    
    data_dir = Path('all_performance_data')
    results = []
    
    # Collect all performance reports
    for report_file in data_dir.rglob('performance_report.json'):
        try:
            with open(report_file) as f:
                data = json.load(f)
                
            # Extract metadata from path
            path_parts = str(report_file).split('/')
            os_info = next((p for p in path_parts if 'ubuntu' in p or 'macos' in p or 'windows' in p), 'unknown')
            profile = next((p for p in path_parts if p in ['quick', 'comprehensive', 'stress', 'memory-focused']), 'unknown')
            
            # Add to results
            result = {
                'os': os_info,
                'profile': profile,
                'status': data.get('analysis', {}).get('summary', {}).get('status', 'unknown'),
                'regressions': data.get('analysis', {}).get('summary', {}).get('regressions_count', 0),
                'warnings': data.get('analysis', {}).get('summary', {}).get('warnings_count', 0),
                'benchmarks': data.get('analysis', {}).get('summary', {}).get('total_benchmarks', 0)
            }
            results.append(result)
            
        except Exception as e:
            print(f"Error processing {report_file}: {e}")
    
    # Create summary DataFrame
    if results:
        df = pd.DataFrame(results)
        
        # Generate summary statistics
        print("=== Performance Test Summary ===")
        print(f"Total test configurations: {len(df)}")
        print(f"Successful configurations: {len(df[df['status'] == 'pass'])}")
        print(f"Configurations with regressions: {len(df[df['regressions'] > 0])}")
        print(f"Configurations with warnings: {len(df[df['warnings'] > 0])}")
        
        # Create summary JSON
        summary = {
            'total_configurations': len(df),
            'successful_configurations': len(df[df['status'] == 'pass']),
            'regression_configurations': len(df[df['regressions'] > 0]),
            'warning_configurations': len(df[df['warnings'] > 0]),
            'overall_status': 'pass' if len(df[df['regressions'] > 0]) == 0 else 'regression_detected',
            'details': df.to_dict('records')
        }
        
        with open('performance_summary.json', 'w') as f:
            json.dump(summary, f, indent=2)
        
        # Create visualization if matplotlib available
        try:
            fig, axes = plt.subplots(2, 2, figsize=(12, 10))
            
            # Status distribution
            status_counts = df['status'].value_counts()
            axes[0, 0].pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%')
            axes[0, 0].set_title('Test Status Distribution')
            
            # Regressions by OS
            regression_by_os = df.groupby('os')['regressions'].sum()
            axes[0, 1].bar(regression_by_os.index, regression_by_os.values)
            axes[0, 1].set_title('Regressions by OS')
            axes[0, 1].set_ylabel('Number of Regressions')
            
            # Warnings by profile
            warnings_by_profile = df.groupby('profile')['warnings'].sum()
            axes[1, 0].bar(warnings_by_profile.index, warnings_by_profile.values)
            axes[1, 0].set_title('Warnings by Profile')
            axes[1, 0].set_ylabel('Number of Warnings')
            
            # Benchmark count distribution
            axes[1, 1].hist(df['benchmarks'], bins=10, alpha=0.7)
            axes[1, 1].set_title('Benchmark Count Distribution')
            axes[1, 1].set_xlabel('Number of Benchmarks')
            axes[1, 1].set_ylabel('Frequency')
            
            plt.tight_layout()
            plt.savefig('performance_analysis.png', dpi=300, bbox_inches='tight')
            print("Performance analysis chart saved as performance_analysis.png")
            
        except ImportError:
            print("Matplotlib not available, skipping visualization")
        
        return summary['overall_status']
    else:
        print("No performance data found")
        return 'no_data'

if __name__ == '__main__':
    status = analyze_performance_data()
    sys.exit(0 if status == 'pass' else 1)
EOF
        
        # Run the analysis
        python aggregate_analysis.py

    - name: Create performance dashboard
      run: |
        echo "Creating performance dashboard..."
        
        # Create HTML dashboard
        cat > performance_dashboard.html << 'EOF'
<!DOCTYPE html>
<html>
<head>
    <title>SciRS2 Performance Dashboard</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .status-pass { color: green; font-weight: bold; }
        .status-regression { color: red; font-weight: bold; }
        .status-warning { color: orange; font-weight: bold; }
        table { border-collapse: collapse; width: 100%; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }
        .metric { background-color: #f9f9f9; padding: 10px; margin: 10px 0; border-radius: 5px; }
    </style>
</head>
<body>
    <h1>SciRS2 Performance Test Dashboard</h1>
    <p><strong>Generated:</strong> $(date -u)</p>
    <p><strong>Commit:</strong> ${{ github.sha }}</p>
    <p><strong>Branch:</strong> ${{ github.ref_name }}</p>
    
    <div class="metric">
        <h2>Performance Summary</h2>
        <div id="summary-content">
            <!-- Summary will be populated by JavaScript -->
        </div>
    </div>
    
    <div class="metric">
        <h2>Test Matrix Results</h2>
        <div id="matrix-results">
            <!-- Matrix results will be populated -->
        </div>
    </div>
    
    <script>
        // Load and display performance summary
        fetch('performance_summary.json')
            .then(response => response.json())
            .then(data => {
                const summaryDiv = document.getElementById('summary-content');
                summaryDiv.innerHTML = \`
                    <p>Total Configurations: \${data.total_configurations}</p>
                    <p>Successful: \${data.successful_configurations}</p>
                    <p>Regressions: \${data.regression_configurations}</p>
                    <p>Warnings: \${data.warning_configurations}</p>
                    <p class="status-\${data.overall_status}">Overall Status: \${data.overall_status}</p>
                \`;
                
                const matrixDiv = document.getElementById('matrix-results');
                let tableHTML = '<table><tr><th>OS</th><th>Profile</th><th>Status</th><th>Regressions</th><th>Warnings</th></tr>';
                
                data.details.forEach(item => {
                    tableHTML += \`<tr>
                        <td>\${item.os}</td>
                        <td>\${item.profile}</td>
                        <td class="status-\${item.status}">\${item.status}</td>
                        <td>\${item.regressions}</td>
                        <td>\${item.warnings}</td>
                    </tr>\`;
                });
                
                tableHTML += '</table>';
                matrixDiv.innerHTML = tableHTML;
            })
            .catch(error => {
                console.error('Error loading performance data:', error);
                document.getElementById('summary-content').innerHTML = 
                    '<p style="color: red;">Error loading performance data</p>';
            });
    </script>
</body>
</html>
EOF

    - name: Upload aggregate analysis
      uses: actions/upload-artifact@v4
      with:
        name: performance-aggregate-analysis
        path: |
          performance_summary.json
          performance_dashboard.html
          performance_analysis.png
          aggregate_analysis.py

    - name: Comment PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          try {
            const summaryData = JSON.parse(fs.readFileSync('performance_summary.json', 'utf8'));
            
            let statusEmoji = '✅';
            let statusText = 'All performance tests passed';
            
            if (summaryData.regression_configurations > 0) {
              statusEmoji = '❌';
              statusText = `${summaryData.regression_configurations} configuration(s) detected performance regressions`;
            } else if (summaryData.warning_configurations > 0) {
              statusEmoji = '⚠️';
              statusText = `${summaryData.warning_configurations} configuration(s) have performance warnings`;
            }
            
            const comment = \`## ${statusEmoji} Performance Test Results
            
${statusText}

### Summary
- **Total Configurations**: ${summaryData.total_configurations}
- **Successful**: ${summaryData.successful_configurations}
- **Regressions**: ${summaryData.regression_configurations}
- **Warnings**: ${summaryData.warning_configurations}

### Details
| OS | Profile | Status | Regressions | Warnings |
|---|---|---|---|---|
\${summaryData.details.map(item => 
  \`| ${item.os} | ${item.profile} | ${item.status} | ${item.regressions} | ${item.warnings} |\`
).join('\\n')}

[View detailed performance dashboard](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            \`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          } catch (error) {
            console.error('Error creating performance comment:', error);
          }

  # Memory leak detection job
  memory-leak-detection:
    name: Memory Leak Detection
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event_name == 'schedule'
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Install Rust and tools
      uses: dtolnay/rust-toolchain@stable

    - name: Install memory analysis tools
      run: |
        sudo apt-get update
        sudo apt-get install -y valgrind heaptrack libopenblas-dev liblapack-dev

    - name: Run memory leak detection
      run: |
        cd scirs2-optim
        
        # Run comprehensive memory leak detection
        python3 examples/memory_leak_detector_demo.py \
          --output-dir ../memory_analysis \
          --test-duration 300 \
          --memory-threshold 0.1
        
        # Run valgrind on critical optimizers
        valgrind --tool=memcheck --leak-check=full --show-leak-kinds=all \
          --track-origins=yes --log-file=../memory_analysis/valgrind_optimizers.log \
          cargo test --release test_adam_memory_usage || true

    - name: Upload memory analysis
      uses: actions/upload-artifact@v4
      with:
        name: memory-leak-analysis
        path: memory_analysis/

  # Cross-platform compatibility testing
  cross-platform-testing:
    name: Cross-Platform Compatibility
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-20.04, ubuntu-22.04, macos-12, macos-13, windows-2019, windows-2022]
        
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable

    - name: Install dependencies
      shell: bash
      run: |
        case "${{ matrix.os }}" in
          ubuntu-*)
            sudo apt-get update
            sudo apt-get install -y libopenblas-dev liblapack-dev
            ;;
          macos-*)
            brew install openblas lapack
            ;;
          windows-*)
            # Windows dependencies handled by vcpkg or pre-installed
            ;;
        esac

    - name: Run cross-platform tests
      shell: bash
      run: |
        cd scirs2-optim
        
        # Run platform-specific test suite
        python3 src/benchmarking/cross_platform_tester.py \
          --platform ${{ matrix.os }} \
          --output ../cross_platform_results_${{ matrix.os }}.json

    - name: Upload cross-platform results
      uses: actions/upload-artifact@v4
      with:
        name: cross-platform-${{ matrix.os }}
        path: cross_platform_results_${{ matrix.os }}.json

  # Final reporting and status
  performance-final-report:
    name: Performance Test Report
    runs-on: ubuntu-latest
    needs: [performance-matrix, performance-analysis, memory-leak-detection, cross-platform-testing]
    if: always()
    
    steps:
    - name: Generate final performance report
      run: |
        echo "# SciRS2 Optimization Module - Performance CI Report" > final_report.md
        echo "" >> final_report.md
        echo "**Timestamp**: $(date -u)" >> final_report.md
        echo "**Commit**: ${{ github.sha }}" >> final_report.md
        echo "**Branch**: ${{ github.ref_name }}" >> final_report.md
        echo "**Trigger**: ${{ github.event_name }}" >> final_report.md
        echo "" >> final_report.md
        
        echo "## Job Results" >> final_report.md
        echo "- Performance Matrix: ${{ needs.performance-matrix.result }}" >> final_report.md
        echo "- Performance Analysis: ${{ needs.performance-analysis.result }}" >> final_report.md
        echo "- Memory Leak Detection: ${{ needs.memory-leak-detection.result }}" >> final_report.md
        echo "- Cross-Platform Testing: ${{ needs.cross-platform-testing.result }}" >> final_report.md
        echo "" >> final_report.md
        
        # Determine overall status
        if [[ "${{ needs.performance-matrix.result }}" == "success" && \
              "${{ needs.performance-analysis.result }}" == "success" ]]; then
          echo "## ✅ Overall Status: PASSED" >> final_report.md
          echo "All performance tests completed successfully with no regressions detected." >> final_report.md
        else
          echo "## ❌ Overall Status: FAILED" >> final_report.md
          echo "Performance regressions or test failures detected. Please review the detailed reports." >> final_report.md
        fi

    - name: Upload final report
      uses: actions/upload-artifact@v4
      with:
        name: performance-final-report
        path: final_report.md